./commercial/GA1/1.0commercial.-vsa-overview.dita:<p>The Cinder volume service hosts the LeftHand Driver to communicate with the backend representing the StoreVirtual cluster, using the LeftHand REST API.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>If either of the servers that host the two overcloud controllers fails, the overcloud controller must be rebuilt and reconnected into the cluster as soon as possible.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>The management controller is similar to the overcloud controller nodes, but it also executes various additional services, including Compute, Sherpa, Telemetry, Reporting, and Block Storage services. If the server that hosts the overcloud management controller fails, the management controller must be rebuilt and restored as soon as possible.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Note that the seed cloud host server is where the seed VM is installed and the installation files are located.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<li>When any change is make in the undercloud from the seed cloud host</li>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed VM host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<codeph>root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_restore.sh --seed -f /root/backup/backup_14-09-02-12-32 -c /root/export.prop
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-02-12-32
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-02-12-32
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>1.Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_backup.sh --undercloud -f /root/backup/
./commercial/GA1/1.0commercial.backup-restore-GA.dita:    Destination Host Folder: /root/backup/
./commercial/GA1/1.0commercial.backup-restore-GA.dita:    Warning: Permanently added '192.0.2.2' (ECDSA) to the list of known hosts.
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-03-12-30
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-03-12-30
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-03-12-46
./commercial/GA1/1.0commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-03-12-46
./commercial/GA1/1.0commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_backup.sh --help
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -f|--dest-host-folder   - folder path to which to backup
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -H|--dest-host-ip       - ip of host to which to backup
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -u|--dest-host-user     - username of host to which to backup
./commercial/GA1/1.0commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_restore.sh --help
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -f|--source-host-folder     - folder path from which to restore
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -H|--source-host-ip     - ip of host from which to restore
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -u|--source-host-user   - username of host from which to restore
./commercial/GA1/1.0commercial.backup-restore-GA.dita:        -c|--config-file        - Installer will source this config file on the host (and on the seed in case undercloud is being restored)
./commercial/GA1/1.0commercial.backup-restore-GA.dita:<li>Log in to the seed cloud host.</li>
./commercial/GA1/1.0commercial.dashboard.launch.dita:<p>Ask the cloud operator for the host name or public IP address where the dashboard is located, your user name, and your password.</p>
./commercial/GA1/1.0commercial.dashboard.launch.dita:<p>In the address bar, enter the host name or IP address for the dashboard.</p>
./commercial/GA1/1.0commercial.eula.dita:<p>e Third Party and Open Source Components. To the extent any component of the software is subject to any third party license terms, including open source license terms, then those third party license terms or open source license terms shall govern with respect to the subject component; otherwise, the terms of this Agreement shall govern.</p>
./commercial/GA1/1.0commercial.eula.dita:<li>Per Server License: Following purchase, this per-server license is to be assigned to a Physical Server that will be used to run your Cloud Fabric.  After the Physical Servers are licensed and those licenses are properly assigned, you may run any number of instances of the Management Software to deploy, configure, manage and operate your Cloud Fabric.</li>
./commercial/GA1/1.0commercial.eula.dita:<b>b. Assigning Licenses:</b>  Before you can install and use the Management Software to deploy your Cloud Fabric, you must assign to each Physical Server running the host fabric one per-Physical Server license. Each Physical Server to which you assign a license is a licensed host server.</p>
./commercial/GA1/1.0commercial.faq.dita:<p>The undercloud also hosts images for various server types which will form the functional cloud environment - the overcloud. These images are overcloud Controller, overcloud Compute, overcloud Swift &amp; overcloud Compute Proxy (required for clouds that support VMWare ESX as a hypervisor).</p>
./commercial/GA1/1.0commercial.faq.dita:<p>Yes. It includes an integrated <tm tmtype="reg">Linux</tm> host OS hardened and tested for this distribution.</p>
./commercial/GA1/1.0commercial.faq.dita:<!-- removed per JR's comment I We are hosting the support discussion forum for the edition at [https://ask.openstack.org](https://ask.openstack.org).  Developers in the community are very familiar with this forum and already participate in OpenStack-related discussions there. Please tag your questions with 'HPHelion' to get our attention for any questions and issues you raise.--> 
./commercial/GA1/1.0commercial.install-3par.dita:<p>Install and configure the 3PAR StoreServ device and create Common Provisioning Groups (CPGs) which you are planning to use for the cloud as Cinder backend. The StoreServ device should be accessible from the management network of the cloud. If you are using Fibre Channel, ensure SAN connectivity between the compute host(s), the overcloud controller where the Volume Operations service is running, and the HPE 3PAR StoreServ array.</p>
./commercial/GA1/1.0commercial.install-add-nodes.dita:<codeph>nova-manage service disable --service=nova-compute --host=&lt;hostName of Compute Node&gt;
./commercial/GA1/1.0commercial.install-GA-CSV.dita:<p>During the installation process after the seed VM is installed, the installer script looks for information about the baremetal systems. Specifically, it looks for this information in a file called <codeph>baremetal.csv</codeph>. Before you begin the installation process, you must create this file and upload the file to the installer system (called the seed cloud host) at the appropriate installation step.</p>
./commercial/GA1/1.0commercial.install-GA-DNSaaS.dita:<p>A chosen backend driver and its prerequisites:</p>
./commercial/GA1/1.0commercial.install-GA-DNSaaS.dita:<p>PowerDNS (self hosted)<!--A BR tag was used here in the original source.-->
./commercial/GA1/1.0commercial.install-GA-DNSaaS.dita:<p>Microsoft DNS (self-hosted)</p>
./commercial/GA1/1.0commercial.install-GA-DNSaaS.dita:<li>keystone_host â€” Hostname or IP address of Keystone endpoint.
./commercial/GA1/1.0commercial.install-GA-DNSaaS.dita:<li>msdns_servers: A comma separated list of the Microsoft DNS servers short hostnames</li>
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:<p>The HPE Helion OpenStack vCenter ESX compute proxy is a driver that enables the Compute service to communicate with a VMware vCenter server managing one or more ESX hosts. The HPE Helion OpenStack Compute Service (Nova) requires this driver to interface with VMWare ESX hypervisor APIs.</p>
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:# 1) hostname -&gt; should not have '_'    
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:hostname=
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:hostname = enter the name of the host name of the compute proxy
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:<p>A vCenter proxy VM named <codeph>hp_helion_vcenter_proxy</codeph> will be available in the specified vCenter. You can access that proxy VM from the seed VM host as the <codeph>heat-admin</codeph> user without password.</p>
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:3. Edit the [`compute_proxy.conf`](#undercloud) file to change the host-name of the vCenter (Compulsory) instead of `ip-address` in the `-??-ip-address=VCENTER_IP_ADDRESS` line.
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:4. If DNS name resolution is not available add a `/etc/hosts` entry for the vCenter ip address.
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:3. Provide the FQDN host-name of the vCenter instead of the ip-address in the **Server Address** field.
./commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita:<p>If you have not deployed the HPE Virtual Cloud Networking's Open vSwitch vApp (OVSvApp), see the <xref href="../../commercial/GA1/1.0commercial.install-GA-ovsvapp.dita" >Deploying and configuring OVSvApp for HPE Virtual Cloud Networking (VCN) on ESX hosts</xref> document for complete instructions.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<li>Preparing seed cloud host to run seed VM</li>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<section id="prepseed"> <title>Prepare the seed cloud host to create the seed VM</title>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>On the server identified to run the seed VM, called the seed cloud host (or installation system), make sure that Ubuntu 14.04 LTS Server edition is installed and operating, as listed in <xref href="../../commercial/GA1/1.0commercial.install-GA-prereqs.dita#ubuntu" type="section">Installation: Prerequisites</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>Make sure you have met all the hardware requirements and have completed the required tasks before you begin your installation. The following sections walk you through the steps to be executed on the seed cloud host:</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>Before you begin your installation on the seed cloud host, if necessary configure the proxy information for your environment using the following steps:</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>Launch a terminal and log in to your seed cloud host as root:</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:export no_proxy=localhost,127.0.0.1,&lt;your 10.x IP address&gt;,&lt;provider_network&gt;
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>Log out and re-login to the seed cloud host to activate the proxy configuration.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>Make sure you are logged into the seed cloud host as root. If not:</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed
./commercial/GA1/1.0commercial.install-GA-esx.dita:<b>Note:</b> If <codeph>hp_ced_host_manager.sh</codeph> fails to start the seed, restart the installation (step 1) and then follow the rest of the steps.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>When prompted for host authentication, type <codeph>yes</codeph> to allow the SSH connection to proceed.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>If you are integrating LDAP into your environment, copy the configuration files, as described in <xref href="../../commercial/GA1/1.0commercial.install-GA-LDAP.dita" >Integrating LDAP</xref>, to the seed cloud host.</p>
./commercial/GA1/1.0commercial.install-GA-esx.dita:        OVERCLOUD_IP=$(jq '.overcloud.endpointhost' /root/tripleo/ce_env.json)
./commercial/GA1/1.0commercial.install-GA-esx.dita:<p>The HPE Helion OpenStack vCenter ESX compute proxy is a driver that enables the Compute service to communicate with a VMware vCenter server that manages one or more ESX hosts. The HPE Helion OpenStack Compute service (Nova) requires this driver to interface with VMWare ESX hypervisor APIs.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<codeph>bridge_interface</codeph> - Use this variable to specify the interface on the seed cloud host to use as the bridge interface, for example <codeph>em2</codeph> or <codeph>eth2</codeph>. This interface connects to the untagged management network and will be used to PXE boot undercloud and overcloud servers:</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<codeph>network_gateway</codeph> - Use this variable to specify a host other than the seed cloud host as the gateway, for example 192.168.130.1. Typically this IP will be the physical gateway of the network.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<codeph>network_seed_range_start</codeph>, <codeph>network_seed_range_end</codeph> - Use these variables to specify an IP address range for the seed cloud host to administrate/manage the undercloud node(s), for example 192.168.130.4-192.168.130.22.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<b>Note:</b> You must choose the <codeph>network_seed_range_end_xxx</codeph> and to be consistent with any values already chosen for <codeph>network_seed_range_end</codeph> and <codeph>network_seed_range_end</codeph>. All addresses must be on a common subnet.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<b>Note:</b> You must choose the <codeph>network_undercloud_range_xxx</codeph> to be consistent with any values already chosen for <codeph>network_seed_ip</codeph> and <codeph>network_cidr</codeph>. All addresses must be on a common subnet.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<codeph>overcloud_server</codeph> - Use this variable to set the IP address of an NTP server accessible on the public interface for overcloud hosts. This is required.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<codeph>undercloud_server</codeph> - Use this variable to set the IP address of an NTP server accessible on the public interface for undercloud hosts. This is required.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<b>Note:</b> The overcloud neutron external network (ext-net) assumes the gateway IP is the lowest non-zero host IP address in the <codeph>cidr</codeph> range.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<p>If you plan to use custom IP addresses in your HPE Helion OpenStack deployment, open the JSON file in the installation package named <codeph>kvm-custom-ips.json</codeph> and edit the following environment variables. Save the file on the seed cloud host (installation system). The variables are defined in <xref type="section" href="#topic18514/env">Definition of Environment variables used during install</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<p>Save the file on the seed cloud host.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<p>If you plan to use custom IP addresses in your HPE Helion OpenStack deployment, open the JSON file in the installation package named <codeph>esx-custom-ips.json</codeph> and edit the following environment variables. Save the file on the seed cloud host (installation system). The variables are defined in <xref type="section" href="#topic18514/env">Definition of Environment variables used during install</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-JSON.dita:<p>If you intend to use custom IP addresses and a VLAN provider network for external access in your HPE Helion OpenStack deployment, open the JSON file in the installation package named <codeph>esx-custom-ips.json</codeph> and edit the following environment variables. Save the file on the seed cloud host (installation system). The variables are defined in <xref type="section" href="#topic18514/env">Definition of Environment variables used during install</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<xref type="section" href="#topic10604/prepseed">Preparing seed cloud host to create the seed VM</xref>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<section id="prepseed"> <title>Prepare the cloud seed host to create the seed VM</title>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>On the server identified to run the seed VM, called the seed VM host (or installation system), make sure that Ubuntu 14.04 LTS Server edition is installed and operating, as listed in <xref href="../../commercial/GA1/1.0commercial.install-GA-prereqs.dita#ubuntu" type="section" >Installation: Prerequisites</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>Make sure you have met all the hardware requirements and have completed the required tasks before you begin your installation. The following sections walk you through the steps to be executed on the seed VM host:</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>Before you begin your installation on the seed VM host, if your environment uses a proxy server to access the Internet, configure the proxy information using the following steps:</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>Launch a terminal and log in to your seed VM host as root:</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:export no_proxy=localhost,127.0.0.1,&lt;your 10.x IP address&gt;
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>Log out and re-login to the seed VM host to activate the proxy configuration.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>Make sure you are logged into the seed VM host as root. If not:</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed |&amp; tee seedinstall.log
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<b>Note:</b> If <codeph>hp_ced_host_manager.sh</codeph> fails to start the seed, restart the installation (step 1) and then follow the rest of the steps.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>When prompted for host authentication, type <codeph>yes</codeph> to allow the SSH connection to proceed.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>If you are integrating LDAP into your environment, copy the configuration files, as described in <xref href="../../commercial/GA1/1.0commercial.install-GA-LDAP.dita" >Integrating LDAP</xref>, to the seed VM host.</p>
./commercial/GA1/1.0commercial.install-GA-kvm.dita:        OVERCLOUD_IP=$(jq '.overcloud.endpointhost' /root/tripleo/ce_env.json); echo ${OVERCLOUD_IP}
./commercial/GA1/1.0commercial.install-GA-kvm.dita:From the seed cloud host, you can connect to the demo VM using the following steps:
./commercial/GA1/1.0commercial.install-GA-kvm.dita:<p>HPE Helion OpenStack defaults to VxLAN to support tenant network isolation in a KVM Cloud Type. You can configure VLAN on HPE Helion OpenStack to enable communication with tenant's virtual machines hosted in a legacy infrastructure and/or based on VMWare ESX.</p>
./commercial/GA1/1.0commercial.install-GA-LDAP.dita:                            <entry>ldap://localhost</entry>
./commercial/GA1/1.0commercial.install-GA-LDAP.dita:                            <entry>ldap://localhost</entry>
./commercial/GA1/1.0commercial.install-GA-LDAP.dita:            <p>You need to copy the configuration files to the seed VM host during the installation,
./commercial/GA1/1.0commercial.install-GA-LDAP.dita:            <p>On the seed VM host, perform the following:</p>
./commercial/GA1/1.0commercial.install-GA-LDAP.dita:                        "value": "ldap://localhost"
./commercial/GA1/1.0commercial.install-GA-NTP.dita:sudo nmap -p123 -sU -P0 &lt;localhost | known ntp host&gt;
./commercial/GA1/1.0commercial.install-GA-NTP.dita:<p>HPE Helion OpenStack uses stratum 10. If your NTP stratum is lower than 10, set up your host as the time source by fudging a stratum 10.</p>
./commercial/GA1/1.0commercial.install-GA-NTP.dita:<p>Configure host as a time source.</p>
./commercial/GA1/1.0commercial.install-GA-overview.dita:        <li>1 seed cloud host (installer system)</li>
./commercial/GA1/1.0commercial.install-GA-overview.dita:                  compute proxy on the ESX hosts</xref> and <xref
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.0: Deploying and Configuring OVSvApp on ESX hosts</title>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<li>Uploading the OVSvApp file to one of the ESX hosts in your data center.</li>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<li>Adding your settings to the configuration file so that the OVSvApp deployment script can clone the file on each host being managed by the overcloud controller.</li>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>The following topics in this section explain how to deploy and verify deployment of OVSvApp for VCN on ESX hosts.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<xref type="section" href="#topic23018/uninstallvcn">Uninstalling VCN on ESX hosts</xref>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Please make sure that ESX host does not have another iteration of the OVSvApp already deployed.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>The ESX host must be reachable from the server where OVSvApp VM installation is launched. The ipaddress of the ESX hosts should be the same ipaddress with which the vCenter server manages that host. For more information see <xref href="1.0commercial.install-GA-prereqs.dita#network_prepare" type="section">Preparing the network for an ESX installation</xref> in <i>Prerequisites</i>.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>All ESX hosts must have synchronized time settings. If hosts have different time, the deployment will fail.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Use the vShpere client to select <b>Disable: Allow VM power on operations that violate availability constraints</b> as a part of cluster settings, as shaown. If not, ESX host might hang at 2% during transition to maintenance mode.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>If VDS will be configured automatically (<codeph>is_auto_dvs = True</codeph>) the installer requires one physical NIC name as input. This physical NIC must be unused(not part of any VSS or VDS) and its name should be same across all ESX hosts within a datacenter.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>The traffic between two tenant VMs on the same network and on the same ESX Compute host cannot be blocked. If custom security groups are used, add explicit security group rules to allow traffic between the VMs, regardless of the compute host they are provisioned on. Using rules to allow traffic will help maintain VM connectivity.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>You must upload the OVSvApp appliance to one of the ESX hosts that is hosting VMs provisioned from HPE Helion OpenStack environment. You must then configure the settings in the configuration file. The file can be used to clone and deploy OVSvApp on each host being managed by the controller.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<li>Specify distributed virtual switch (VDS) ports in the <codeph>ovs_vapp.ini</codeph>. Make sure the VDS ports are attached with the proper hosts.</li>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Use the vSphere client to upload the <codeph>overcloud_esx_ovsvapp.ova</codeph> file to one of the ESX hosts in your data center:</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#Clusters on which OVSvAPP will be hosted
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:[new-host-addition]
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:add_new_hosts=
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#Hosts in the given cluster are already added to DVS ? True if already part of DVS. False If you want to add.
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:host_in_dvs=
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#If host_in_dvs=False then Except *OPTIONAL each and every other fields are mandatory.
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#Unused Physical NIC (same nic across all hosts) to be used for uplink DVS. Make sure no VSS or VDS is using this NIC(*Not required if is_auto_dvs=False).
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<b>Note</b>: During deployment, the <codeph>ovs_vm_name</codeph> setting is appended with each VM host name and IP address to appear as <codeph>&lt;ovs_vm_name&gt;_&lt;IP&gt;</codeph>.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#RabbitMQ host(Mulitple hosts can be given by comma separated value)
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:rabbitmq_host=
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#If set to True(If you have a DRS enabled cluster), then on OVSvAPP crash/kernel panic the host will be put to maintenance mode.
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:#Maintenance mode will trigger DRS to migrate the tenant VMS. If set to false, then esx host will be shut down along with all tenant VMs. (*OPTIONAL)
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<li>If set to true, OVSvApp VM is powered off and the ESX host is put in Maintenance mode.</li>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<li>If set to false, the ESX host will be shut down along with all tenant VMs.</li>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>After the OVSvApp deployment script executes successfully, you can see the OVSvApp deployed on all the specified ESX hosts.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Login to the overcloud controller from the seed VM host:</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>During installation of OVSvApp VMs on a large scale, OVSvApp VM can hang and installation might not proceed. If this happens, execute the <codeph>neutron agent list</codeph> command. If the output shows a OVSvApp VM in the <codeph>xxx</codeph> agent state, rerun the installation for that specific failed OVSvApp VM by specifying the ESX host name in the <codeph>new_hosts</codeph> field under the <codeph>new-host-addition</codeph> section of the <codeph>ovs_vapp.ini</codeph> file.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>If DRS and HA are enabled on the cluster, VMs except OVSvApp VM will migrate to other ESX hosts.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>If the <codeph>neutron agent list</codeph> command shows a specific OVSvApp agent up and running, but you see an ESX host in maintenance mode, you can disable agent monitoring for the OVSvApp solution. To disable agent monitoring, add a flag <codeph>enable_agent_monitor</codeph> set to <codeph>false</codeph> as <codeph>enable_agent_monitor = false</codeph> to the <codeph>/etc/neuton/neutron.conf</codeph> file. Restart the server to activate the value.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>The VM port binding is with the host name of the OVSvApp VM on the ESX Compute host which provisioned the tenant VM.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>For Helion ESX type install, do not attempt to revert the Overcloud nodes from 1.01 to 1.0 through the restore process. Restoring will power down the ESX hosts associated with the registered vCenter cluster. <!-- ALM 11335 --></p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Note: You do not need to specify the <codeph>Clusters on which OVSvApp will be hosted</codeph> value.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<section id="uninstallvcn"> <title>Uninstall OVSvApp VM on ESX hosts</title>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>To uninstall VCN on ESX hosts, access the ESX hosts from vSphere Client, and delete each OVSvApp VM.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Disable vMotion from vSwitch properties. This will prevent DRS from bringing back VMs on the host when the host is brought back from maintenance mode as in Step 4.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>a. In the vSphere client, select the host in the vSphere Client inventory.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>c. Click <b>Virtual Switch</b> to display the virtual switches for the host.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Place the ESX host on which the 1.01 version of OVSvApp will be installed into maintenance mode :</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>In the vSphere Client, right click on the ESX host and select <b>Enter Maintenance mode</b>.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>All virtual machines on the host are migrated to different hosts when the host enters maintenance mode.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>In the vSphere Client, right click on the ESX host and select <b>Exit Maintenance mode</b>.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Install 1.01 version of OVSvApp VM on that ESX host using the <codeph>add_new_hosts</codeph> variable under the <codeph>new-host-addition</codeph> section in <codeph>ovs_vapp.ini</codeph> file</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<codeph>add_new_hosts=True
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>Re-enable vMotion on vSwitch properties of that ESX host.</p>
./commercial/GA1/1.0commercial.install-GA-ovsvapp.dita:<p>a. In the vSphere Client, right click on the ESX host.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<xref type="section" href="#topic14044/installer">Preparing the seed cloud host</xref>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<xref type="section" href="#topic14044/installer">Preparing the seed cloud host</xref>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>The machine hosting the seed VM, and all baremetal systems have to be connected to a management network.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>Nodes on this management network must be able to reach the ILOS of the baremetal systems to enable host reboots as part of the install process.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>Ensure network interfaces that are not used for PXE boot are disabled from BIOS to prevent PXE boot attempts from those devices.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<section id="installer"> <title>Preparing the seed cloud host</title>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>The following tasks need to be performed on the seed cloud host, where the seed VM will be installed. The seed cloud host is alternatively known as the installer system.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>The seed cloud host must have Ubuntu 14.04 LTS installed before performing the HPE Helion OpenStack installation.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>On the seed cloud host, the OpenSSH server must be running and the firewall
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>On the seed cloud host, the user <codeph>root</codeph> must have a public key, for example:</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>Before you start the installation, you must install NTP on the seed cloud host (installation system) and configure it as a NTP server. You will configure the undercloud and overcloud systems as NTP clients during the installation process.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>For information on installing NTP on the seed cloud host, see HPE Helion <xref href="../../commercial/GA1/1.0commercial.install-GA-NTP.dita" >OpenStack Installation: NTP Server</xref>.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>Log in to your seed cloud host as root:</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>Copy the installation package to the seed cloud host.</p>
./commercial/GA1/1.0commercial.install-GA-prereqs.dita:<p>During the installation process after the seed VM is installed, the installer script looks for information about the baremetal systems. Specifically, it looks for this information in a file called <codeph>baremetal.csv</codeph>. Before you begin the installation process, you must create this file and upload the file to the installer system (called the seed cloud host) at the appropriate installation step.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<p>External VLAN - Used for binding a routable address to a Compute (Nova) VM launched in Helion. Compute VMs are hosted in a Compute Node.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<p>Management VLAN - Every baremetal host has an address on this network for in-band management purposes.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<p>To protect against external attacks on Helion services, your firewall should be configured with a rule to block any requests originating from outside the network that attempts to reach any of the HPE Helion OpenStack nodes or any 3PAR StoreServ  or StoreVirtual VSA appliances dedicated to the HPE Helion OpenStack installation, other than those indicated this table:</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<p>The customer deploying HPE Helion OpenStack is responsible for securing the block storage networks. Network data flows for block storage should be restricted using access control lists or other mechanisms in the customer's network devices which can include routers, switches, or firewalls. Block storage data flows interacting with HPE Helion OpenStack are described here to assist with defining those controls. References are given to documentation on data flows within the storage cluster itself, but not necessarily interacting with HPE Helion OpenStack nodes.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>Cloud Controller (Cinder host)</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>Cloud Controller (Cinder host)</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>CMC to StoreVirtual <!--A BR tag was used here in the original source.-->Recommended to install on the seed cloud host</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<p>HPE Helion Openstack supports iSCSI or Fibre Channel connectivity with 3PAR StoreServ. If using Fibre Channel, then the Compute nodes and the overcloud controller hosting Block Storage (Cinder) will require Fibre Channel connectivity with the 3PAR array. For iSCSI, connectivity will be through the management VLAN. The StoreServ REST API and SSH command line interfaces must be accessible from the management VLAN as well.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<b>Note:</b> In the following table, the Volume Operation host refers to the overcloud controller that hosts the Volume Operations (Cinder) service.</p>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.0commercial.install-GA-security.dita:<li>Traffic between the OVSvApp VMs running on every ESX Host to communicate with the Network Operations message queue on the overcloud controller</li>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<xref type="section" href="#topic7297/otherseed">seed cloud host requirements and recommendations</xref>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<p>Host Interconnects/Protocols:</p>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<li>Capable of hosting VMs</li>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<li>seed cloud host configured in UTC (Coordinated Universal Time)</li>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<p>For Compute nodes, Intel or AMD hardware virtualization support required. The CPU cores and memory requirements must be sized based on the VM instances hosted by the Compute node.</p>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<entry morerows="3"> Seed Cloud Host </entry>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<entry> 1TB - This host will store the downloaded images as well as act as a host where backup data is preserved.</entry>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<entry>32GB - Memory must be sized based on the VM instances hosted by the Compute node.</entry>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<entry>8 CPU cores - Intel or AMD 64-bit processor with hardware virtualization support. The CPU cores must be sized based on the VM instances hosted by the Compute node. </entry>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<p>Software requirements for the Seed Cloud Host:</p>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<section id="otherseed"> <title>Other seed cloud host requirements and recommendations</title>
./commercial/GA1/1.0commercial.install-GA-supportmatrix.dita:<p>Other requirements and recommendations for the seed cloud host are as follows:</p>
./commercial/GA1/1.0commercial.install-GA-verify.dita:            <p>From the seed cloud host, connect to the undercloud Horizon console.</p>
./commercial/GA1/1.0commercial.install-GA-verify.dita:                    <p>Point your web browser on the seed cloud host to the undercloud Horizon
./commercial/GA1/1.0commercial.install-GA-verify.dita:            <p>From the seed cloud host, connect to the overcloud Horizon console.</p>
./commercial/GA1/1.0commercial.install-GA-verify.dita:                    <p>Point your web browser on the seed cloud host to the overcloud Horizon
./commercial/GA1/1.0commercial.install-GA-verify.dita:            <p>From the seed cloud host, you can connect to the demo VM using the following
./commercial/GA1/1.0commercial.install-GA-verify.dita:                        cloud host to the following IP address, using the undercloud IP address from
./commercial/GA1/1.0commercial.install-GA-verify.dita:                        host to the following IP address, using the undercloud IP address from the
./commercial/GA1/1.0commercial.install-GA-verify.dita:                    <p>a. From the seed cloud host log in to the undercloud as super user:</p>
./commercial/GA1/1.0commercial.install-GA-vsa.dita:<p>We recommend that you install CMC on the same KVM host that is used to run the seed VM. This host has direct network connectivity to servers running HPE StoreVirtual VSA. However, you may select an alternate host as long as it is accessible from the HPE Helion OpenStack management network.</p>
./commercial/GA1/1.0commercial.install-GA-vsa.dita:<p>The message "<i>Started VM vsa-hostname</i>" indicates the successful installation of StoreVirtual on the machine.The IP Address of the StoreVirtual storage system can be retrieved from this log file.</p>
./commercial/GA1/1.0commercial.map.volumetype.dita:  <image href="../../media/view-extra-specs-hos-1.1.png" placement="break"/> Create Volume Type Extra Specs Dialog Box 
./commercial/GA1/1.0commercial.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:host                 | icehousecompute               |
./commercial/GA1/1.0commercial.networking-maskedIP.dita:    | hostId                               | 091ce2ae798d669b1ec9cc53 ...    |
./commercial/GA1/1.0commercial.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:hypervisor_hostname  | icehousecompute.example.com   |
./commercial/GA1/1.0commercial.release-notes.dita:<b>Support for VMWare ESX</b> - VMWare ESX is a baremetal hypervisor. HPE Helion OpenStack makes ESX host onboarding and management easier and lets you setup the ESX proxy node during installation of the overcloud.</p>
./commercial/GA1/1.0commercial.release-notes.dita:<p>To potentially avoid this issue, when creating users and projects, .  first create the project, then create user(s), and add those users to the project (rather than creating the user first, then creating a project).<!--(HORI-3110) --></p>
./commercial/GA1/1.0commercial.release-notes.dita:The programs included with the Debian Host Linux system are free software. The exact license terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian Host Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.</p>
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:<codeblock><codeph> # sirius register-storevirtual-cluster -name=&lt;CLUSTER_NAME&gt; --hostname=&amp;lt;CLUSTER_IP_ADDRESS&gt; --subnet=&lt;SUBNET&gt; --username=&amp;lt;USERNAME&gt; --password=&amp;lt;PASSWORD&gt; --port=&amp;lt;SSH_PORT&gt;
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:| hostname   | 10.1.192.47                          |
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:| hplefthand_api_url            | https://10.1.192.47:8081/lhos                                                                     |
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:hplefthand_api_url=https://10.1.192.47:8081/lhos
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:<codeblock><codeph># sirius register-storeserv --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:| hostname     | 15.214.241.21                        |
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:        "hplefthand_api_url": "https://10.1.197.47:8081/lhos",
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:        "hplefthand_api_url": "https://10.1.244.41:8081/lhos",
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:            "hplefthand_api_url": "https://10.1.197.47:8081/lhos",
./commercial/GA1/1.0commercial.sirius-cli-workflow.dita:            "hplefthand_api_url": "https://10.1.244.41:8081/lhos",
./commercial/GA1/1.0commercial.sirius-cli.dita:  <codeph># sirius register-storevirtual-cluster -name=&lt;CLUSTER_NAME&gt; --hostname=&lt;CLUSTER_IP_ADDRESS&gt;  --subnet=&lt;SUBNET&gt; --username=&lt;USERNAME&gt; --password=&lt;PASSWORD&gt; --port=&lt;SSH_PORT&gt;
./commercial/GA1/1.0commercial.sirius-cli.dita:  <codeph># sirius register-storeserv --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;
./commercial/GA1/1.0commercial.sirius-cli.dita:<i>Optional Arguments</i>:   --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;</p>
./commercial/GA1/1.0commercial.technical-overview.ga.dita:    <entry>HPE Helion OpenStack Distributed Virtual Routing (DVR) allows you to define connectivity among different Virtual Network Switches (VNS) as well as connectivity between VNS hosts and the external network. HPE Helion OpenStack provides Distributed Virtual Routing to cloud users.
./commercial/GA1/1.0commercial.technical-overview.ga.dita:<li>Capable of hosting VMs</li>
./commercial/GA1/1.0commercial.technical-overview.ga.dita:<p>A seed VM host, also called the installer system, to run the baremetal install and host the Seed VM with the following configuration:</p>
./commercial/GA1/1.0commercial.technical-overview.ga.dita:    <entry>This network connects the traffic between OVSvApp VMs running on every ESX Host, Network Operations service and the vCenter Proxy that exists for every vCenter, and vCenter Proxy to communicate with the message queue for Block Storage and Network Operations. Also, connects EON to communicate with the vCenter server. </entry>
./commercial/GA1/1.0commercial.technical-overview.ga.dita:The Seed VM is expected to use eth0 to connect to the cluster network (and hence through to the management network). If your host uses another NIC, for example eth1, then you need to set the environment variable appropriately, for example BRIDGE_INTERFACE=eth1, as seen by root.
./commercial/GA1/1.0commercial.technical-overview.ga.dita:    The host server running the seed VM is also used to run backup restore procedures for the seed VM, undercloud and overcloud. The seed VM is also used to run the StoreVirtual CMC Management Console.
./commercial/GA1/1.0commercial.technical-overview.ga.dita:<b>Note:</b> You cannot build or rebuild the images. Direct editing of the Orchestration (Heat) templates is possible, but not supported. Configuration is limited to those items supported by the configuration tool and Horizon.</p>
./commercial/GA1/1.0commercial.technical-overview.ga.dita:<p>The administrator can monitor the availability of all hosts and services in the overcloud using the <xref href="https://www.icinga.org/" scope="external" format="html" >Icinga Monitoring server</xref> deployed in the undercloud.</p>
./commercial/GA1/1.0commercial.troubleshooting.dita:  <codeph>      "host-ip": "192.168.122.1", 
./commercial/GA1/1.0commercial.troubleshooting.dita:            remove those node(s) using following command.</p>
./commercial/GA1/1.0commercial.undercloud-eon-cli.dita:<section id="host-details"> <title>Host details<!--Removed anchor point host-details--><!-- id="host-details" --></title>
./commercial/GA1/1.0commercial.undercloud-eon-cli.dita:<p>You can view the list of hosts of cluster details of the host when cluster <codeph>moid</codeph> is specified.</p>
./commercial/GA1/1.0commercial.undercloud-eon-cli.dita:  <codeph> # eon host-list --vcenter-id &lt;VCENTER_ID&gt; [--clusters &lt;CLUSTER_MOIDS&gt; [&lt;CLUSTER_MOIDS&gt; ...]]
./commercial/GA1/1.0commercial.undercloud-oc-config-storeserv.dita:<b>Note</b>: Ensure that you allocate only those CPGs that will be used by this cloud. Changing any attributes of the CPG after allocation, may disrupt cloud functionality if the corresponding change is not updated in Sirius.</p>
./commercial/GA1/1.0commercial.undercloud-oc-config-storeserv.dita:<p>SSH to the Seed as root from KVM host using the IP address of seed VM as defined in the environment variables file:</p>
./commercial/GA1/1.0commercial.undercloud-oc-config-storevirtual.dita:<p>SSH to the Seed as root from KVM host using the IP address of seed VM as defined in the environment variables file:</p>
./commercial/GA1/1.0commercial.undercloud-oc-config-storevirtual.dita:      "hplefthand_api_url": "https://192.0.2.40:8081/lhos",
./commercial/GA1/1.0commercial.undercloud-resource-esx-manage-vm.dita:<p>vCenter provides centralized management of virtual host and virtual machines from a single console. You can register only three vCenters in the compute service, although a single administrator can manage multiple workloads.</p>
./commercial/GA1/1.0commercial.undercloud-resource-esx-manage-vm.dita:<li>In the <b>Compute Proxy Hostname</b> box, enter the name of the host name of the compute proxy.</li>
./commercial/GA1/1.0commercial.undercloud-resource-esx.dita:<p>The HPE Helion OpenStack Compute service provides a way to instantiate virtual machine instances on publicly accessible physical machines hosted in your cloud environment.</p>
./commercial/GA1/1.0commercial.undercloud-storage-storevirtual.dita:<li>Click the <b>More</b> drop-down list for the cluster whose configuration you want to view.</li>
./commercial/GA1/1.0commercial.update-101-monitor.dita:<p>Log in the seed VM host.</p>
./commercial/GA1/1.0commercial.update-101-monitor.dita:<p>Log in the seed VM host.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>If the undercloud needed updating, perform this update before updating the overcloud, as described in <xref href="../../commercial/GA1/1.0commercial.update-101-undercloud.dita" >Updating the Undercloud Host</xref>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>Point the install script to the overcloud. The patch update scripts are based on the Ansible platform. When patching the overcloud, because the script is launched from the seed cloud host, you need to point the script to the overcloud node.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>b. On the seed cloud host, copy the file to the seed host.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>c. Edit the <codeph>uc_stackrc</codeph> file to replace the localhost in the <codeph>OS_AUTH_URL</codeph> variable with the IP address of the undercloud.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>Log in the seed VM host:</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<b>Note:</b>  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to <xref type="section" href="#topic14215/redeploy">Redeploy Compute Proxy and OVSvAPP on ESX Host</xref> after updating Controller Management node and before proceeding to controller nodes.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<b>Note:</b>  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to <xref type="section" href="#topic14215/redeploy">Redeploy Compute Proxy and OVSvAPP on ESX Host</xref> after updating Controller Management node and before proceeding to controller nodes.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>b.  To get a list of hosts inside a zone:</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<codeph>nova host-list [--zone &lt;zone&gt;]
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>Migrating hosts is outside the scope of this document, however using the list and availability zone info a user can use techniques. See OpenStack documentation for <xref href="http://docs.openstack.org/admin-guide-cloud/content/section_live-migration-usage.html" scope="external" format="html" >Migrate instances</xref> and <xref href="http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html" scope="external" format="html" >Configure migrations</xref> to migrate workloads to a specific node.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<section id="redeploy"> <title>Redeploy Compute Proxy and OVSvAPP on ESX Host</title>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>If your infrastructure includes ESX proxy hosts, update the Compute Proxy and OVSvAPP after updating the overcloud controller management node. The update package contains new images for ESX. Failure to do so will prevent users from launching VMs in vCenter Hosts.
./commercial/GA1/1.0commercial.update-101-overcloud.dita:To manage the VMs launched using the older Compute proxy, use the same hostname entered for older Compute proxy.</p>
./commercial/GA1/1.0commercial.update-101-overcloud.dita:<p>Follow the steps in <xref href="../../commercial/GA1/1.0commercial.install-GA-ovsvapp.dita" >Deploying and Configuring OVSvApp on ESX hosts</xref> to install OVSvAPP, but use the latest <codeph>overcloud-esx-ovsvapp.ova</codeph> and <codeph>ovsvapp.tgz</codeph>.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>a. Edit the seed cloud host root directory. In the <codeph>/root/tripleo/tripleo-undercloud-passwords</codeph> file, modify the <codeph>UNDER_CLOUD_ADMIN_PASSWORD</codeph> value.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Prior to starting update you need to verify you are running Ubuntu that is greater than 3.13.0-36 on your seed cloud host machine.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Verifying and obtaining the correct kernel may vary depending on distribution.  From a command prompt seed cloud host run the following command to verify the operating system and current version:</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:1. Use SSH to access the seed cloud host:
./commercial/GA1/1.0commercial.update-101-prereqs.dita:        ssh heat-admin@<seed_cloud_host_IP>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:2. Copy the TAR file to the seed cloud host and extract contents. From an SSH session to the seed cloud host do the following:
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>The administrator should know the IP address of the seed cloud host node.  Other IP address can be obtained using the commands below.  Please record these for later use.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Log in the seed cloud host.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Log in the seed cloud host.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Log in the seed cloud host.</p>
./commercial/GA1/1.0commercial.update-101-prereqs.dita:<p>Log in the seed cloud host:</p>
./commercial/GA1/1.0commercial.update-101-seed.dita:<xref type="section" href="#topic4522/backup">Backup the seed cloud host</xref>
./commercial/GA1/1.0commercial.update-101-seed.dita:<section id="backup"> <title>Back up the seed cloud host</title>
./commercial/GA1/1.0commercial.update-101-seed.dita:1. Use SSH to access the seed cloud host:
./commercial/GA1/1.0commercial.update-101-seed.dita:        ssh root@<seed_cloud_host_IP>
./commercial/GA1/1.0commercial.update-101-seed.dita:<p>Copy the TAR file to the seed cloud host and extract contents. From an SSH session to the seed cloud host do the following:</p>
./commercial/GA1/1.0commercial.update-101-seed.dita:<codeph>ssh root@&lt;seed_cloud_host_IP&gt;
./commercial/GA1/1.0commercial.update-101-seed.dita:<p>Copy the TAR file to the KVM Host and extract contents. From an SSH session to the seed cloud host do the following:</p>
./commercial/GA1/1.0commercial.update-101-seed.dita:<codeph>ssh root@&lt;seed_cloud_host_IP&gt;
./commercial/GA1/1.0commercial.update-101-seed.dita:scp /tmp/heat_templates/seed_update_1.0.0-1.01.tar &lt;username&gt;@&lt;KVMHOST_IP&gt;:/tmp/
./commercial/GA1/1.0commercial.update-101-seed.dita:<p>Login to KVM Host and execute the following command:</p>
./commercial/GA1/1.0commercial.update-101-seed.dita:<p>Execute the <codeph>seed_update.sh</codeph> script to backup and copy the seed settings to host system:</p>
./commercial/GA1/1.0commercial.update-101-seed.dita:On the host node where the scripts were extracted, execute the script to restore seed settings on the updated seed.
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<codeph>ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:* FATAL: all hosts have already failed -- aborting
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<p>If restarting MySQL fails, then the database might be out of sync. Consult the MySQL error logs, located at /var/log/mysql/error.log.  In this case, never attempt to restart MySQL with <codeph>sudo /etc/init.d/mysql bootstrap-pxc</codeph> as it will bootstrap the host as a single node cluster thus worsening what already appears to be a split-brain scenario.</p>
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<p>Ansible uses SSH to communicate with remote nodes. In heavily loaded, single host virtualized environments, SSH can lose connectivity.  It should be noted that similar issues in a physical environment may indicate issues in the underlying network infrastructure.</p>
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:FATAL: all hosts have already failed - aborting
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<p>Early Ubuntu Trusty kernel versions have known issues with KVM that can impact SSH connectivity to instances. Test hosts should have a minimum kernel version of 3.13.0-36-generic.</p>
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<codeph>failed: [192.0.2.24] =&gt; {"changed": true, "cmd": "rabbitmqctl -n rabbit@$(hostname) status"
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<codeph>2014-10-23 23:03:51.302 31612 WARNING wsme.api [-] Client-side error: Node c241b0d5-abe1-4219-883c-e8dbaf1c5b35 is locked by host hLinux, please retry after the current operation is completed.
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<p>The file is located at: <codeph>/var/log/upstart/ironic-api.log</codeph>. Search for <codeph>locked by host hLinux</codeph>.</p>
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<codeph>2014-10-23 23:03:51.302 31612 WARNING wsme.api [-] Client-side error: Node c241b0d5-abe1-4219-883c-e8dbaf1c5b35 is locked by host hLinux, please retry after the current operation is completed.
./commercial/GA1/1.0commercial.update-101-troubleshooting.dita:<p>The file is located at: <codeph>/var/log/upstart/ironic-api.log</codeph>. Search for <codeph>locked by host hLinux</codeph>.</p>
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<p>If the seed VM needed updating, perform this update before updating the undercloud, as described in <xref href="../../commercial/GA1/1.0commercial.update-101-seed.dita" >Updating the Seed Cloud Host</xref>.</p>
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<p>Copy the TAR file to the seed cloud host and extract contents. From an SSH session to the seed cloud host do the following:</p>
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<codeph>ssh root@&lt;seed_cloud_host_IP&gt;
./commercial/GA1/1.0commercial.update-101-undercloud.dita:    If you are low on space and you updated the seed previously please refer to the [Cleanup section](/helion/openstack/update/seed/101/) in *Updating the Seed Cloud Host* for information on removing post-update files. The backup/restore process should remove these files. You can check to make sure the files have been removed. --></p>
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<p>Point the install script to the undercloud. The patch update script is based on the Ansible platform. For the undercloud, because the script is launched from the seed cloud host, you need to point the script to the seed cloud host.</p>
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<codeph>ssh root@&lt;seed_cloud_host_IP&gt;
./commercial/GA1/1.0commercial.update-101-undercloud.dita:b. On the seed cloud host, copy the `undercloud stackrc` file:
./commercial/GA1/1.0commercial.update-101-undercloud.dita:c. Edit the `uc_stackrc` file to replace the localhost in the `OS_AUTH_URL` variable with the IP address of the undercloud.  
./commercial/GA1/1.0commercial.update-101-undercloud.dita:<p>Log in the seed VM host.</p>
./commercial/GA1/1.0commercial.vlan-providernetwork.dita:<p>HPE Helion OpenStack defaults to VxLAN to support tenant network isolation in a KVM Cloud Type. <!---However, we need to deploy Helion Cloud to customers desiring to migrate gradually from legacy VLAN to VxLAN, a non-default install feature. This whitepaper walks through a way to configure Helion OpenStack tenant networks to use VLAN Provider Network.--> The deployment of HPE Helion OpenStack enables  tenant's virtual machines hosted in a legacy infrastructure and/or based on VMWare ESX to communicate to a virtual machine running in HPE Helion OpenStack <!---Typically, a Hybrid Application Deployment across two or more Infrastructure Providers (one being Helion OpenStack--></p>
./commercial/GA1/1.0commercial.vlan-providernetwork.dita:<p>Login to Seed VM Host</p>
./commercial/GA1/1.0commercial.vlan-providernetwork.dita:# bash -x tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed | tee  create-seed.log
./commercial/GA1/1.0commercial.vlan-providernetwork.dita:<p>Validate if you can ping the VMs from the KVM Host.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<xref href="../../commercial/GA1/1.0commerical.flexible-control-pane-overview.dita#kvmsetup" type="section">Set up KMV hosts</xref>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<li>On each of the three <xref href="../../commercial/GA1/1.0commerical.flexible-control-pane-overview.dita#kvmsetup" type="section">KVM hosts</xref> prepared earlier, create a <i>brbm</i> bridge and add the physical NIC which connects to the Helion Management network (that carries the PXE/DHCP, message queue, and other API-related traffic) along with other networks.</li>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>For example, on KVM Host A, the NIC is <b>eth1</b>:</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Similarly, for KVM Hosts B and C</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<li>Log in to KVM Host A.</li>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Copy the private and public keys to KVM Hosts B and C.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<li>Test and ensure that you can connect to Hosts B and C from A without having to provide any password.</li>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Download and extract the installer to the <i>/root</i> folder on KVM Host A.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Create a kvms.csv file with the details of the three KVM hosts:</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed |&amp; tee install.log
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Once the seed VM installation completes, you will observe that the process has created shell VMs on the 3 KVM hosts provided in the kvms.csv file.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>This process also creates a virtual power public key on KVM Host A. Copy this file to KVM hosts B and C.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>In order to ensure that the overcloud control plane nodes land on different KVM hosts to maintain HA, modify the baremetal.csv file and update the last column as shown below such that the overcloud controller nodes represented by the 2nd, 3rd and 4th row land on different KVM hosts.
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:- Undercloud Ironic service is unable to reach the KVM hosts
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:Ensure that you are able to connect to the KVM hosts and IPMI network from the undercloud node.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>The folder was not automatically copied onto the seed node from the KVM host.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Manually copy the contents of the <b>tripleo/config</b> folder from the KVM host onto the seed node.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-installation.dita:<p>Check that virtualization is enabled in the host BIOS and that host configuration is set up to load the KVM modules.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<xref type="section" href="#topic9437/kvmsetup">KVM Host Setup</xref>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>Currently, the Flexible Control Plane requires deployment on three KVM hosts.
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>In the deployment scenario in Figure 2, the cloud control plane is distributed across three KVM hosts with the deployment of VSA storage nodes and overcloud Nova compute nodes on physical servers.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<section id="kvmhost"> <title>KVMHost</title>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>The following configuration is used for the KVM host.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<section id="kvmsetup"> <title>KVM Host Setup</title>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>Set up KVM hosts and ensure all the hardware requirements are met and the required prerequisites are fulfilled before you begin your installation. Note that there are further prerequisites beyond those listed for HPE Helion OpenStack 1.01 mentioned previously.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<li>The following packages must be installed on all the KVM hosts. 
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<li>Set up the password-less logins between all the KVM hosts. This enables the root user of any KVM host to be able to log in via SSH to all other KVM hosts without a password.</li>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<li>Set up password-less login for the root user of the same KVM host. This enables the root user of the KVM host to be able to log in to the same KVM host without a password.</li>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<b>HP_ MULTI_ KVM</b>  This variable enables the heterogeneous environment to support the multiple hypervisors to host HPE Helion OpenStack Control Plane. The best practice is to set it to three (3).</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>The <i>uc_custom_flavors.json</i> file, which you will create later, during the installation process defined in the <xref href="../../commercial/GA1/1.0commerical.flexible-control-pane-installation.dita" >installation instructions</xref> is required to define the flavors that will be used during the deployment. This flavor information is added to the undercloud and used when deploying the control plane nodes as VMs on target KVM host.</p>
./commercial/GA1/1.0commerical.flexible-control-pane-overview.dita:<p>A kvms.csv file will also be created later via the <xref href="../../commercial/GA1/1.0commerical.flexible-control-pane-installation.dita" >installation instructions</xref>. It is required to define the KVM hosts being used for Flexible Control Plane deployment. This file contains the IP of the KVM host and an account with administrator or root privileges.</p>
./commercial/GA1/1.0commerical.high-availability.dita:<p>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual machine(VM) dies and the compute VM is lost along with its local ephemeral storage, the re-launching of the dead compute VM succeeds because it launches on another Nova-Compute KVM Hypervisor/server.</p>
./commercial/GA1/1.0commerical.high-availability.dita:<p>The nova-api service list, which is listening for requests on the IP of its host machine, then receives the request and deals with it accordingly. The database service is also accessed through the load balancer <!---(TODO: Section discussing database lockup issue with concurrent writes - this could require HA proxy always selecting a single node for access or just for writes, if possible)-->. RabbitMQ, on the other hand, is not currently accessed through VIP/HA proxy as the clients are configured with the set of nodes in the RabbitMQ cluster and failover between cluster nodes is automatically handled by the clients.</p>
./commercial/GA1/1.0commerical.high-availability.dita:<p>It is important for the overcloud HA setup to tolerate network failures, specifically those that result in a partition of the cluster, whereby one of the three nodes in the overcloud control plane cannot communicate with the remaining two nodes of the cluster. The description of network partition handling is separated into the main HA components of the overcloud.<!---**sentence seems to be incomplete. Please validate** ??--></p>
./commercial/GA1/1.0commerical.high-availability.dita:<p>Nova host aggregates and availability zones are not supported for general consumption in the current release.</p>
./commercial/GA1/1.0commerical.services-compute-overview.dita:<p>The HPE Helion OpenStack Compute Service leverages the  OpenStack compute service to instantiate virtual machine instances on publicly accessible physical machines hosted in your cloud environment.</p>
./commercial/GA1/1.0commerical.services-dvr-overview.dita:<p>Distributed Virtual Routing (DVR) allows you to define connectivity among different VNSs as well as connectivity between VNS hosts and the external network. HPE Helion OpenStack provides Distributed Virtual Routing to cloud users.</p>
./commercial/GA1/1.0commerical.services-dvr-overview.dita:To enable distributed routing this flag is enabled. It can be either **True** or **False**. If **False** is chosen, it works in the *Legacy mode*. If **True** is chosen, it works in the *DVR mode*.
./commercial/GA1/1.0commerical.services-identity-overview.dita:- **Role** -??- A set of rights and privileges that can be assigned to a user.  A user assuming that role inherits those rights and privileges. A role is also called a *personality*.
./commercial/GA1/1.0commerical.services-identity-overview.dita:- **Role** -??- A set of rights and privileges that can be assigned to a user.  A user assuming that role inherits those rights and privileges. A role is also called a *personality*. -->
./commercial/GA1/1.0commerical.services-ironic-overview.dita:<p>HPE Helion OpenStack leverages the OpenStack Ironic service during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./commercial/GA1/1.0commerical.services-object-overview.dita:<p>At its core, Swift is built from a set of software services and data constructs hosted on a cluster of servers.</p>
./commercial/GA1/1.0commerical.services-overview.dita:<b>Ironic</b>. The Ironic service runs during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./commercial/GA1/1.0commerical.services-overview.dita:<b>DVR</b>. Distributed Virtual Routing (DVR) allows you to define connectivity among different VNSs as well as connectivity between VNS hosts and the external network. HPE Helion OpenStack provides Distributed Virtual Routing to cloud users.</p>
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:<codeph>ssh root@&lt;seed_cloud_host_IP&gt;
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:# set a variable to the failed host name from above
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:failed_host=&lt;name&gt;
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-conductor --host=$failed_host
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-cert --host=$failed_host
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-scheduler --host=$failed_host
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-consoleauth --host=$failed_host
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:# show the monitored hosts
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:check_mk --list-hosts  
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:<p>On the controller node, execute the following command to view the log errors, especially those running <codeph>os-refresh-config</codeph>.</p>
./commercial/GA1/1.0commerical.services-remove-replace-failed-overcloud-nodes.dita:<p>On the seed node, verify the logs in <codeph>/var/log/upstart</codeph>, especially those from heat, nova, and ironic.</p>
./commercial/GA1/1.0commerical.services-scale-out-swift.dita:<p>It is recommended to use these Starter servers as host for the following Swift services for scaled out Swift cluster:</p>
./commercial/GA1/1.0commerical.services-scale-out-swift.dita:<p>For containers created before the deployment of scale-out Swift, default policy is policy-0.  Policy-0 is used to store objects. However, a new storage policy (termed as policy-1) is configured as soon as you deploy a scale-out Swift and it becomes the new default policy. After the deployment of scale-out Swift, all new containers are mapped to the new storage policy and  objects associated to those containers are stored in the ring with policy-1. For more details on storage policy, see <xref href="http://docs.openstack.org/developer/swift/overview_policies.html" scope="external" format="html" >OpenStack Swift overview policies</xref>.</p>
./commercial/GA1/1.0commerical.services-scale-out-swift.dita:- **Default policy.** The ***default*** policy can be any policy defined in the cluster. The default policy is automatically chosen when a container is created without a storage policy specified.
./commercial/GA1/1.0commerical.services-swift-deployment-add-proxy-node.dita:  server &lt;proxy node hostname&gt; &lt;proxy nodes IP address&gt;:8080 check inter 2000 rise 2 fall 5 
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                <li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                    displays as "<b><i>View Service Details For This Host</i></b>". The page
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-health-check.dita:                    navigates to Service Status Details For Host &lt;<b>Swift node IP address</b>>
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        href="../../media/icinga_host-details.png" id="image_qg3_fgv_ns"/><p>The
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                <li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                    displays as "<i><b>View Service Details For This Host</b></i>". <p> The page
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        navigates to Service Status Details For Host &lt;<b><i>Swift node IP
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                <li> Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                <li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                    displays as "<i><b>View Service Details For This Host</b></i>" (as shown in the
./commercial/GA1/1.0commerical.services-swift-deployment-monitor-swift-disk.dita:                    above image). The page navigates to Service Status Details For Host
./commercial/GA1/1.0commerical.services-swift-deployment-shrink-remove-proxy-node.dita:  server &lt;Proxy node hostname&gt; &lt;Proxy nodes IP address of &gt;:8080 check inter 2000 rise 2 fall 5 
./commercial/GA1/1.0commerical.services-swift-deployment.dita:The authenticity of host '192.0.2.29 (192.0.2.29)' can't be established.
./commercial/GA1/1.0commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<section id="download-the-hpssacli-debian-package-into-the-kvm-host"> <title>Download the hpssacli debian package into the KVM host</title>
./commercial/GA1/1.0commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<li>Copy the package from KVM host to
./commercial/GA1/1.0commerical.services-swift-diagnosis-disk-health-hpssacli.dita:          KVM_host<codeblock><codeph># scp details_slot_&lt;slot number&gt;.zip ubuntu@&lt;KVM_Host IP address&gt;:
./commercial/GA1/1.0commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<!-- **Now retrieve the ssd_report.zip to kvm host using scp from server to analyse.??? is this applicable for seed only??** -->
./commercial/GA1/1.1.1commercial.helion-update.dita:    <note> You can also deploy the HPE Helion OpenStack v1.1.1 release from scratch. For those
./commercial/GA1/1.1.1commercial.helion-update.dita:<p>The procedure will guide you through updating to Helion v1.1.1 from the seed host using
./commercial/GA1/1.1.1commercial.helion-update.dita:            seed host.</note> From <codeph>/root</codeph> on the seed host, create the
./commercial/GA1/1.1.1commercial.helion-update.dita:<li>To purge the known_hosts entries on the seed machine,
./commercial/GA1/1.1.1commercial.helion-update.dita:            will be prompted to re-add the seed to your known_hosts file when you SSH into the seed
./commercial/GA1/1.1.1commercial.helion-update.dita:{{#connect_host}}
./commercial/GA1/1.1.1commercial.helion-update.dita:accept = {{#accept_host}}{{{.}}}:{{/accept_host}}{{{accept}}}
./commercial/GA1/1.1.1commercial.helion-update.dita:connect = {{connect_host}}:{{connect}}
./commercial/GA1/1.1.1commercial.helion-update.dita:{{/connect_host}}
./commercial/GA1/1.1.1commercial.helion-update.dita:<li>Find the files named <codeph>host_metadata.js*</codeph> by
./commercial/GA1/1.1.1commercial.helion-update.dita:          entering:<codeblock><codeph>find / -name host_metadata.json* -ls
./commercial/GA1/1.1.1commercial.helion-update.dita:          entering:<codeblock><codeph>find / -name host_metadata.json* -delete
./commercial/GA1/1.1.1commercial.helion-update.dita:            entering:<codeblock><codeph>find / -name host_metadata.json* -ls
./commercial/GA1/1.1.1commercial.helion-update.dita:After running update on the overcloud, the VMs running on the Compute hosts are in a shutdown state, so you need to first start the HPE Helion Development Platform service control plane VMs. To do this: 
./commercial/GA1/1.1.1commercial.helion-update.dita:<section id="backup-failed-due-to-seed-host-running-out-of-disk-space"> <title>Backup failed due to seed running out of disk space</title>
./commercial/GA1/1.1.1commercial.helion-update.dita:<section id="seed-host-update-fails-noting-unable-to-ping-192021"> <title>Seed update fails noting unable to ping 192.0.2.1</title>
./commercial/GA1/1.1.1commercial.helion-update.dita:<codeblock><codeph>"Waiting for seed host to configure"
./commercial/GA1/1.1.1commercial.helion-update.dita:<p>Pings to host 192.0.2.1 will time out with the following message:</p>
./commercial/GA1/1.1.1commercial.helion-update.dita:and the host has failed to reboot. When this happens, you will get this error Message:</p>
./commercial/GA1/1.1.1commercial.helion-update.dita:<codeblock><codeph>ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
./commercial/GA1/1.1.1commercial.helion-update.dita:* `FATAL: all hosts have already failed  - aborting`
./commercial/GA1/1.1.1commercial.helion-update.dita:<p>If restarting MySQL fails, then the database is most certainly out of sync and the MySQL error logs, located at <codeph>/var/log/mysql/error.log</codeph>, will need to be checked.  In this case, never attempt to restart MySQL with <codeph>sudo /etc/init.d/mysql bootstrap-pxc</codeph> as it will bootstrap the host as a single-node cluster thus worsening what already appears to be a split-brain scenario. If you need help with this matter, contact HPE support.</p>
./commercial/GA1/1.1.1commercial.helion-update.dita:host virtualized environments, SSH can lose connectivity.  It should be noted
./commercial/GA1/1.1.1commercial.helion-update.dita:pipe FATAL: all hosts have already failed  - aborting
./commercial/GA1/1.1.1commercial.helion-update.dita:<p>Early Ubuntu Trusty kernel versions have known issues with KVM which severely impact SSH connectivity to instances. To avoid this issue, Test hosts should have a minimum kernel version of 3.13.0-36-generic.</p>
./commercial/GA1/1.1.1commercial.helion-update.dita:"rabbitmqctl -n rabbit@$(hostname) status" stderr: Error:
./commercial/GA1/1.1.1commercial.helion-update.dita:<codeblock><codeph>nova hypervisor-servers &lt;hostname&gt; 
./commercial/GA1/1.1.1commercial.helion-update.dita:<p>Return to the host from which you ran the upgrade and re-run the playbook without the <codeph>-e online_upgrade=True</codeph> option.</p>
./commercial/GA1/1.1.1commercial.helion-update.dita:<section id="overcloud-error-message-ansible-host-key-checkingfalse"> <title>Overcloud error message: ANSIBLE_HOST_KEY_CHECKING=False</title>
./commercial/GA1/1.1.1commercial.helion-update.dita:<codeblock><codeph>   Attempting to connect to each host
./commercial/GA1/1.1.1commercial.helion-update.dita:    + ANSIBLE_HOST_KEY_CHECKING=False
./commercial/GA1/1.1.1commercial.helion-update.dita:        self.parser = InventoryScript(filename=host_list)
./commercial/GA1/1.1.1commercial.helion-update.dita:        self.parser = InventoryScript(filename=host_list)
./commercial/GA1/1.1.1commercial.release-notes.dita:<b>Support for VMWare ESX</b> - VMWare ESX is a baremetal hypervisor. HPE Helion OpenStack makes ESX host onboarding and management easier and lets you setup the ESX proxy node during installation of the overcloud.</p>
./commercial/GA1/1.1.1commercial.release-notes.dita:<p>After installation, VMs hosted on a particular compute host are not pingable. To resolve this issue, restart the Neutron-Open vSwitch-agent on that compute host.</p>
./commercial/GA1/1.1.1commercial.release-notes.dita:<p>VMs scheduled on a particular host may not get their IP address in a timely manner. On that particular host, the neutron-openvswitch-agent process takes a large percentage of the CPU. Restarting the neutron services on that host will allow things to recover. <!-- Vnetcore-746 --></p>
./commercial/GA1/1.1.1commercial.release-notes.dita:The programs included with the Debian Host Linux system are free software. The exact license terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian Host Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.</p>
./commercial/GA1/1.1.1commerical.high-availability.dita:          <p>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual
./commercial/GA1/1.1.1commerical.high-availability.dita:      <p>The nova-api service list, which is listening for requests on the IP of its host machine,
./commercial/GA1/1.1.1commerical.high-availability.dita:      <p>It is important for the overcloud HA setup to tolerate network failures, specifically those
./commercial/GA1/1.1.1commerical.high-availability.dita:        service is deployed on the designated host, and only that host. If a failed controller0 is
./commercial/GA1/1.1.1commerical.high-availability.dita:        <li>Get the hostname of the new node. For controller1 this would be a unique name similar
./commercial/GA1/1.1.1commerical.high-availability.dita:          to:<codeblock><codeph>$ hostname
./commercial/GA1/1.1.1commerical.high-availability.dita:        <li>Export<codeblock><codeph>OVERCLOUD_CINDER_VOLUME_SINGLETON=&lt;hostname&gt;
./commercial/GA1/1.1.1commerical.high-availability.dita:      <p>Nova host aggregates and availability zones are not supported for general consumption in
./commercial/GA1/1.13rd-party-license-agreements.dita:  <xref href="http://gaf2871b9d2d13cf45c1306b35bf01764.cdn.hpcloudsvc.com/3rd%20party%20HOS%20V1.1%2002.25.15.pdf" scope="external" format="pdf">Download the PDF document.</xref>
./commercial/GA1/1.1commcerical.install-TLS.dita:Once the seed host is up, from the underseed or other node that can reach the seed, run these commands before continuing with the rest of the deployment according to the installation guide:</p>
./commercial/GA1/1.1commercial.-vsa-overview.dita:<p>The Cinder volume service hosts the LeftHand Driver to communicate with the backend representing the StoreVirtual cluster, using the LeftHand REST API.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>If either of the servers that host the two overcloud controllers (controller1 and controller2) fails, the overcloud controller must be rebuilt and reconnected into the cluster as soon as possible.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>The management controller, called controller0, is similar to the overcloud controller nodes, but it also executes various additional services, including Compute, Sherpa, Telemetry, Reporting, and Block Storage services. If the server that hosts the overcloud management controller fails, the management controller must be rebuilt and restored as soon as possible.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Note that the seed cloud host server is where the seed VM is installed and the installation files are located.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<li>When any change is make in the undercloud from the seed cloud host</li>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed VM host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<codeph>root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_restore.sh --seed -f /root/backup/backup_14-09-02-12-32 -c /root/export.prop
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-02-12-32
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-02-12-32
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>1.Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_backup.sh --undercloud -f /root/backup/
./commercial/GA1/1.1commercial.backup-restore-GA.dita:    Destination Host Folder: /root/backup/
./commercial/GA1/1.1commercial.backup-restore-GA.dita:    Warning: Permanently added '192.0.2.2' (ECDSA) to the list of known hosts.
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-03-12-30
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-03-12-30
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Destination Host Folder: /root/backup/
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Source Host Folder: /root/backup/backup_14-09-03-12-46
./commercial/GA1/1.1commercial.backup-restore-GA.dita:Backup from local host. Local Backup Folder is set to: /root/backup/backup_14-09-03-12-46
./commercial/GA1/1.1commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_backup.sh --help
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -f|--dest-host-folder - folder path to which to backup"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -H|--dest-host-ip     - ip of host to which to backup"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -u|--dest-host-user   - username of host to which to backup"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:  <codeph>    root@kvmhost:~/tripleo/tripleo-incubator/scripts# ./hp_ced_restore.sh --help
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -f|--source-host-folder - folder path from which to restore"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -H|--source-host-ip     - ip of host from which to restore"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -u|--source-host-user   - username of host from which to restore"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:        -c|--config-file        - will source this file on the host and/or Seed(Mandatory for --seed and --undercloud options)"
./commercial/GA1/1.1commercial.backup-restore-GA.dita:<li>Log in to the seed cloud host.</li>
./commercial/GA1/1.1commercial.dashboard.launch.dita:<p>Ask the cloud operator for the host name or public IP address where the dashboard is located, your user name, and your password.</p>
./commercial/GA1/1.1commercial.dashboard.launch.dita:<p>In the address bar, enter the host name or IP address for the dashboard.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>If a cloud compute node fails (due to a hardware malfunction for example), you can evacuate instances to make them available again. You can optionally include the target host on the <codeph>evacuate</codeph> command. If you omit the host, the scheduler determines the target host.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>To preserve user data on the server disk, you must configure shared storage on the target host. Also, you must validate that the current VM host is down; otherwise, the evacuation fails with an error.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>To list hosts and find a different host for the evacuated instance, run:</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<codeph>$ nova host-list
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Evacuate the instance. You can pass the instance password to the command by using the<codeph>--password &lt;pwd&gt;</codeph> option. If you do not specify a password, one is generated and printed after the command finishes successfully. The following command evacuates a server without shared storage from a host that is down, to the specified host_b:</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<codeph>$ nova evacuate evacuated_server_name host_b
./commercial/GA1/1.1commercial.disaster-recovery.dita:<codeph>$ nova evacuate evacuated_server_name host_b --on-shared-storage    
./commercial/GA1/1.1commercial.disaster-recovery.dita:Review host information</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Identify the VMs on the affected hosts, using tools such as a combination of <codeph>nova list</codeph> and <codeph>nova show</codeph> or <codeph>euca-describe-instances</codeph>. For example, the following output displays information about instance <codeph>i-000015b9</codeph> that is running on node <codeph>np-rcc54</codeph>:</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Review the status of the host by querying the Compute database. Important information is highlighted below. The following example converts an EC2 API instance ID into an OpenStack ID. If you used the nova commands, you can substitute the ID directly. You can find the credentials for your database in <codeph>/etc/nova.conf</codeph>.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:                hostname: at3-ui02
./commercial/GA1/1.1commercial.disaster-recovery.dita:                    host: np-rcc54
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>After you have determined the status of the VM on the failed host, decide to which compute host the affected VM should be moved. For example, run the following database command to move the VM to <codeph>np-rcc46</codeph>:</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<codeph>mysql&gt; UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Change the <codeph>DHCPSERVER</codeph> value to the host IP address of the compute host that is now the VM's new home.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>The above database update and <codeph>nova reboot</codeph> command are typically all that are required to recover a VM from a failed host. However, if further problems occur, consider looking at recreating the network filter configuration using <codeph>virsh</codeph>, restarting the Compute services or updating the <codeph>vm_state</codeph> and <codeph>power_state</codeph> in the Compute database.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>The following procedure runs on <codeph>nova-compute</codeph> hosts, based on the KVM hypervisor, and should restore the situation:</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Set the nova uid in <codeph>/etc/passwd</codeph> to the same number in all hosts (for example, 112).</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Set the <codeph>libvirt-qemu uid</codeph> in <codeph>/etc/passwd</codeph> to the same number in all hosts (for example, 119).</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Set the nova group in <codeph>/etc/group</codeph> file to the same number in all hosts (for example, 120).</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Set the <codeph>libvirtd</codeph> group in <codeph>/etc/group</codeph> file to the same number in all hosts (for example, 119).</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Repeat the steps for the <codeph>libvirt-qemu</codeph> owned files, if those need to change.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>In an active/passive configuration, systems are set up to bring additional resources online to replace those that have failed. For example, OpenStack would write to the main database while maintaining a disaster recovery database that can be brought online in the event that the main database fails.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>How to re-create all the network/bridges when the KVM host is rebooted.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<section id="kvm-host"> <title>KVM Host</title>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>First shut down the seed then manually access the OS and shut down the host.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Normally power on the KVM Host and then re-create the bridge/routers. For the steps supported in this release, please contact customer support.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Normally shut down the Seed VM and after that the KVM Host, to execute any necessary maintenance in the server.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Normally power on the KVM Host and after that the Seed VM.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Normally power on the server using <codeph>nova start</codeph> option in the seed/undercloud or manually start the server using iLO. After power on process ends, the VM`s hosted in the node will be automatically started.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>Start the KVM Host.</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>SSH to the KVM Host and start the Seed VM (<codeph>virsh</codeph> start seed).</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>i. Stop the KVM Host manually access the OS and shut down the machine</p>
./commercial/GA1/1.1commercial.disaster-recovery.dita:<p>a. Start the KVM Host</p>
./commercial/GA1/1.1commercial.eula.dita:<p>e Third Party and Open Source Components. To the extent any component of the software is subject to any third party license terms, including open source license terms, then those third party license terms or open source license terms shall govern with respect to the subject component; otherwise, the terms of this Agreement shall govern.</p>
./commercial/GA1/1.1commercial.eula.dita:<li>Per Server License: Following purchase, this per-server license is to be assigned to a Physical Server that will be used to run your Cloud Fabric.  After the Physical Servers are licensed and those licenses are properly assigned, you may run any number of instances of the Management Software to deploy, configure, manage and operate your Cloud Fabric.</li>
./commercial/GA1/1.1commercial.eula.dita:<b>b. Assigning Licenses:</b>  Before you can install and use the Management Software to deploy your Cloud Fabric, you must assign to each Physical Server running the host fabric one per-Physical Server license. Each Physical Server to which you assign a license is a licensed host server.</p>
./commercial/GA1/1.1commercial.faq.dita:<p>The undercloud also hosts images for various server types which will form the functional cloud environment - the overcloud. These images are overcloud Controller, overcloud Compute, overcloud Swift &amp; overcloud Compute Proxy (required for clouds that support VMWare ESX as a hypervisor).</p>
./commercial/GA1/1.1commercial.faq.dita:<p>Yes. It includes an integrated <tm tmtype="reg">Linux</tm> host OS hardened and tested for this distribution.</p>
./commercial/GA1/1.1commercial.faq.dita:<!-- removed per JR's comment I We are hosting the support discussion forum for the edition at [https://ask.openstack.org](https://ask.openstack.org).  Developers in the community are very familiar with this forum and already participate in OpenStack-related discussions there. Please tag your questions with 'HPHelion' to get our attention for any questions and issues you raise.--> 
./commercial/GA1/1.1commercial.install-3par.dita:<p>Install and configure the 3PAR StoreServ device and create Common Provisioning Groups (CPGs) which you are planning to use for the cloud as Cinder backend. The StoreServ device should be accessible from the management network of the cloud. If you are using Fibre Channel, ensure SAN connectivity between the compute host(s), the overcloud controller where the Volume Operations service is running, and the HPE 3PAR StoreServ array.</p>
./commercial/GA1/1.1commercial.install-add-nodes.dita:          node:<codeblock><codeph>nova-manage service disable --service=nova-compute --host=&lt;hostName of Compute Node&gt;
./commercial/GA1/1.1commercial.install-GA-apparmor.dita:<p>AppArmor profiles can modified, as needed, for your organization. However, the default profiles, as installed, have been configured to provide optimal protection for HPE Helion OpenStack All profiles are configured for <i>enforce</i> mode (for more information, see <xref type="section" href="#topic2805/modes">AppArmor profile modes</xref> below) and designed to mitigate hostile VMs exploiting hypervisor breakout attacks.</p>
./commercial/GA1/1.1commercial.install-GA-CSV.dita:<p>During the installation process after the seed VM is installed, the installer script looks for information about the baremetal systems. Specifically, it looks for this information in a file called <codeph>baremetal.csv</codeph>. Before you begin the installation process, you must create this file and upload the file to the installer system (called the seed cloud host) at the appropriate installation step.</p>
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:          <p>A chosen backend driver and its prerequisites:</p>
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              <p>PowerDNS (self hosted)<!--A BR tag was used here in the original source.--> A
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              <p>Microsoft DNS (self-hosted)</p>
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              short hostnames.</li>
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              <codeph>dynect_also_notify</codeph> - List of hostnames for name servers to notify
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              <b>RabbitMQ Hosts</b> - Enter the IP address for the Messaging Outputs, each entry on
./commercial/GA1/1.1commercial.install-GA-DNSaaS.dita:              file on the seed cloud host.</li>
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:<p>The HPE Helion OpenStack vCenter ESX compute proxy is a driver that enables the Compute service to communicate with a VMware vCenter server managing one or more ESX hosts. The HPE Helion OpenStack Compute Service (Nova) requires this driver to interface with VMWare ESX hypervisor APIs.</p>
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:# 1) hostname -&gt; should not have '_'    
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:hostname=
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:hostname = enter the name of the host name of the compute proxy
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:<p>A vCenter proxy VM named <codeph>hp_helion_vcenter_proxy</codeph> will be available in the specified vCenter. You can access that proxy VM from the seed VM host as the <codeph>heat-admin</codeph> user without password.</p>
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:3. Edit the [`compute_proxy.conf`](#undercloud) file to change the host-name of the vCenter (Compulsory) instead of `ip-address` in the `-??-ip-address=VCENTER_IP_ADDRESS` line.
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:4. If DNS name resolution is not available add a `/etc/hosts` entry for the vCenter ip address.
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:3. Provide the FQDN host-name of the vCenter instead of the ip-address in the **Server Address** field.
./commercial/GA1/1.1commercial.install-GA-ESX-Proxy.dita:<p>If you have not deployed the HPE Virtual Cloud Networking's Open vSwitch vApp (OVSvApp), see the <xref href="../../commercial/GA1/1.1commercial.install-GA-ovsvapp.dita" >Deploying and configuring OVSvApp for HPE Virtual Cloud Networking (VCN) on ESX hosts</xref> document for complete instructions.</p>
./commercial/GA1/1.1commercial.install-GA-esx.dita:        executed on the seed cloud host:</p>
./commercial/GA1/1.1commercial.install-GA-esx.dita:      <p>Follow the steps below to install the seed VM from the seed cloud host:</p>
./commercial/GA1/1.1commercial.install-GA-esx.dita:        <li>Make sure you are logged into the seed cloud host as root. If not:
./commercial/GA1/1.1commercial.install-GA-esx.dita:              command:<codeblock>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed |&amp; tee seedinstall.log
./commercial/GA1/1.1commercial.install-GA-esx.dita:            If <codeblock>hp_ced_host_manager.sh</codeblock> fails to start the seed, restart the
./commercial/GA1/1.1commercial.install-GA-esx.dita:        cloud host: </p>
./commercial/GA1/1.1commercial.install-GA-esx.dita:        <li>When prompted for host authentication, type <codeblock>yes</codeblock> to allow the SSH
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:                variables. Save the file on the seed cloud host (installation system). The variables
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:<p>Save the file on the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:                variables. Save the file on the seed cloud host (installation system). The variables
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:<p>Save the file on the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:                following environment variables. Save the file on the seed cloud host (installation
./commercial/GA1/1.1commercial.install-GA-JSON-scenarios.dita:<p>Save the file on the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<p>Save the file on the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:  <p>The default disk sizes per role are set as defined in the <xref href="../../commercial/GA1/1.1commercial.install-GA-supportmatrix.dita#topic12282/baremetal" type="section">Support Matrix</xref>. You should only set this value to override those defaults. You should deploy with at least the recommended disk size in all cases.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>bridge_interface</codeph> - Use this variable to specify the interface on the seed cloud host to use as the bridge interface, for example <codeph>em2</codeph> or <codeph>eth2</codeph>. This interface connects to the untagged management network and will be used to PXE boot undercloud and overcloud servers:</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>seed_server</codeph> - Use this variable to set the DNS name or IP address of an NTP server accessible on the public interface for undercloud hosts. This is required.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>undercloud_server</codeph> - Use this variable to set the DNS name or IP address of an NTP server accessible on the public interface for undercloud hosts. This is required.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>overcloud_server</codeph> - Use this variable to set the DNS name or IP address of an NTP server accessible on the public interface for overcloud hosts. This is required.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>vsa_public_interface</codeph> -  Use this variable to specify the physical interface used by a node which is hosting a VSA VM (for example: eth1).</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>network_gateway</codeph> - Use this variable to specify a host other than the seed cloud host as the gateway, for example 192.168.130.1. Typically this IP will be the physical gateway of the network.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:<codeph>network_seed_range_start</codeph>, <codeph>network_seed_range_end</codeph> - Use these variables to specify an IP address range for the seed cloud host to administrate/manage the undercloud node(s), for example 192.168.130.4-192.168.130.22.</p>
./commercial/GA1/1.1commercial.install-GA-JSON.dita:        non-zero host IP address in the <codeph>cidr</codeph> range.</note>
./commercial/GA1/1.1commercial.install-GA-kvm.dita:<p>Make sure you have met all the hardware requirements and have completed the required tasks before you begin your installation. The following sections walk you through the steps to be executed on the seed cloud host:</p>
./commercial/GA1/1.1commercial.install-GA-kvm.dita:<p>Make sure you are logged into the seed cloud host as root. If not:</p>
./commercial/GA1/1.1commercial.install-GA-kvm.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed |&amp; tee seedinstall.log
./commercial/GA1/1.1commercial.install-GA-kvm.dita:          <note>If <codeph>hp_ced_host_manager.sh</codeph> fails to start the seed, restart the
./commercial/GA1/1.1commercial.install-GA-kvm.dita:<p>If you are integrating LDAP into your environment, copy the configuration files, as described in <xref href="../../commercial/GA1/1.1commerical.services-identity-integrate-ldap.dita" >Integrating LDAP</xref>, to the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-kvm.dita:<p>When prompted for host authentication, type <codeph>yes</codeph> to allow the SSH connection to proceed.</p>
./commercial/GA1/1.1commercial.install-GA-NTP.dita:<p>This page provides detailed information on <xref type="section" href="#topic5226/client">configuring HPE Helion OpenStack nodes</xref> for Network Time Protocol (NTP) and <xref type="section" href="#topic5226/server">instlling an NTP server on the seed cloud host</xref>.</p>
./commercial/GA1/1.1commercial.install-GA-NTP.dita:<p>HPE Helion OpenStack requires that all nodes point to the same NTP server. You can use an external NTP server or configure the seed cloud host as a server.</p>
./commercial/GA1/1.1commercial.install-GA-NTP.dita:<p>Log in to the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-NTP.dita:<p>If your NTP stratum is numerically less than 10, set up your host as the time source by fudging a stratum 10.</p>
./commercial/GA1/1.1commercial.install-GA-NTP.dita:<p>Configure host as a time source.</p>
./commercial/GA1/1.1commercial.install-GA-overview.dita:<li>1 seed cloud host (installer system)</li>
./commercial/GA1/1.1commercial.install-GA-overview.dita:<p>You install HPE Helion OpenStack by first installing the seed cloud by running a single command. The host system for the seed cloud is called the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-overview.dita:<p>After installing the seed, launch a console session to the seed cloud host and execute a single command to install the undercloud and overcloud, with all the appropriate nodes.</p>
./commercial/GA1/1.1commercial.install-GA-overview.dita:<p>For ESX hypervisor support, after installing you will deploy the compute proxy on the ESX hosts and deploy OVSvApp for HPE Virtual Cloud Networking.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Deploying and Configuring OVSvApp on ESX hosts</title>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<searchtitle>HPE Helion Openstack 1.1: Deploying and Configuring OVSvApp on ESX hosts</searchtitle>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<li>Uploading the OVSvApp file to one of the ESX hosts in your data center.</li>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<li>Adding your settings to the configuration file so that the OVSvApp deployment script can clone the file on each host being managed by the overcloud controller.</li>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>The following topics in this section explain how to deploy and verify deployment of OVSvApp for VCN on ESX hosts.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Make sure that Python version is lower than 2.7.9 on the seed cloud host (the system from which the OVSvApp installation will be launched).</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Please make sure that ESX host does not have another iteration of the OVSvApp already deployed.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>The ESX host must be reachable from the server where OVSvApp VM installation is launched. The ipaddress of the ESX hosts should be the same ipaddress with which the vCenter server manages that host. For more information see <xref href="../../commercial/GA1/1.1commercial.install-GA-prereqs.dita#network_prepare" type="section">Preparing the network for an ESX installation</xref> in <i>Prerequisites</i>.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>All ESX hosts must have synchronized time settings. If hosts have different time, the deployment will fail.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Use the vShpere client to select <b>Disable: Allow VM power on operations that violate availability constraints</b> as a part of cluster settings, as shaown. If not, ESX host might hang at 2% during transition to maintenance mode.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>If VDS will be configured automatically (<codeph>auto_dvs = True</codeph>) the installer requires one physical NIC name as input. This physical NIC must be unused(not part of any VSS or VDS) and its name should be same across all ESX hosts within a datacenter.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>The traffic between two tenant VMs on the same network and on the same ESX Compute host cannot be blocked. If custom security groups are used, add explicit security group rules to allow traffic between the VMs, regardless of the compute host they are provisioned on. Using rules to allow traffic will help maintain VM connectivity.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:            <p>You must upload the OVSvApp appliance to one of the ESX hosts that is hosting VMs
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:                on each host being managed by the controller.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:                    proper hosts.</li>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Use the vSphere client to upload the <codeph>overcloud_esx_ovsvapp.ova</codeph> file to one of the ESX hosts in your data center:</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Clusters on which OVSvAPP will be hosted
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:[new-host-addition]
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:add_new_hosts=
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Hosts in the given cluster are already added to DVS ? True if already part of DVS. False If you want to add.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:host_in_dvs=
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#If host_in_dvs=False then Except *OPTIONAL each and every other fields are mandatory.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Unused Physical NIC (same nic across all hosts) to be used for uplink DVS. Make sure no VSS or VDS is using this NIC(*Not required if auto_dvs=False).
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:                        with each VM host name and IP address to appear as
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>e. Specify RabbitMQ settings. The <codeph>rabbitmq_host</codeph> should point to the overcloud controller's VIP</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#RabbitMQ host(Mulitple hosts can be given by comma separated value)
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:rabbitmq_host=
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#If set to True(If you have a DRS enabled cluster), then on OVSvAPP crash/kernel panic the host will be put to maintenance mode.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Maintenance mode will trigger DRS to migrate the tenant VMS. If set to false, then esx host will be shut down along with all tenant VMs. (*OPTIONAL)
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<li>If set to true, OVSvApp VM is powered off and the ESX host is put in Maintenance mode.</li>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<li>If set to false, the ESX host will be shut down along with all tenant VMs.</li>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Kibana Rabbit host for Centralized Logging
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:kibana_rabbit_host=
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Username for Kibana rabbit host
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Password for Kibana rabbit host
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>After the OVSvApp deployment script executes successfully, you can see the OVSvApp deployed on all the specified ESX hosts.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Login to the overcloud controller from the seed VM host:</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:                            hosted</codeph> value.</note>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>Use the following steps from the seed cloud host to update the OVSvApp from version 1.1 to version 1.1.1.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>During the update, the OVSvApp might not be able to communicate with the controller. Setting this parameter to False will prevent the ESX host from being put into maintenance mode.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>From the seed cloud host, copy <codeph>/root/helion-update-1.1-to-&#60;version&#62;/tripleo/hp-ovsvapp/</codeph> folder from seed VM using the following command:</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:#Clusters on which OVSvAPP will be hosted
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<p>e. Provide the Kibana Rabbit Host address and password in the [logger] section.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:<codeph>Fatal error: Needed to prompt for a connection or sudo password (host: &#60;ovs-vapp-ip&#62;), but input would be ambiguous in parallel mode
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:2. Disable vMotion from vSwitch properties. This will prevent DRS from bringing back VMs on the host when the host is brought back from maintenance mode as in Step 4. 
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    a. In the vSphere client, select the host in the vSphere Client inventory.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    c. Click **Virtual Switch** to display the virtual switches for the host.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:3. Place the ESX host on which the 1.01 version of OVSvApp will be installed into maintenance mode :
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    In the vSphere Client, right click on the ESX host and select **Enter Maintenance mode**.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    All virtual machines on the host are migrated to different hosts when the host enters maintenance mode.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    In the vSphere Client, right click on the ESX host and select **Exit Maintenance mode**.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:9. Install 1.01 version of OVSvApp VM on that ESX host using the `add_new_hosts` variable under the `new-host-addition` section in `ovs_vapp.ini` file
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:        add_new_hosts=True
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:11. Re-enable vMotion on vSwitch properties of that ESX host.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.dita:    a. In the vSphere Client, right click on the ESX host.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Deploying and Configuring OVSvApp on ESX hosts: Sample ovs_vapp.ini file</title>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:<searchtitle>HPE Helion Openstack 1.1: Deploying and Configuring OVSvApp on ESX hosts: Sample ovs_vapp.ini file</searchtitle>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:<p> <xref href="../../commercial/GA1/1.1commercial.install-GA-ovsvapp.dita" >â–²  Deploying and Configuring OVSvApp on ESX hosts</xref>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:<p>The following is a sample <codeph>ovs_vapp.ini</codeph> file for use with <xref href="../../commercial/GA1/1.1commercial.install-GA-ovsvapp.dita" >Deploying and Configuring OVSvApp on ESX hosts</xref>.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Clusters on which OVSvAPP will be hosted
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:[new-host-addition]
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:add_new_hosts=true_or_false
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Hosts in the given cluster are already added to DVS ? True if already part of DVS. False If you want to add.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:host_in_dvs=true_or_false
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#If host_in_dvs=False then Except *OPTIONAL each and every other fields are mandatory.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Unused Physical NIC (same nic across all hosts) to be used for uplink DVS. Make sure no VSS or VDS is using this NIC(*Not required if auto_dvs=False).
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#RabbitMQ host(Mulitple hosts can be given by comma separated value)
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:rabbitmq_host=rabbitmq_host_ipv4_address
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#If set to True(If you have a DRS enabled cluster), then on OVSvAPP crash/kernel panic the host will be put to maintenance mode.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Maintenance mode will trigger DRS to migrate the tenant VMS. If set to false, then esx host will be shut down along with all tenant VMs. (*OPTIONAL)
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Kibana Rabbit host for Centralized Logging
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:kibana_rabbit_host=kibana_rabbit_host_ipv4_address
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:#Password for Kibana rabbit host
./commercial/GA1/1.1commercial.install-GA-ovsvapp.sample.dita:kibana_rabbit_pwd=kibana_rabbit_host_password
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Updating OVSvApp on ESX hosts</title>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<searchtitle>HPE Helion Openstack 1.1: Deploying and Configuring OVSvApp on ESX hosts</searchtitle>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<p>e. Provide the Kibana Rabbit Host address and password in the [logger] section.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<codeph>Fatal error: Needed to prompt for a connection or sudo password (host: &lt;ovs-vapp-ip&gt;), but input would be ambiguous in parallel mode
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<p>Use the following steps from the seed cloud host to update the OVSvApp.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<p>During the update, the OVSvApp might not be able to communicate with the controller. Setting this parameter to False will prevent the ESX host from being put into maintenance mode.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<p>From the seed cloud host, copy <codeph>/root/helion-update-1.1-to-&lt;version&gt;/tripleo/hp-ovsvapp/</codeph> folder from seed VM using the following command:</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<p>e. Provide the Kibana Rabbit Host address and password in the [logger] section.</p>
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:<codeph>Fatal error: Needed to prompt for a connection or sudo password (host: &lt;ovs-vapp-ip&gt;), but input would be ambiguous in parallel mode
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:2. Disable vMotion from vSwitch properties. This will prevent DRS from bringing back VMs on the host when the host is brought back from maintenance mode as in Step 4. 
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    a. In the vSphere client, select the host in the vSphere Client inventory.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    c. Click **Virtual Switch** to display the virtual switches for the host.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:3. Place the ESX host on which the 1.01 version of OVSvApp will be installed into maintenance mode :
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    In the vSphere Client, right click on the ESX host and select **Enter Maintenance mode**.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    All virtual machines on the host are migrated to different hosts when the host enters maintenance mode.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    In the vSphere Client, right click on the ESX host and select **Exit Maintenance mode**.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:9. Install 1.01 version of OVSvApp VM on that ESX host using the `add_new_hosts` variable under the `new-host-addition` section in `ovs_vapp.ini` file
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:        add_new_hosts=True
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:11. Re-enable vMotion on vSwitch properties of that ESX host.
./commercial/GA1/1.1commercial.install-GA-ovsvapp.update.dita:    a. In the vSphere Client, right click on the ESX host.
./commercial/GA1/1.1commercial.install-GA-post-esx.dita:<p>The HPE Helion OpenStack vCenter ESX compute proxy is a driver that enables the Compute service to communicate with a VMware vCenter server that manages one or more ESX hosts. The HPE Helion OpenStack Compute service (Nova) requires this driver to interface with VMWare ESX hypervisor APIs.</p>
./commercial/GA1/1.1commercial.install-GA-post-kvm.dita:<p>HPE Helion OpenStack defaults to VxLAN to support tenant network isolation in a KVM Cloud Type. You can configure VLAN on HPE Helion OpenStack to enable communication with tenant's virtual machines hosted in a legacy infrastructure and/or based on VMWare ESX.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs-network.dita:<p>The machine hosting the seed VM, and all baremetal systems have to be connected to a management network.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs-network.dita:<p>Nodes on this management network must be able to reach the iLO subsystem (<xref href="http://www8.hp.com/us/en/products/servers/ilo/index.html" scope="external" format="html" >HPE Integreated Lights-Out</xref>) of each baremetal systems to enable host reboots as part of the install process.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs-network.dita:<p>Ensure network interfaces that are not used for PXE boot are disabled from BIOS to prevent PXE boot attempts from those devices.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<li>Preparing the seed cloud host:
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<li>seed cloud host configured in UTC (Coordinated Universal Time)</li>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<section id="seed"> <title>Preparing the seed cloud host</title>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>The following tasks need to be performed on the seed cloud host, where the seed VM will be installed. The seed cloud host is alternatively known as the installer system.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>The seed cloud host must have Ubuntu 14.04 LTS or Debian 8 installed before performing the HPE Helion OpenStack installation.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>On the seed cloud host, the OpenSSH server must be running and the firewall
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>On the seed cloud host, the user <codeph>root</codeph> must have a public key, for example:</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:  <p>For more information about the Debian or Ubuntu packages that are required for the seed cloud host, see <xref href="1.1commercial.install-GA-supportmatrix.dita#topic12282/software-requirements" type="section">Support Matrix</xref>.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>You can install NTP on the seed cloud host and configure it as an NTP server. Or, you can use a pre-existing NTP server that is reachable from the management network.  You will also need to configure the undercloud and overcloud systems as NTP clients pointing to the NTP server you have chosen to use during the installation process.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>For information on installing NTP on the seed cloud host, see <xref href="1.1commercial.install-GA-NTP.dita" >Installing an NTP Server</xref>.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>Before you begin your installation on the seed cloud host, if necessary configure the proxy information for your environment using the following steps:</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>Launch a terminal and log in to your seed cloud host as root:</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:export no_proxy=localhost,127.0.0.1,&lt;your 10.x IP address&gt;,&lt;provider_network&gt;
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>Log out and re-login to the seed cloud host to activate the proxy configuration.</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>Log in to your seed cloud host as root:</p>
./commercial/GA1/1.1commercial.install-GA-prereqs.dita:<p>Copy the installation package to the seed cloud host.</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:<p>External VLAN - Used for binding a routable address to a Compute (Nova) VM launched in Helion. Compute VMs are hosted in a Compute Node.</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:<p>Management VLAN - Every baremetal host has an address on this network for in-band management purposes.</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:<p>To protect against external attacks on Helion services, your firewall should be configured with a rule to block any requests originating from outside the network that attempts to reach any of the HPE Helion OpenStack nodes or any 3PAR StoreServ  or StoreVirtual VSA appliances dedicated to the HPE Helion OpenStack installation, other than those indicated this table:</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:<p>The customer deploying HPE Helion OpenStack is responsible for securing the block storage networks. Network data flows for block storage should be restricted using access control lists or other mechanisms in the customer's network devices which can include routers, switches, or firewalls. Block storage data flows interacting with HPE Helion OpenStack are described here to assist with defining those controls. References are given to documentation on data flows within the storage cluster itself, but not necessarily interacting with HPE Helion OpenStack nodes.</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:<entry colsep="1" rowsep="1">Cloud Controller (Cinder host)</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:<entry colsep="1" rowsep="1">Cloud Controller (Cinder host)</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:                seed cloud host</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:<p>HPE Helion Openstack supports iSCSI or Fibre Channel connectivity with 3PAR StoreServ. If using Fibre Channel, then the Compute nodes and the overcloud controller hosting Block Storage (Cinder) will require Fibre Channel connectivity with the 3PAR array. For iSCSI, connectivity will be through the management VLAN. The StoreServ REST API and SSH command line interfaces must be accessible from the management VLAN as well.</p>
./commercial/GA1/1.1commercial.install-GA-security.dita:      <note>In the following table, the Volume Operation host refers to the overcloud controller
./commercial/GA1/1.1commercial.install-GA-security.dita:        that hosts the Volume Operations (Cinder) service.</note>
./commercial/GA1/1.1commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/1.1commercial.install-GA-security.dita:<li>Traffic between the OVSvApp VMs running on every ESX Host to communicate with the Network Operations message queue on the overcloud controller</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:              Seed Cloud Host</xref>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE H240 12Gb 2-port Int Smart Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE H240ar 12Gb 1-port Int Smart Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE H241 12Gb 2-ports Ext Smart Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE H244br 12Gb 2-ports Int Smart Host Bus Adapter </li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE SN1100E 16Gb Single-Port Fibre Channel Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE SN1100E 16Gb Dual-Port Fibre Channel Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE StoreFabric SN1000Q 16GB 1-port PCIe Fibre Channel Host Bus Adapter</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE StoreFabric SN1000Q 16GB 2-port PCIe Fibre Channel Host Bus Adapter </li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE LPe1205A 8Gb Fibre Channel Host Bus Adapter for BladeSystem c-Class</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE QMH2572 8Gb Fibre Channel Host Bus Adapter for BladeSystem c-Class</li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:        <li>HPE QMH2672 16Gb Fibre Channel Host Bus Adapter </li>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:          <p>Host Interconnects/Protocols:</p>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:          <p>Capable of hosting VMs</p>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:            and memory requirements must be sized based on the VM instances hosted by the Compute
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:              <entry morerows="3">Seed Cloud Host</entry>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:              <entry>1TB - This host will store the downloaded images as well as act as a host where
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:              <entry>32GB - Memory must be sized based on the VM instances hosted by the Compute
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:                support. The CPU cores must be sized based on the VM instances hosted by the Compute
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:      <title>Software requirements for the seed cloud host</title>
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:      <p>The Seed Cloud Host is a computer running Ubuntu 14.04 or Debian 8. The following packages
./commercial/GA1/1.1commercial.install-GA-supportmatrix.dita:      <p>Other recommendations for the seed cloud host are as follows:</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>Point your web browser on the seed cloud host to the undercloud Horizon console using the <codeph>UNDERCLOUD_IP_ADDRESS</codeph> obtained after the install.</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>Point your web browser on the seed cloud host to the overcloud Horizon console using the <codeph>OVERCLOUD_IP_ADDRESS</codeph> obtained after the install.</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>From the seed cloud host, you can connect to the demo VM using the following steps:</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>To access the Icinga monitoring console, launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>To access the Kibana logging console, launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./commercial/GA1/1.1commercial.install-GA-verify.dita:<p>a. From the seed cloud host log in to the undercloud as super user:</p>
./commercial/GA1/1.1commercial.install-GA-virtual-control.dita:<codeph># HP_VM_MODE=y bash -x /myStore/myWork/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed
./commercial/GA1/1.1commercial.install-GA-virtual-control.dita:                <li>In the second KVM host, pre-create the required number of VMs and capture the
./commercial/GA1/1.1commercial.install-GA-virtual-control.dita:        HP_VM_MODE=y bash -x /myStore/myWork/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh -??-create-seed
./commercial/GA1/1.1commercial.install-GA-vsa.dita:<p>We recommend that you install CMC on the same KVM host that is used to run the seed VM. This host has direct network connectivity to servers running HPE StoreVirtual VSA. However, you may select an alternate host as long as it is accessible from the HPE Helion OpenStack management network.</p>
./commercial/GA1/1.1commercial.install-GA-vsa.dita:<p>Ensure that VSA host nodes boot from the local disk. If it boots from the remote disk then ensure that a single path is only enabled.</p>
./commercial/GA1/1.1commercial.install-GA-vsa.dita:            message "<i>Started VM vsa-hostname</i>" indicates the successful installation of
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<b>Diskspace</b>: Each VM has a <i>virtual disk</i> size of 512 GB. However, each VM may occupy from 16 GB to 27 GB, or more of real disk space on the host system. Diskspace usage grows over time. For this reason, you should ensure your system has enough free diskspace for the demo. </li>
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed 2>&gt;&amp;1|tee seedvminstall.log</codeph>
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<codeph>bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed 2>&gt;&amp;1|tee seedvminstall.log</codeph>
./commercial/GA1/1.1commercial.install-laptop-demo.dita:+ scp -o PasswordAuthentication=no -o StrictHostKeyChecking=no 
./commercial/GA1/1.1commercial.install-laptop-demo.dita:-o UserKnownHostsFile=/dev/null tripleo/tripleo-incubator/scripts/../../../baremetal.csv.vm root@192.168.122.120:baremetal.csv
./commercial/GA1/1.1commercial.install-laptop-demo.dita:Warning: Permanently added '192.168.122.120' (ECDSA) to the list of known hosts.
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<p>Before you shut down the laptop and if you would like to preserve the HPE Helion OpenStack installation, you can use the host manager script to save the status of the virtual machines. To do this, follow these steps:</p>
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<codeph>tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --save-vms
./commercial/GA1/1.1commercial.install-laptop-demo.dita:<codeph>tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --resume-vms
./commercial/GA1/1.1commercial.map.volumetype.dita:<image href="../../media/view-extra-specs-hos-1.1.png" placement="break"/>View Extra Specs Option 
./commercial/GA1/1.1commercial.network-administrator-notes.dita:<p>The OpenStack Block Storage service (code name Cinder) works through the interaction of a series of daemon processes named cinder-* that reside persistently on the host machine or machines.  For Helion, you have the option to configure backend support for:</p>
./commercial/GA1/1.1commercial.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:host                 | icehousecompute               |
./commercial/GA1/1.1commercial.networking-maskedIP.dita:    | hostId                               | 091ce2ae798d669b1ec9cc53 ...    |
./commercial/GA1/1.1commercial.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:hypervisor_hostname  | icehousecompute.example.com   |
./commercial/GA1/1.1commercial.release-notes.dita:<b>Support for VMWare ESX</b> - VMWare ESX is a baremetal hypervisor. HPE  Helion Openstack makes ESX host onboarding and management easier and lets you setup the ESX proxy node during installation of the overcloud.</p>
./commercial/GA1/1.1commercial.release-notes.dita:<p>To potentially avoid this issue, when creating users and projects, first create the project, then create user(s), and add those users to the project (rather than creating the user first, then creating a project).<!--(HORI-3110) --></p>
./commercial/GA1/1.1commercial.release-notes.dita:<p>Icinga monitoring reports Cinder false alarms. <!--core2977/2985-->The Icinga monitor for the Cinder volume service runs on all three Helion controller nodes (Controller0, 1 and 2), but the Cinder volume service only runs on Controller0. Icinga may send a "Critical Alert" that the Cinder volume service is not running on Controller1 and Controller2. This is a false alarm, since the service does not run on those controllers, and can be ignored; there is no actual impact to any service.</p>
./commercial/GA1/1.1commercial.release-notes.dita:<p>Live migration of an instance in stopped, suspended, or rescued state may fail. Make sure instances are not in the stopped, suspended, or rescued state before attempting to migrate them to a new host. A 400 error may appear; a message "Failed to migrate instance..." may be received; or the client may quietly time out, leaving the instance stuck in the migrating state and thus unmanageable. <!-- DOCS-823 --></p>
./commercial/GA1/1.1commercial.release-notes.dita:The programs included with the Debian Host Linux system are free software. The exact license terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian Host Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.</p>
./commercial/GA1/1.1commercial.replacing_starter_swift_node.dita:    <p>If scaleout Swift nodes are already deployed, copy the rings to those nodes. With this the
./commercial/GA1/1.1commercial.services-kibana.dita:<p>Launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./commercial/GA1/1.1commercial.services-kibana.dita:<p>a. From the seed cloud host log in to the undercloud as super user:</p>
./commercial/GA1/1.1commercial.services-volume-fibre.dita:<p>Zoning is a fabric-based service in a Storage Area Network (SAN), which enables you to group host and storage nodes that need to communicate. Zoning allows nodes to communicates with each other if they are the member of a same zone.</p>
./commercial/GA1/1.1commercial.services-volume-fibre.dita:* We recommend that you install CMC on the same KVM host that is used to run the seed VM. This host has direct network connectivity to servers running HPE StoreVirtual VSA. However, you may select an alternate host as long as it is accessible from the HPE Helion OpenStack management network.
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:  <codeph> # sirius register-storevirtual-cluster -name=&lt;CLUSTER_NAME&gt; --hostname=&lt;CLUSTER_IP_ADDRESS&gt; --subnet=&lt;SUBNET&gt; --username=&lt;USERNAME&gt; --password=&lt;PASSWORD&gt; --port=&lt;SSH_PORT&gt;
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:| hostname   | 10.1.192.47                          |
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:| hplefthand_api_url            | https://10.1.192.47:8081/lhos                                                                     |
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:hplefthand_api_url=https://10.1.192.47:8081/lhos
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:  <codeph># sirius register-storeserv --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:| hostname     | 15.214.241.21                        |
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:        "hplefthand_api_url": "https://10.1.197.47:8081/lhos",
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:        "hplefthand_api_url": "https://10.1.244.41:8081/lhos",
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:            "hplefthand_api_url": "https://10.1.197.47:8081/lhos",
./commercial/GA1/1.1commercial.sirius-cli-workflow.dita:            "hplefthand_api_url": "https://10.1.244.41:8081/lhos",
./commercial/GA1/1.1commercial.sirius-cli.dita:  <codeph># sirius register-storevirtual-cluster -name=&lt;CLUSTER_NAME&gt; --hostname=&lt;CLUSTER_IP_ADDRESS&gt;  --subnet=&lt;SUBNET&gt; --username=&lt;USERNAME&gt; --password=&lt;PASSWORD&gt; --port=&lt;SSH_PORT&gt;
./commercial/GA1/1.1commercial.sirius-cli.dita:  <codeph># sirius register-storeserv --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;
./commercial/GA1/1.1commercial.sirius-cli.dita:<i>Optional Arguments</i>:   --name &lt;STORESERV_NAME&gt; --hostname &lt;STORESERV_IP&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --port &lt;SSH_PORT&gt; --san-ip &lt;SAN_IP&gt; --san-username &lt;SAN_USERNAME&gt; --san-password &lt;SAN_PASSWORD&gt; --device-type &lt;DEVICE_TYPE&gt;</p>
./commercial/GA1/1.1commercial.technical-overview.ga.dita:                                hosts and the external network. HPE Helion OpenStack provides Distributed Virtual Routing to cloud
./commercial/GA1/1.1commercial.technical-overview.ga.dita:                                every ESX Host, Network Operations service and the vCenter Proxy
./commercial/GA1/1.1commercial.technical-overview.ga.dita:The Seed VM is expected to use eth0 to connect to the cluster network (and hence through to the management network). If your host uses another NIC, for example eth1, then you need to set the environment variable appropriately, for example BRIDGE_INTERFACE=eth1, as seen by root.
./commercial/GA1/1.1commercial.technical-overview.ga.dita:                                The host server running the seed VM is also used to run backup
./commercial/GA1/1.1commercial.technical-overview.ga.dita:                    those items supported by the configuration tool and Horizon.</note>
./commercial/GA1/1.1commercial.troubleshooting.controllers.dita:<p>On the seed node, verify the logs in <codeph>/var/log/</codeph> directories, especially those in the <codeph>heat</codeph>, <codeph>nova</codeph> and <codeph>ironic</codeph> subdirectories respectively.</p>
./commercial/GA1/1.1commercial.troubleshooting.dita:      <p>The command gathers specific information into a file that will help diagnose your issue. The file is located in the <codeph>/tmp</codeph> directory under a name in the format <codeph>sosreport-&lt;system hostname&gt;-&lt;timestamp&gt;.tar.xz</codeph>. For example: <codeph>/tmp/sosreport-hLinux-20150218083619.tar.xz</codeph>.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:The process of building the seed cloud host, creating the seed VM, the overcloud, and the undercloud are described at: 
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>For KVM installations: Make sure you have all the required software on the KVM host.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<li>Make sure you have qemu, openswitch, libvirt, and phython-libvert packages loaded on the KVM host. </li>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<section id="could-not-access-the-kvm-host-on-the-seed-vm"> <title>Could not access the KVM host on the seed VM</title>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:Also you can bypass the firewall by creating the VLAN interface on the KVM host.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>When you try to launch a VM, you get a <codeph>no valid host found</codeph> error. In this case:</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<xref type="section" href="#topic39140/seednoshow">Seed cloud host cannot be reached after it is rebooted</xref>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<codeph>"host-ip": "192.168.122.1", 
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>You can confirm this by looking at the overcloud hostname. If the overcloud nodes hostname is <codeph>hlinux</codeph> instead of the expected hostname, for example, <codeph>overcloud-ce-controller-controller0-26x25y6vuxrb</codeph> nodes are likely not getting their metadata from Nova.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>The following is an example of the expected messages. If the <codeph>nova-api.log</codeph> does not contain messages like this for each overcloud host, the metadata is not being transferred:</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>This problem can be caused by your use of <codeph>sudo ?E</codeph> when executing the <codeph>hp_ced_host_manager</codeph>. Run  <codeph>hp_ced_host_manager</codeph> without specifying <codeph>sudo ?E</codeph>.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>Debugging a Windows Server OS running as a Nova Compute VM on HPE Helion OpenStack will require setting up a serial connection between the VM and a debug host. Once a serial connection has been established between the VM and debug host, which is usually over the COM2 port, the Windows Debug utility will function properly.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<li>Deploy a <xref href="http://docs.openstack.org/user-guide-admin/content/specify-host-to-boot-instances-on.html" scope="external" format="html" >debug host on the same compute host</xref> as the target Windows instance.</li>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<li>The user will need to be able to log into the host with <i>superuser</i> privileges. To safely gain superuser privileges, add <codeph>sudo</codeph> as a prefix to each command executed.</li>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<li>Log on to the Compute hosts where the Windows instances (target and debug hosts) are deployed.</li>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>Install the <i>socat</i> utility on the compute host.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>The Debian package can be copied to the host and installed using:</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>or, if apt repositories are accessible from the host, use:</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>Get the serial source path of the target and the debug host VMs.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>Look in the <i>filename.xml</i> file for the value of the <b>Source Path</b> for the target host. This value is found under the <codeph>Serial type='pty'</codeph> section. <!--A BR tag was used here in the original source.-->Example:</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>Repeat these steps to retrieve the serial source path value for the debug host.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<section id="seednoshow"> <title>The seed cloud host cannot be reached after it is rebooted</title>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>After the seed cloud host is rebooted, the networking setup does not persist and the seed cloud host cannot be reached.</p>
./commercial/GA1/1.1commercial.troubleshooting.install.dita:<p>The networking setup on the FCP seed cloud host must be set up in a particular way. Otherwise, the seed cloud host cannot be reached after it is rebooted.</p>
./commercial/GA1/1.1commercial.troubleshooting.logging.dita:  <codeph>curl http://localhost:9200/_cat/indices?v
./commercial/GA1/1.1commercial.troubleshooting.logging.dita:  <codeph>curl -X DELETE http://localhost:9200/[index name]
./commercial/GA1/1.1commercial.troubleshooting.mysql.dita:<p>Tell the cluster that this node is the bootstrap host</p>
./commercial/GA1/1.1commercial.troubleshooting.ntp.dita:sudo nmap -p123 -sU -P0 &lt;localhost | known ntp host&gt;
./commercial/GA1/1.1commercial.troubleshooting.overcloud.dita:<p>to remove the node's host file</p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<xref href="../../commercial/GA1/1.1commercial.troubleshooting.dita" > â–² Troubleshooting</xref> | <xref href="../../commercial/GA1/1.1commercial.install-GA-ovsvapp.dita" > â–² Deploying and Configuring OVSvApp on ESX hosts</xref> </p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<p>During installation of OVSvApp VMs on a large scale, OVSvApp VM can hang and installation might not proceed. If this happens, execute the <codeph>neutron agent list</codeph> command. If the output shows a OVSvApp VM in the <codeph>xxx</codeph> agent state, rerun the installation for that specific failed OVSvApp VM by specifying the ESX host name in the <codeph>new_hosts</codeph> field under the <codeph>new-host-addition</codeph> section of the <codeph>ovs_vapp.ini</codeph> file.</p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<p>If DRS and HA are enabled on the cluster, VMs except OVSvApp VM will migrate to other ESX hosts.</p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<p>If the <codeph>neutron agent list</codeph> command shows a specific OVSvApp agent up and running, but you see an ESX host in maintenance mode, you can disable agent monitoring for the OVSvApp solution. To disable agent monitoring, add a flag <codeph>enable_agent_monitor</codeph> set to <codeph>false</codeph> as <codeph>enable_agent_monitor = false</codeph> to the <codeph>/etc/neuton/neutron.conf</codeph> file. Restart the server to activate the value.</p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<p>The VM port binding is with the host name of the OVSvApp VM on the ESX Compute host which provisioned the tenant VM.</p>
./commercial/GA1/1.1commercial.troubleshooting.ovsvapp.dita:<p>For Helion ESX type install, do not attempt to revert the Overcloud nodes from 1.01 to 1.0 through the restore process. Restoring will power down the ESX hosts associated with the registered vCenter cluster. <!-- ALM 11335 --></p>
./commercial/GA1/1.1commercial.troubleshooting.swift.dita:          those node(s) using following
./commercial/GA1/1.1commercial.troubleshooting.vsa.dita:<p>If newly added node is in ERROR state or if the maintenance mode is True then remove those node(s) using the following command.</p>
./commercial/GA1/1.1commercial.undercloud-eon-cli.dita:<section id="host-details"> <title>Host details<!--Removed anchor point host-details--><!-- id="host-details" --></title>
./commercial/GA1/1.1commercial.undercloud-eon-cli.dita:<p>You can view the list of hosts of cluster details of the host when cluster <codeph>moid</codeph> is specified.</p>
./commercial/GA1/1.1commercial.undercloud-eon-cli.dita:<codeblock><codeph> # eon host-list --vcenter-id &lt;VCENTER_ID&gt; [--clusters &lt;CLUSTER_MOIDS&gt; [&lt;CLUSTER_MOIDS&gt; ...]]
./commercial/GA1/1.1commercial.undercloud-oc-config-storeserv.dita:            <note>Ensure that you allocate only those CPGs that will be used by this cloud. Changing
./commercial/GA1/1.1commercial.undercloud-oc-config-storeserv.dita:<p>SSH to the Seed as root from KVM host using the IP address of seed VM as defined in the environment variables file:</p>
./commercial/GA1/1.1commercial.undercloud-oc-config-storevirtual.dita:<p>SSH to the Seed as root from KVM host using the IP address of seed VM as defined in the environment variables file:</p>
./commercial/GA1/1.1commercial.undercloud-oc-config-storevirtual.dita:      "hplefthand_api_url": "https://192.0.2.40:8081/lhos",
./commercial/GA1/1.1commercial.undercloud-resource-esx-manage-vm.dita:<p>vCenter provides centralized management of virtual host and virtual machines from a single console. You can register only three vCenters in the compute service, although a single administrator can manage multiple workloads.</p>
./commercial/GA1/1.1commercial.undercloud-resource-esx-manage-vm.dita:<li>In the <b>Hostname</b> box, enter the hostname of the compute proxy.</li>
./commercial/GA1/1.1commercial.undercloud-resource-esx.dita:<p>The HPE Helion OpenStack Compute service provides a way to instantiate virtual machine instances on publicly accessible physical machines hosted in your cloud environment.</p>
./commercial/GA1/1.1commercial.undercloud-storage-storevirtual.dita:<li>Click the <b>More</b> drop-down list for the cluster whose configuration you want to view.</li>
./commercial/GA1/1.1commercial.vlan-providernetwork.dita:<p>HPE Helion OpenStack defaults to VxLAN to support tenant network isolation in a KVM Cloud Type. <!---However, we need to deploy Helion Cloud to customers desiring to migrate gradually from legacy VLAN to VxLAN, a non-default install feature. This whitepaper walks through a way to configure Helion OpenStack tenant networks to use VLAN Provider Network.--> The deployment of HPE Helion OpenStack enables  tenant's virtual machines hosted in a legacy infrastructure and/or based on VMWare ESX to communicate to a virtual machine running in HPE Helion OpenStack <!---Typically, a Hybrid Application Deployment across two or more Infrastructure Providers (one being Helion OpenStack--></p>
./commercial/GA1/1.1commercial.vlan-providernetwork.dita:<p>Login to Seed VM Host</p>
./commercial/GA1/1.1commercial.vlan-providernetwork.dita:# bash -x tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed | tee  create-seed.log
./commercial/GA1/1.1commercial.vlan-providernetwork.dita:          <p>Validate if you can ping the VMs from the KVM host.</p>
./commercial/GA1/1.1commercial.whatsnew.dita:Allows an HPE Helion OpenStack installation on a mix of VMs and physical nodes, support for multiple KVM hosts, and a mechanism for specifying which host a VMs is created on. All HA scenarios are still supported. Note that the flexible control plane has been implemented in this release for evaluation and feedback purposes only and is not supported as part of a production environment. <!--hpm441--></p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<xref href="1.1commerical.flexible-control-pane-overview.dita#kvmsetup" type="section">Set up KVM hosts</xref>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>The installer will create a bridge on each host and a port for the external device will be added to the bridge. The installer will also move the IP address of the external device to the bridge device.
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>In the same manner, establish the value of BRIDGE_INTERFACE for each remote host (see below).</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>Set up the remote hosts. You must set up each remote host identified in your vm-plan file.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>a. Use an existing user or create a new user on <i>each</i> remote host to run the virtual machines. The user must have sufficient privileges to launch virtual machines using libvirt, for example, as a member of the 'libvirtd' group.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>This user is denoted &lt;kvmuser&gt; below and the host is denoted &lt;hvmhost&gt;.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>b. Create a virtual power key on the seed's host. As root, run this command on the seed's host:</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:$ bash /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --local-setup --vm-plan vm-plan
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>This command will perform all prerequisite system checks and create a Secure Shell (SSH) key that can be used for virtual power operations between hosts, if a key does not previously exist.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>c. Append the key to the &lt;kvmuser&gt;'s authorized_keys file on <i>each</i> remote host.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<codeph> $ scp /root/.ssh/id_rsa_virt_power.pub &lt;kvmuser&gt;@&lt;kvmhost&gt;:
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita: $ ssh &lt;kvmuser&gt;@&lt;kvmhost&gt;
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>d. Copy hp_ced_host_manager.sh to <i>each</i> remote host.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<codeph>For example: $ scp /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh \
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:     &lt;kvmuser&gt;@&lt;kvmhost&gt;:hp_ced_host_manager.sh
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>e. Copy hp_ced_ensure_host_bridge.sh to <i>each</i> remote host.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<codeph>For example: $ scp /root/work/tripleo/tripleo-incubator/scripts/hp_ced_ensure_host_bridge.sh \
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:     &lt;kvmuser&gt;@&lt;kvmhost&gt;:hp_ced_ensure_host_bridge.sh
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>f. As root on <i>each</i> remote host, run:</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:bash -x ~&lt;kvmuser&gt;/hp_ced_host_manager.sh --remote-setup
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:You must execute this command as root on the seed's host.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:bash -x /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh \
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>Check that virtualization is enabled in the host BIOS and that host configuration is set up to load the KVM modules.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<section id="a-seed-cloud-host-has-been-rebooted-but-the-vms-hosting-the-control-plane-did-not-start-automatically"> <title>A seed cloud host has been rebooted but the VMs hosting the control plane did not start automatically</title>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>A seed cloud host has been rebooted but the VMs hosting the control plane did not start automatically.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>The seed cloud host responsible for hosting the seed node has been rebooted. This requires manually restarting the VMs.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:  <b>Restart the VMs on the seed's KVM host</b>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>Recover the seed by running the appropriate <codeph>hp_ced_host_manager.sh</codeph> command with the --boot-seed argument.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<codeph>SEED_NTP_SERVER=12.12.12.12 bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --boot-seed</codeph>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:For example, use the same <codeph>hp_ced_host_manager.sh</codeph> used during installation but substitute '--create-seed'.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:  <b>Restart the VMs on a KVM host not running the seed</b>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<section id="the-seed-cloud-host-cannot-be-reached-after-it-is-rebooted"> <title>The seed cloud host cannot be reached after it is rebooted</title>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>After the seed cloud host is rebooted, the networking setup does not persist and the seed cloud host cannot be reached.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-installation.dita:<p>The networking setup on the FCP seed cloud host must be set up in a particular way. Otherwise, the seed cloud host cannot be reached after it is rebooted.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<xref type="section" href="#topic6925/kvmsetup">KVM Host Setup</xref>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<p>Only one deployment architecture is possible, using three KVM hosts.
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<p>In the deployment scenario in Figure 2, the cloud control plane is distributed across three KVM hosts with the deployment of VSA storage nodes and overcloud Nova compute nodes on physical servers.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<image href="../../media/FCP_Figure2_HOS1.1.png" placement="break"/>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<section id="kvmsetup"> <title>KVM Host</title>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<p>For nodes other than KVM Hosts, refer to the <xref href="../../commercial/GA1/1.1commercial.install-GA-supportmatrix.dita" >Support Matrix</xref> documentation.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:   their hosts. This is an example vm-plan file and its format mirrors that of
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<p>Field 1 is the BRIDGE_INTERFACE nic on the remote host.</p>
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:            used on the seed host)
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:<p>Field 4 is the IP address of the remote KVM host.
./commercial/GA1/1.1commerical.flexible-control-pane-overview.dita:        (If empty, this will be the local host do not use 'localhost'.)</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Backup and Restore Flexible Control Plane Hosts</title>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<!-- taken from http://wiki.hpcloud.net/display/core/Helion+1.1+FCP+KVM+Hosts+backup+and+recovery+procedure -->
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>The backup and restore process involves running commands on each of the three servers that host the HPE Helion OpenStack components. The backup process requires an additional server to store backed-up files.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:        Only one deployment architecture is possible, using three physical hosts.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<li>one physical system hosts the seed VM, undercloud controller node, and one of the three overcloud controllers (<codeph>Overcloud-0</codeph>); </li>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<li>one physical system hosts one of the three overcloud controller nodes (<codeph>Overcloud-1</codeph>) and one of two Swift storage nodes (<codeph>Swift-Storage-0</codeph>); </li>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<li>one physical system hosts one of the three overcloud controller nodes (<codeph>Overcloud-2</codeph>); and one of two Swift storage nodes (<codeph>Swift-Storage-1</codeph>). </li>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Backing up and restoring the host where the following components run:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Backing up and restoring the host where the following components run:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Backing up and restoring the host where the following components run:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>View the <codeph>baremetal.csv</codeph> file found on the seed cloud host using the following command:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>View the <codeph>vm-plan</codeph> file available on the seed cloud host using the following command:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<section id="seed"> <title>Back up the seed cloud host, seed VM, undercloud node, and Overcloud-0 node</title>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>This procedure will back up the seed cloud host, the seed VM, Undercloud node and <codeph>Overcloud-0</codeph> node.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Log into the seed cloud host as root:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>SSH to the seed VM, undercloud node, and Overcloud-0 node host:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>where &lt;IP_address&gt; is the address of the host.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>tar SPzcvf - /root/.ssh /root/tripleo /root/baremetal.csv /tmp/vm-plan /var/lib/libvirt/images/baremetal_0.qcow2 /var/lib/libvirt/images/baremetal_3.qcow2 /var/lib/libvirt/images/seed.qcow2 /root/dom-xml-config | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh  ssh &lt;user&gt;@&lt;IP_address&gt; "cat &gt; /backup/kvm-seed-host-$NOW.enc"
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>The backup of seed host, undercloud, and <codeph>Overcloud-0</codeph> is complete.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>/root/hp_ced_host_manager.sh</codeph> </li>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>/root/hp_ced_ensure_host_bridge.sh</codeph>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Log into the seed cloud host as root.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>SSH to the Overcloud-1 and Swift Storage host.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>where &lt;IP_address&gt; is the address of the host.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>tar SPzcvf - /var/lib/libvirt/images/baremetal_1.qcow2 /var/lib/libvirt/images/baremetal_4.qcow2 /root/dom-xml-config /root/hp_ced_host_manager.sh /root/hp_ced_ensure_host_bridge.sh /root/id_rsa_virt_power.pub | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh &lt;user&gt;@&lt;IP_address&gt; "NOW=$(date +"%T-%m-%d-%Y") cat &gt; /backup/kvm-oc-1-sw-st-1-host-$NOW.enc"
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>The backup of Overcloud-1 and Swift-Storage-0 KVM host is complete.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>/root/hp_ced_host_manager.sh</codeph> </li>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>/root/hp_ced_ensure_host_bridge.sh</codeph>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Log into the seed cloud host as root.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>SSH to the Overcloud-2 and Swift Storage host:</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>where &lt;IP_address&gt; is the address of the host.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>tar SPzcvf - /root/.ssh /var/lib/libvirt/images/baremetal_2.qcow2 /var/lib/libvirt/images/baremetal_5.qcow2 /root/dom-xml-config /root/hp_ced_host_manager.sh /root/hp_ced_ensure_host_bridge.sh /root/id_rsa_virt_power.pub | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh &lt;user&gt;@&lt;IP_address&gt; "NOW=$(date +"%T-%m-%d-%Y") cat &gt; /backup/kvm-oc-2-sw-st-1-host-$NOW.enc"
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>The backup of Overcloud-2 and Swift-Storage-1 KVM host is complete.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>The seed cloud host must have Linux for HPE Helion Ubuntu 14.04 LTS or Debian 8 installed before performing the restore procedure.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>On the seed cloud host, the OpenSSH server must be running and the firewall configuration should allow access to the SSH ports.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Use the following section to restore the host where the seed VM, undercloud, and overcloud
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Log into the seed cloud host as root.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>cat /backup/kvm-seed-host-backup-date.enc | ssh heat-admin@&lt;IP_address&gt; 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Use the following steps to restore the physical KVM host where Overcloud Controller-1 and Swift Storage-0 VM will run.</p>
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>cat /backup/kvm-seed-host-backup-date.enc | ssh heat-admin@&lt;IP_address&gt; 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<p>Use the following steps to restore the physical KVM host where Overcloud Controller-2 and Swift
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>backup-node# cat /backup/kvm-oc-2-sw-st-1-host-date.enc | ssh heat-admin@&lt;IP_address&gt; 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'
./commercial/GA1/1.1commerical.flexible-control-plane-backup.dita:<codeph>bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>Power on the seed cloud host. Ensure that the networking is operating, by logging into the seed cloud host remotely.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>If you cannot access the seed cloud host through the external interface, run the following command to restart the Open vSwitch (OVS) service:</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --boot-seed
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>The BRIDGE_INTERFACE command sets the interface on the seed cloud host to use as the bridge interface, for example em2 or eth2.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>Once the undercloud is up, use the following command to verify the host name.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:<codeph>hostname
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>When the undercloud is properly configured, the host name is similar to <codeph>undercloud-undercloud-rweuuzkaj4</codeph> and not <codeph>hlinux</codeph>.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>Once the overcloud is up, use the following command to verify the host name.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:<codeph>hostname
./commercial/GA1/1.1commerical.high-availability-restore.dita:<p>When the overcloud is properly configured, the host name is similar to <codeph>overcloud-ce-controller-rweuuzkaj4</codeph> and not <codeph>hlinux</codeph>.</p>
./commercial/GA1/1.1commerical.high-availability-restore.dita:### Start the seed cloud host
./commercial/GA1/1.1commerical.high-availability-restore.dita:2. SSH to the KVM Host and start the Seed VM (virsh start seed)
./commercial/GA1/1.1commerical.high-availability.dita:<p>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual machine(VM) dies and the compute VM is lost along with its local ephemeral storage, the re-launching of the dead compute VM succeeds because it launches on another Nova-Compute KVM Hypervisor/server.</p>
./commercial/GA1/1.1commerical.high-availability.dita:<p>The nova-api service list, which is listening for requests on the IP of its host machine, then receives the request and deals with it accordingly. The database service is also accessed through the load balancer <!---(TODO: Section discussing database lockup issue with concurrent writes - this could require HA proxy always selecting a single node for access or just for writes, if possible)-->. RabbitMQ, on the other hand, is not currently accessed through VIP/HA proxy as the clients are configured with the set of nodes in the RabbitMQ cluster and failover between cluster nodes is automatically handled by the clients.</p>
./commercial/GA1/1.1commerical.high-availability.dita:<p>It is important for the overcloud HA setup to tolerate network failures, specifically those that result in a partition of the cluster, whereby one of the three nodes in the overcloud control plane cannot communicate with the remaining two nodes of the cluster. The description of network partition handling is separated into the main HA components of the overcloud.<!---**sentence seems to be incomplete. Please validate** ??--></p>
./commercial/GA1/1.1commerical.high-availability.dita:<p>Nova host aggregates and availability zones are not supported for general consumption in the current release.</p>
./commercial/GA1/1.1commerical.services-compute-overview.dita:<p>The HPE Helion OpenStack Compute Service leverages the OpenStack Nova compute service to instantiate virtual machine instances on publicly accessible physical machines hosted in your cloud environment.</p>
./commercial/GA1/1.1commerical.services-dvr-overview.dita:<p>Distributed Virtual Routing (DVR) allows you to define connectivity among different VNSs as well as connectivity between VNS hosts and the external network. HPE Helion OpenStack provides Distributed Virtual Routing to cloud users.</p>
./commercial/GA1/1.1commerical.services-dvr-overview.dita:To enable distributed routing this flag is enabled. It can be either **True** or **False**. If **False** is chosen, it works in the *Legacy mode*. If **True** is chosen, it works in the *DVR mode*.
./commercial/GA1/1.1commerical.services-identity-configure-v3.dita:<codeph>OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
./commercial/GA1/1.1commerical.services-identity-configure-v3.dita:<p>The Keystone v3 endpoint is in the format: <codeph>http://&lt;host&gt;:&lt;port&gt;/v3</codeph> and is the same host/port as the v2 endpoint. The <codeph>local_settings.py</codeph> file will have the endpoint for v2 by default.</p>
./commercial/GA1/1.1commerical.services-identity-domains.dita:<p>Each domain defines a namespace where certain API-visible name attributes exist, which affects whether those names must be globally unique or unique within that domain.</p>
./commercial/GA1/1.1commerical.services-identity-integrate-ldap.dita:                            <entry>ldap://localhost</entry>
./commercial/GA1/1.1commerical.services-identity-integrate-ldap.dita:                            <entry>ldap://localhost</entry>
./commercial/GA1/1.1commerical.services-identity-integrate-ldap.dita:<p>The name is important and is always of the form: keystone.&lt;domain-name&gt;.conf, where the domain name must match the name of the domain created in Keystone. Options provided in the domain-specific configuration file will override those in the primary configuration file (i.e. keystone.conf) for the specified domain.</p>
./commercial/GA1/1.1commerical.services-identity-integrate-ldap.dita:url = ldap://localhost
./commercial/GA1/1.1commerical.services-identity-overview.dita:        project level. A user assuming that role inherits those rights and privileges. A role is
./commercial/GA1/1.1commerical.services-ironic-overview.dita:<p>HPE Helion OpenStack leverages the OpenStack Ironic service during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./commercial/GA1/1.1commerical.services-logging-overview.dita:<p>Launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./commercial/GA1/1.1commerical.services-logging-overview.dita:<p>From the seed cloud host log in to the undercloud as super user:</p>
./commercial/GA1/1.1commerical.services-object-overview.dita:<p>At its core, Object Storage (Swift) is built from a set of software services and data constructs hosted on a cluster of servers.</p>
./commercial/GA1/1.1commerical.services-overview.dita:<b>Ironic</b>. The Ironic service runs during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:# set a variable to the failed host name from above
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:failed_host=&lt;name>
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-conductor --host=$failed_host
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-cert --host=$failed_host
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-scheduler --host=$failed_host
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:nova-manage service disable --service=nova-consoleauth --host=$failed_host</codeblock>
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:# show the monitored hosts
./commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita:check_mk --list-hosts</codeblock>
./commercial/GA1/1.1commerical.services-scale-out-swift.dita:<p>It is recommended to use these Starter servers as host for the following Swift services for scaled out Swift cluster:</p>
./commercial/GA1/1.1commerical.services-scale-out-swift.dita:<p>For containers created before the deployment of scale-out Swift, default policy is policy-0.  Policy-0 is used to store objects. However, a new storage policy (termed as policy-1) is configured as soon as you deploy a scale-out Swift and it becomes the new default policy. After the deployment of scale-out Swift, all new containers are mapped to the new storage policy and  objects associated to those containers are stored in the ring with policy-1. For more details on storage policy, see <xref href="http://docs.openstack.org/developer/swift/overview_policies.html" scope="external" format="html" >OpenStack Swift overview policies</xref>.</p>
./commercial/GA1/1.1commerical.services-scale-out-swift.dita:- **Default policy.** The ***default*** policy can be any policy defined in the cluster. The default policy is automatically chosen when a container is created without a storage policy specified.
./commercial/GA1/1.1commerical.services-swift-deployment-add-proxy-node.dita:  server &lt;proxy node hostname&gt; &lt;proxy nodes IP address&gt;:8080 check inter 2000 rise 2 fall 5 
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:`*.ring.gz` files. The builder files are required to determine the list of hosts and disk drives that needs to be monitored. `swift-ring-builder` command is used in monitoring scripts to get the host and disk details from the builder files. 
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:<li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:<li>In the <b>Host</b> column, click the icon  next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:                            "<b><i>View Service Details For This Host</i></b>".  The page navigates
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.dita:                    to Service Status Details For Host &lt;<b>Swift node IP address </b>&gt; and
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                <li>Click <b>Status</b> on the left panel and click  <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                <li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                    displays as "<b><i>View Service Details For This Host</i></b>". The page
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-check.dita:                    navigates to Service Status Details For Host &lt;<b>Swift node IP address</b>>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                <li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                    displays as "<i><b>View Service Details For This Host</b></i>". The page
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-health-swift-services.dita:                    navigates to Service Status Details For Host &lt;<b>Swift node IP address</b>>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:<li>Click <b>Status</b> on the left panel and click  <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        href="../../media/icinga_host-details.png" id="image_qg3_fgv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:<li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:                            "<i><b>View Service Details For This Host</b></i>". <p> The page
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-replica-swift-status.dita:                        navigates to Service Status Details For Host &lt;<b><i>Swift node IP
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:<li> Click <b>Status</b> on the left panel and click  <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:<li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:                            "<i><b>View Service Details For This Host</b></i>" (as shown in the
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-disk.dita:                    above image). The page navigates to Service Status Details For Host
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:                <li> Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:<li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:                            "<b><i>View Service Details For This Host</i></b>".  The page navigates
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-drive-audit.dita:                    to Service Status Details For Host &lt;<b>Swift node IP address</b>> and
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:                <li> Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:<li>In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:                            "<b><i>View Service Details For This Host</i></b>". The page navigates
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-file-ownership.dita:                    to Service Status Details For Host &lt;<b>Swift node IP address</b> > and
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:<li>In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:                            "<b><i>View Service Details For This Host</i></b>". The page navigates
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-mount-points.dita:                    to Service Status Details For Host &lt;<b><i>Swift node IP address</i></b>> and
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:</b> service running in the undercloud to monitor the network access of all the Swift nodes. The host that are part of ring can be determined using swift-ring-builder and only those nodes are verified for connectivity.</p>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:<section id="monitoring-the-network-access-of-the-host"> <title>Monitoring the Network Access of the Host</title>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:<p>Perform the following steps to monitor the network access of the host:</p>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:<li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:<li> In the <b>Host</b> column, click the icon next to the host IP when the tooltip displays as
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                            "<b><i>View Service Details For This Host</i></b>".  The page navigates
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                    to Service Status Details For Host &lt;<b>Swift node IP address</b> > and
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                    displays the <b>the status of host for network connectivity</b>. <image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-access-ping.dita:                        href="../../media/swift_icinga-swift-ping-host.png" id="image_sw2_42w_ns"
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:            <p>The Icinga monitors the minimum NIC speed requirement of the host. The performance of
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                is recommended to have a minimum NIC speed. If the host does not meet the required
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                <li>In the <b>Host</b> column, click the icon next to the host IP when the tooltip
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                    displays as "<b><i>View Service Details For This Host</i></b>". The page
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-network-interface.dita:                    navigates to Service Status Details For Host &lt;<b>Swift node IP address</b>>
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                <li>Click <b>Status</b> on the left panel and click <b>Host Details</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                        href="../../media/icinga_host-details.png" id="image_etd_hdv_ns"/><p>The
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                        page navigates to the <b>Host Status Details for all Hosts</b>.<image
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                <li>In the <b>Host</b> column, <b>click the icon</b> next to the host IP when the
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                    tooltip displays as "<i><b>View Service Details For This Host</b></i>". The page
./commercial/GA1/1.1commerical.services-swift-deployment-monitor-swift-proxy-health-checks.dita:                    navigates to Service Status Details For Host &lt;<b>Swift node IP address</b>>
./commercial/GA1/1.1commerical.services-swift-deployment-shrink-remove-proxy-node.dita:  server &lt;Proxy node hostname&gt; &lt;Proxy nodes IP address of &gt;:8080 check inter 2000 rise 2 fall 5 
./commercial/GA1/1.1commerical.services-swift-deployment.dita:The authenticity of host '192.0.2.29 (192.0.2.29)' can't be established.
./commercial/GA1/1.1commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<xref type="section" href="#topic6748/download">Download the hpssacli debian package in the KVM host</xref>
./commercial/GA1/1.1commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<section id="download"> <title>Download the hpssacli debian package in the KVM host</title>
./commercial/GA1/1.1commerical.services-swift-diagnosis-disk-health-hpssacli.dita:<li>Copy the package from KVM host to
./commercial/GA1/1.1commerical.services-swift-diagnosis-disk-health-hpssacli.dita:          KVM_host<codeblock><codeph>scp details_slot_&lt;slot number&gt;.zip ubuntu@&lt;KVM_Host IP address&gt;:
./commercial/GA1/1.1commerical.troubleshooting.compute.dita:When the Compute Service deletes an entity, such as an instance, it <i>marks</i> the relevant database rows as deleted, but does not actually <i>remove</i> those rows from the database. As the number of "soft-deleted" rows builds up, they begin to have a significantly detrimental impact on the database's performance.</p>
./commercial/GA1/1.1commerical.troubleshooting.compute.dita:<codeph>delete from aggregate_hosts where deleted != 0 and deleted_at &lt; @keep_date limit 10000;
./commercial/GA1/ceilometer/1.1commercial.services-reporting-troubleshoot.dita:<p>Use the RabbitMQ CLI to re-start the instances and then the host.</p>
./commercial/GA1/ceilometer/1.1commercial.services-reporting-troubleshoot.dita:<p>Restart the RabbitMQ host</p>
./commercial/GA1/ceilometer/1.1commerical.services-reporting-bestpractice.dita:&lt;VirtualHost *:8777&gt;
./commercial/GA1/ceilometer/1.1commerical.services-reporting-bestpractice.dita:&lt;/VirtualHost&gt;
./commercial/GA1/ceilometer/1.1commerical.services-reporting-bestpractice.dita:<p>This option does not apply to meters that are notification-only because those meters are not polled.</p>
./commercial/GA1/ceph/1.1commenrcial.ceph-configuration-recovery-and-backup-procedure-helion-nodes.dita:<p>Make sure the Ceph cluster uses the same IP range as the HPE Helion OpenStack overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion set up.</p>
./commercial/GA1/ceph/1.1commenrcial.ceph-monitoring.dita:<li>Icinga: The main alerting server that runs each check periodically on every host.</li>
./commercial/GA1/ceph/1.1commenrcial.ceph-monitoring.dita:<codeph>Check_mk</codeph>: The utility that runs on the undercloud controller and on the each of the hosts that must be monitored. This utility runs local checks and sends result back to the main server.</li>
./commercial/GA1/ceph/1.1commenrcial.ceph-monitoring.dita:<p>The Ceph client installer script installs the Ceph health monitoring scripts in the <codeph>/usr/lib/check_mk_agent/local/</codeph> directory. The <codeph>check_mk</codeph> utility detects those scripts automatically through a cron job that runs periodically and begins using them to add the reporting details in the Icinga.</p>
./commercial/GA1/ceph/1.1commenrcial.ceph-troubleshooting.dita:<p>Make sure the Ceph cluster uses the same IP range as the HPE Helion OpenStack overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion setup.</p>
./commercial/GA1/ceph/1.1commercia.ceph-high-availability-RADOSGW-authentication.dita:<codeblock><codeph>&lt;VirtualHost *:35357&gt;
./commercial/GA1/ceph/1.1commercia.ceph-high-availability-RADOSGW-authentication.dita:&lt;/VirtualHost&gt;
./commercial/GA1/ceph/1.1commercia.ceph-high-availability-RADOSGW-authentication.dita:&lt;VirtualHost *:5000&gt;
./commercial/GA1/ceph/1.1commercia.ceph-high-availability-RADOSGW-authentication.dita:&lt;/VirtualHost&gt;
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:12. From the KVM host, log into the instance:
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:        | attachments                          | [{u'device': u'/dev/vde', u'server_id': u'd6c98de0-b65e-4e43-bd5e-04c81ad26cd1', u'id': u'580d3e95-970f-4a9c-92ea-284799dcbc82',                                            u'host_name': None, u'volume_id': u'580d3e95-970f-4a9c-92ea-284799dcbc82'}]                                                     |
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:        | os-vol-host-attr:host                | overcloud-controller1-thg43e77ptei                                                                                              |
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:        | OS-EXT-SRV-ATTR:host                 | overcloud-novacompute0-k3kakatgtgb2                                            |                                                
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:        | OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-novacompute0-k3kakatgtgb2.novalocal                                  |                                                
./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita:        | hostId                               | cf6bb4eb58517b0e06246628e3d0559267a2594c06ea44100e2fae1e                       |
./commercial/GA1/ceph/1.1commercial.ceph-authentication.dita:<p>Each user has a keyring file on Ceph hosts. But the keyring file does not contain the Ceph references to verify user authorizations; instead the Monitor servers have their own internal keyrings. When you add a user to a Ceph installation, you create a keyring file on the Ceph hosts in <codeph>/etc/ceph</codeph> and integrate a key into a cluster using the <codeph>ceph auth add</codeph> command.</p>
./commercial/GA1/ceph/1.1commercial.ceph-automated-install.dita:   "HOST": "127.0.0.1",
./commercial/GA1/ceph/1.1commercial.ceph-automated-install.dita:<b>HOST</b> is the local machine IP or loopback IP which will be your seed IP.</p>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:# Variables here are applicable to the ceph-cluster host group
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          node to Helion nodes by entering:<p>a. Start from the KVM host after the helion and Ceph
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:            nodes are up.</p><p>b. From the root shell on the KVM host, SSH to the seed node, by
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:            and configures the <codeph>/etc/hosts</codeph> file on Ceph nodes.</p><p>After the
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:            bootstrap is run, verify that the <codeph>hosts</codeph> file and
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:              <codeph>host_vars/*</codeph> files are generated.</p><p>The following is a sample host
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:      <codeblock><codeph>ansible-playbook -i hosts osd_mon_setup.yml
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:      <codeblock><codeph> ansible-playbook -i hosts client_setup.yml
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:      command:<codeblock>ansible-playbook -i hosts client_setup.yml</codeblock></p>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:      <codeblock>ansible-playbook -i hosts client_setup.yml --skip-tags computes</codeblock>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:    <section><b>Integration of Ceph with HOS components</b><p>Perform the following steps to
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:        integrate ceph with HOS components</p><ol id="ol_h12_vpp_xs">
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          various HOS components with Ceph using passthrough:<p>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:                <stentry>Ceph Integrated with HOS Components</stentry>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:    <codeblock><codeph>ansible-playbook -i hosts rados_setup.yml
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:    <codeblock><codeph>ansible-playbook -i hosts sync_all_nodes.yml
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          node-provisioning tool (as mentioned above) from the seed host. You need to have three
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          <codeblock><codeph> ansible-playbook -i  hosts osd_mon_setup.yml
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          the seed host as mentioned above.</li>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:            file, and configures <codeph>/etc/hosts</codeph> file on ceph nodes.</p>
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:          <codeblock><codeph> ansible-playbook -i hosts osd_mon_setup.yml --tags "add-osd"
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:        <li>To function properly, keep the original volume, whose snapshot was taken. If the
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:| attachments                          | [{u'device': u'/dev/vde', u'server_id': u'd6c98de0-b65e-4e43-bd5e-04c81ad26cd1', u'id': u'580d3e95-970f-4a9c-92ea-284799dcbc82',                                            u'host_name': None, u'volume_id': u'580d3e95-970f-4a9c-92ea-284799dcbc82'}]                                                     |
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:| os-vol-host-attr:host                | overcloud-controller1-thg43e77ptei                                                                                              |
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:| OS-EXT-SRV-ATTR:host                 | overcloud-novacompute0-k3kakatgtgb2                                            |                                                
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:| OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-novacompute0-k3kakatgtgb2.novalocal                                  |                                                
./commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.dita:| hostId                               | cf6bb4eb58517b0e06246628e3d0559267a2594c06ea44100e2fae1e                       |
./commercial/GA1/ceph/1.1commercial.ceph-glossary.dita:<p><b>Ceph Node [ Node, Host ]</b> - Any single machine or server in a Ceph System.</p>
./commercial/GA1/ceph/1.1commercial.ceph-helion-openstack-ceph-storage-administration.dita:<codeblock><codeph>ceph [-m monhost] {command}
./commercial/GA1/ceph/1.1commercial.ceph-helion-openstack-ceph-storage-administration.dita:<codeblock><codeph>ceph [-m monhost] mon_status
./commercial/GA1/ceph/1.1commercial.ceph-helion-openstack-ceph-storage-administration.dita:<p>Stale Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by <codeph>mon_osd_report_timeout</codeph>).</p>
./commercial/GA1/ceph/1.1commercial.ceph-helion-openstack-ceph-storage-administration.dita:<p>To get a status of just the monitor you connect to (use -m HOST:PORT to select), execute the following command:</p>
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:mon_initial_members = ceph-mon1 - monitor host name
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:mon_host = 192.x.x.x - monitor IP
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<li>Generate a monitor map using the monitor hostname, IP and fsid and save it as
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:          entering:<codeblock><codeph>monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:          entering:<codeblock><codeph>sudo mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:          entering.<codeblock><codeph>ceph-mon --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:          entering.<codeblock><codeph>sudo touch /var/lib/ceph/mon/{cluster-name}-{hostname}/upstart
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-mon1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<li>Log in to OSD host as root user. Connect to the OSD
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>ceph osd crush add-bucket {hostname} host
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush add-bucket ftcceph1-osd1 host
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita: added bucket ftcceph1-osd1 type host to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>ceph osd crush move {hostname} root=default
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.0 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:        hostname after <codeph>host=</codeph> and run <codeph>chmod +x crush.sh</codeph> and
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.0 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.1 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.2 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.3 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.4 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.5 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.6 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.7 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.8 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.9 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.10 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.11 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.12 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 0 name 'osd.0' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 1 name 'osd.1' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 2 name 'osd.2' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 3 name 'osd.3' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd3
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd4
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<li>Log in to OSD host as root user. Connect to the OSD
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>ceph osd crush add-bucket {hostname} host
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush add-bucket ftcceph1-osd1 host
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:    added bucket ftcceph1-osd1 type host to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<codeblock><codeph>ceph osd crush move {hostname} root=default
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.0 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:        hostname after <codeph>host=</codeph> and run <codeph>chmod +x crush.sh</codeph> and
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.0 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.1 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.2 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.3 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.4 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.5 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.6 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.7 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.8 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.9 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.10 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.11 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:ceph osd crush add osd.12 1.0 host=ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 0 name 'osd.0' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 1 name 'osd.1' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 2 name 'osd.2' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:add item id 3 name 'osd.3' weight 1 at location {host=ftcceph1-osd1} to crush map
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd3
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd4
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:mon_host = xxxx
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-mon1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-admin
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-gateway2
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd1
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ceph-osd3
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:<p>On a two monitor deployment, no failures can be tolerated to maintain a quirum but with 3 monitors, one failure  can be tolerated and with 5 monitors, two failures can be tolerated. Since, monitors are light-weight, it is possible to run on same host but it is recommended to run them on separate hosts because fsync issues with kernel may impair performance. Deploy your hardware and install all the required software. Follow the similar steps as how other Ceph cluster nodes are deployed.</p>
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:            machine<codeblock><codeph>ssh {new-mon-host}
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:          host = new-mon-host addr =
./commercial/GA1/ceph/1.1commercial.ceph-manual-install.dita:host = ftcceph1-mon2
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway-client.dita:<p>Make sure you also have the Swift or Rados Gateway credentials sourced on the Duplicity host system.</p>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:<p>Perform the following steps from the Ceph admin node. It is assumed that the hostname for a gateway node is <codeph>gateway</codeph>, and the host name of the second node (for HA) is <codeph>gateway1</codeph>.</p>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                        below. Make sure that the hostname of the gateway node(s) is
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                        considered for both host and RGW DNS name fields by
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:host = gateway
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:host = gateway1
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:<li>Edit <codeph>/etc/hosts</codeph> file to include the <codeph>fqdn</codeph> of a gateway node by
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                                  follows:<codeblock><codeph>address=/.{fqdn}/{host ip}
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                                  is defined in the <codeph>/etc/hosts</codeph>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                        host is considered. If necessary, you can configure a
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:                                        virtual host on port 80. The file contents are shown
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:&lt;VirtualHost *:443&gt;
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:&lt;/VirtualHost&gt;
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:* Edit the `/etc/hosts` file to include the gateway node by entering:
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:        export no_proxy=localhost,127.0.0.1,192.x.x.x,gateway.ex.com,gateway
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:        <VirtualHost *:35357>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:        </VirtualHost>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:        <VirtualHost *:5000>
./commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.dita:        </VirtualHost>
./commercial/GA1/ceph/1.1commerical.services-ceph-archref.dita:<p>At the minimum, a Ceph cluster includes one Admin node, one Monitor node (MON) and 3 Object Storage Daemons (OSDs). The Monitor node can also coexist on the same Host as one of the OSDs. Administrative and control operations are typically issued from Admin node. Note that there is also a Metadata server node (MDS) which stores metadata on behalf of the Ceph Filesystem (that is, Ceph Block Devices and Ceph Object Storage do not use MDS). There is no need for a MDS node at this time as HPE does not support the Ceph Filesystem interface.</p>
./commercial/GA1/Landing/1.1commercial.landing.services.dita:<!--A BR tag was used here in the original source.-->Learn about the Ironic service for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./commercial/GA1/neutron/1.1commercial_neutron-intro.dita:<p>To accomplish these tasks in the most efficient order, this guide organizes the tasks that the network administrator must complete (or verify that are complete) before installing Helion OpenStack and those tasks that must be accounted for after the HPE Helion OpenStack installation.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<p>A chosen backend driver and its prerequisites:</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<p>PowerDNS (self hosted)<!--A BR tag was used here in the original source.-->
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<p>Microsoft DNS (self-hosted)</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<codeph>msdns_servers</codeph> - A comma separated list of the Microsoft DNS servers short hostnames.</li>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<codeph>dynect_also_notify</codeph> - List of hostnames for name servers to notify when a zone is changed (Comma separated). These should be the IP addresses provided by DynECT during signup.</li>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<b>RabbitMQ Hosts</b> - Enter the IP address for the Messaging Outputs, each entry on a line, with no commas. </li>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-DNaaS.dita:<codeph>CA Cert</codeph> - Enter the value of the <codeph>ephemeral-ca.crt</codeph> file on the seed cloud host.</li>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<p>External VLAN - Used for binding a routable address to a Compute (Nova) VM launched in Helion. Compute VMs are hosted in a Compute Node.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<p>Management VLAN - Every baremetal host has an address on this network for in-band management purposes.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<p>To protect against external attacks on Helion services, your firewall should be configured with a rule to block any requests originating from outside the network that attempts to reach any of the HPE Helion OpenStack nodes or any 3PAR StoreServ  or StoreVirtual VSA appliances dedicated to the HPE Helion OpenStack installation, other than those indicated this table:</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<p>The customer deploying HPE Helion OpenStack is responsible for securing the block storage networks. Network data flows for block storage should be restricted using access control lists or other mechanisms in the customer's network devices which can include routers, switches, or firewalls. Block storage data flows interacting with HPE Helion OpenStack are described here to assist with defining those controls. References are given to documentation on data flows within the storage cluster itself, but not necessarily interacting with HPE Helion OpenStack nodes.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>Cloud Controller (Cinder host)</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>Cloud Controller (Cinder host)</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>CMC to StoreVirtual <!--A BR tag was used here in the original source.-->Recommended to install on the seed cloud host</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<p>HPE Helion Openstack supports iSCSI or Fibre Channel connectivity with 3PAR StoreServ. If using Fibre Channel, then the Compute nodes and the overcloud controller hosting Block Storage (Cinder) will require Fibre Channel connectivity with the 3PAR array. For iSCSI, connectivity will be through the management VLAN. The StoreServ REST API and SSH command line interfaces must be accessible from the management VLAN as well.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<b>Note:</b> In the following table, the Volume Operation host refers to the overcloud controller that hosts the Volume Operations (Cinder) service.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<entry>Overcloud Controller (Volume Operations host)</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-configure-network.dita:<li>Traffic between the OVSvApp VMs running on every ESX Host to communicate with the Network Operations message queue on the overcloud controller</li>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-neutron-integration.dita:<entry>ldap://localhost</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-neutron-integration.dita:<entry>ldap://localhost</entry>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-neutron-integration.dita:<p>You need to copy the configuration files to the seed VM host during the installation, after the seed VM is installed and before launching the installation of the overcloud and undercloud.</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-neutron-integration.dita:<p>On the seed VM host, perform the following:</p>
./commercial/GA1/neutron/1.1commercial_neutron-post-installation-neutron-integration.dita:                        "value": "ldap://localhost"
./commercial/GA1/neutron/1.1commercial_neutron-post-installation.dita:| OS-EXT-SRV-ATTR:host                 | icehousecompute               |
./commercial/GA1/neutron/1.1commercial_neutron-post-installation.dita:| hostId                               | 091ce2ae798d669b1ec9cc53 ...  |
./commercial/GA1/neutron/1.1commercial_neutron-post-installation.dita:| OS-EXT-SRV-ATTR:hypervisor_hostname  | icehousecompute.example.com   |
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<li>Capable of hosting VMs</li>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<li>seed cloud host configured in UTC (Coordinated Universal Time)</li>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<p>For Compute nodes, Intel or AMD hardware virtualization support required. The CPU cores and memory requirements must be sized based on the VM instances hosted by the Compute node.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<entry morerows="3"> Seed Cloud Host </entry>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<entry> 1TB - This host will store the downloaded images as well as act as a host where backup data is preserved.</entry>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<entry>32GB - Memory must be sized based on the VM instances hosted by the Compute node.</entry>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<entry>8 CPU cores - Intel or AMD 64-bit processor with hardware virtualization support. The CPU cores must be sized based on the VM instances hosted by the Compute node. </entry>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<p>This section explains the HPE Helion OpenStack software requirements. The software requirement for the Seed Cloud Host is Ubuntu 14.04 with the following packages.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<section id="otherseed"> <title>Other seed cloud host requirements and recommendations</title>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation-hardware-software.dita:<p>Other requirements and recommendations for the seed cloud host are as follows:</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<li>The machine hosting the seed VM, and all baremetal systems have to be connected to a management network.</li>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<li>Nodes on this management network must be able to reach the iLO subsystem (<xref href="http://www8.hp.com/us/en/products/servers/ilo/index.html" scope="external" format="html" >HPE Integrated Lights-Out</xref>) of each baremetal systems to enable host reboots as part of the install process.</li>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<li>Ensure network interfaces that are not used for PXE boot are disabled from BIOS to prevent PXE boot attempts from those devices.</li>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<section id="preparing-the-seed-cloud-host"> <title>Preparing the seed cloud host</title>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>The following tasks need to be performed on the seed cloud host, where the seed VM will be installed. The seed cloud host is alternatively known as the installer system.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>The seed cloud host must have Ubuntu 14.04 LTS installed before performing the HPE Helion OpenStack installation.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>On the seed cloud host, the OpenSSH server must be running and the firewall configuration should allow access to the SSH ports.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>On the seed cloud host, the user root must have a public key, for example:</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:Before you start the installation, you must install NTP on the seed cloud host (installation system) and configure it as a NTP server. You will configure the undercloud and overcloud systems as NTP clients during the installation process.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>For information on installing NTP on the seed cloud host, see <xref href="../../../commercial/GA1/1.1commercial.install-GA-NTP.dita" >HPE Helion OpenStack Installation: NTP Server</xref>.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>Log in to your seed cloud host as root:</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>Copy the installation package to the seed cloud host.</p>
./commercial/GA1/neutron/1.1commercial_neutron-pre-installation.dita:<p>During the installation process after the seed VM is installed, the installer script looks for information about the baremetal systems. Specifically, it looks for this information in a file called <codeph>baremetal.csv</codeph>. Before you begin the installation process, you must create this file and upload the file to the installer system (called the seed cloud host) at the appropriate installation step.</p>
./community/community.dashboard.launch.dita:<p>Ask the cloud operator for the host name or public IP address from which you can access the dashboard, and for your user name and password.</p>
./community/community.dashboard.launch.dita:<p>In the address bar, enter the host name or IP address for the dashboard.</p>
./community/community.eula.dita:<p>e Third Party and Open Source Components. To the extent any component of the software is subject to any third party license terms, including open source license terms, then those third party license terms or open source license terms shall govern with respect to the subject component; otherwise, the terms of this Agreement shall govern.</p>
./community/community.eula.dita:<li>Per Server License: Following purchase, this per-server license is to be assigned to a Physical Server that will be used to run your Cloud Fabric.  After the Physical Servers are licensed and those licenses are properly assigned, you may run any number of instances of the Management Software to deploy, configure, manage and operate your Cloud Fabric.</li>
./community/community.eula.dita:<b>b. Assigning Licenses:</b>  Before you can install and use the Management Software to deploy your Cloud Fabric, you must assign to each Physical Server running the host fabric one per-Physical Server license. Each Physical Server to which you assign a license is a licensed host server.</p>
./community/community.faq.dita:<p>HPE Helion OpenStack Community edition is API-compatible with HPE Helion Public Cloud, allowing workloads developed on one to be deployed on the other.  Compatibility with other Public Clouds will depend on which OpenStack platform features they chose to support. It is tested and supported by HPE for enterprise-grade, small-scale, private cloud deployments. Workloads are upgradeable to <xref href="../community/community.index.dita" >HPE Helion OpenStack</xref>, with features and capabilities needed for the enterprise, government, and service provider markets, to be launched later this calendar year.</p>
./community/community.faq.dita:<p>Yes. It includes an integrated <tm tmtype="reg">Linux</tm> host OS, hardened and tested for OpenStack cloud.</p>
./community/community.faq.dita:<p>We are hosting the support discussion forum for the HPE Helion OpenStack Community edition at <xref href="https://ask.openstack.org" scope="external" format="html" >https://ask.openstack.org</xref>.  Developers in the community are very familiar with this forum and already participate in OpenStack technology-related discussions there. Please tag your questions with 'HPHelion' to get our attention for any questions and issues you raise.</p>
./community/community.faq.dita:    <td width=50%><p>*Includes integrated HPE Linux host OS for better speed, support, and control across the full solution stack</p> 
./community/community.faq.dita:The HPE Helion OpenStack Community edition is one of the first distributions in the market based on the most recent Icehouse release of OpenStack from the community. It is pure OpenStack with no proprietary technology or add-ins. For example, TripleO is used for deployment. It comes with HPE Linux as a host OS, hardened and tested for HPE Helion OpenStack Community edition. More competitive information is available on the HPE Helion OpenStack Community battlecard on the sales portal.  
./community/community.faq.dita:HPE Helion OpenStack Community edition is API-compatible with HPE Helion Public Cloud, allowing workloads developed on one to be deployed on the other.  Compatibility with other Public Clouds will depend on which OpenStack platform features they chose to support.  It is tested and supported by HPE for enterprise-grade, small-scale, private cloud deployments. Workloads are upgradeable to HPE Helion OpenStack with features and capabilities needed for the enterprise, government, and service provider markets, to be launched later this calendar year.
./community/community.faq.dita:Yes. It includes an integrated HPE Linux&#174; host OS hardened and tested for this distribution giving HPE full control of the stack (for compute and controller node hypervisors).
./community/community.faq.dita:HPE Helion OpenStack Community edition is API compatible with HPE Helion Public Cloud, allowing workloads developed on one to be deployed on the other.  Compatibility with other Public Clouds will depend on which OpenStack platform features they chose to support.  It is tested and supported by HPE for enterprise-grade, small-scale, private cloud deployments. Workloads are upgradeable to HPE Helion OpenStack with features and capabilities needed for the enterprise, government and service provider markets, to be launched later this calendar year.
./community/community.faq.dita:* 64 bit Linux machine with at least 16GB of memory to act as the host of the seed VM.
./community/community.faq.dita:1. Configure the hypervisor host's network. The host must provide connectivity to the virtual machine Admin Node on an isolated private network 
./community/community.faq.dita:3. Download the HPE Cloud OS ISO to the host's local file system or data store.
./community/community.hwsw-requirements.dita:            <xref type="section" href="#topic7297/otherseed">seed cloud host requirements and
./community/community.hwsw-requirements.dita:          <p>Host Interconnects/Protocols:</p>
./community/community.hwsw-requirements.dita:        <li>Capable of hosting VMs</li>
./community/community.hwsw-requirements.dita:            <li>seed cloud host configured in UTC (Coordinated Universal Time)</li>
./community/community.hwsw-requirements.dita:            and memory requirements must be sized based on the VM instances hosted by the Compute
./community/community.hwsw-requirements.dita:              <entry morerows="3"> Seed Cloud Host </entry>
./community/community.hwsw-requirements.dita:              <entry> 1TB - This host will store the downloaded images as well as act as a host
./community/community.hwsw-requirements.dita:              <entry>32GB - Memory must be sized based on the VM instances hosted by the Compute
./community/community.hwsw-requirements.dita:                support. The CPU cores must be sized based on the VM instances hosted by the Compute
./community/community.hwsw-requirements.dita:      <p>Software requirements for the Seed Cloud Host:</p>
./community/community.hwsw-requirements.dita:      <title>Other seed cloud host requirements and recommendations</title>
./community/community.hwsw-requirements.dita:      <p>Other requirements and recommendations for the seed cloud host are as follows:</p>
./community/community.install-overview.dita:<p>Undercloud - The undercloud is a complete OpenStack installation, which is then used to deploy the overcloud. In HPE Helion OpenStack Community, the undercloud can be a separate baremetal server or a virtual machine on the Seed host.</p>
./community/community.install-overview.dita:<p>Overcloud<!--Removed anchor point overcloud--><!-- id="overcloud" --> - The overcloud is the end-user OpenStack cloud. In HPE Helion OpenStack Community, the undercloud can be separate baremetal servers or virtual machines on the Seed host.</p>
./community/community.install-overview.dita:<p>In addition, an installation system, called the seed cloud host, that meets the following configuration is required:</p>
./community/community.install-overview.dita:<p>In addition, an installation system, called the seed cloud host, that meets the following configuration is required:</p>
./community/community.install.baremetal.dita:<b>IMPORTANT:</b> During the installation process, <b>DO NOT RESTART</b> the seed cloud host (the system running the installer and seed VM). Restarting this system disrupts the baremetal bridge networking configuration and disables both the undercloud and overcloud. If the system is inadvertently restarted, you must complete the installation process again.</p>
./community/community.install.baremetal.dita:<b>Optional:</b> To use an interface other than 'eth0' on the HOST as the bridge interface, for example eth3, use the following command:</p>
./community/community.install.baremetal.dita:<b>Optional:</b> To use a host other than the seed as the gateway, use the following command:</p>
./community/community.install.baremetal.dita:  <codeph>bash -x /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed
./community/community.install.baremetal.dita:<p>Set the IP address of an NTP server accessible on the public interface for overcloud and undercloud hosts using the following commands</p>
./community/community.install.baremetal.dita:<p>If you choose a pool of addresses outside the baremetal subnet, you should make sure those addresses are accessible.</p>
./community/community.install.baremetal.dita:<p>The overcloud neutron external network (ext-net) assumes the gateway IP is the lowest non-zero host IP address in the FLOATING_CIDR range.</p>
./community/community.install.baremetal.dita:<p>By <xref type="section" href="#topic16854/NetworkDefault">default</xref>, the bridge interface, the seed VM IP address, and the gateway host are configured during the installation process. To change any or all of those configurations, complete the following steps:</p>
./community/community.install.baremetal.dita:<b>OPTIONAL</b>: If you modified the gateway host, execute:</p>
./community/community.install.virtual.dita:<p>There are no restrictions imposed on external device name on the host system in virtual mode as the external interface is not used.</p>
./community/community.install.virtual.dita:  <codeph>HP_VM_MODE=y bash -x ~root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed
./community/community.install.virtual.dita:<codeph>OVERCLOUD_NTP_SERVER</codeph> - Use this variable to set the IP address of an NTP server accessible on the public interface for overcloud hosts. Determine the time server that works best based on your environment. The NTP server can be on the local system, if needed.</p>
./community/community.install.virtual.dita:<codeph>UNDERCLOUD_NTP_SERVER</codeph> - Use this variable to set the IP address of an NTP server accessible on the public interface for undercloud hosts. Determine the time server that works best based on your environment. The NTP server can be on the local system, if needed.</p>
./community/community.network-requirements.dita:<p>Any network interfaces that are not used for PXE boot must be disabled from BIOS to prevent PXE boot attempts from those devices.</p>
./community/community.network-requirements.dita:<li>The external interface of host of the seed VM is added to a software bridge, called <i>brbm</i>, on the host and the IP address of the external interface is transferred to the bridge. This allows external traffic on 192.0.2.0/24 to be routed to the seed VM.</li>
./community/community.network-requirements.dita:<p>In the default configuration, the default pool of floating IP addresses for the overcloud is within the baremetal subnet. You can select a different pool of addresses during the installation. If you select a pool of addresses outside the baremetal subnet, make sure those addresses are accessible.</p>
./community/community.network-requirements.dita:<b>Important:</b> IP addresses in the ranges chosen for the undercloud and overcloud must not be used by other nodes
./community/community.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:host                 | icehousecompute               |
./community/community.networking-maskedIP.dita:    | hostId                               | 091ce2ae798d669b1ec9cc53 ...    |
./community/community.networking-maskedIP.dita:    | OS-EXT-SRV-ATTR:hypervisor_hostname  | icehousecompute.example.com   |
./community/community.release-notes.dita:<i>A baremetal deployment of HPE Helion OpenStack Community</i>. First released in June, 2014, this edition installs in a physical environment. The baremetal installation allows you to build an OpenStack cloud spanning multiple physical nodes. During installation, this edition sets up an undercloud host and deploys the overcloud on 3 controller hosts, 2 physical Swift nodes, and up to 36 physical compute nodes. </li>
./community/community.release-notes.dita:<li>If you determine that your VM seed has not started correctly when you executed the <codeph>hp_ced_host_manager.sh</codeph> script, run the script a second time to ensure you start the seed.</li>
./community/community.release-notes.dita:<b>Live Migration of Instance in Stopped, Suspended, or Rescued State May Fail</b> Make sure instances are not in the Stopped, Suspended, or Rescued state before attempting to migrate them to a new host. A 400 error may appear; a message "Failed to migrate instance..." may be received; or the client may quietly time out, leaving the instance stuck in the Migrating state and thus unmanageable.</li>
./community/community.services-compute-howto.dita:<p>Leveraging OpenStack Compute, the HPE Helion OpenStack Community Compute provides a way to instantiate virtual servers on publicly accessible physical machines hosted in HPE data centers.</p>
./community/community.services-compute-overview.dita:<p>The HPE Helion OpenStack Compute Service leverages the  OpenStack compute service to instantiate virtual machine instances on publicly accessible physical machines hosted in your cloud environment.</p>
./community/community.services-icinga.dita:<p>Click <b>Status</b> on the left panel and then click <b>Host Details</b>.
./community/community.services-icinga.dita:<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
./community/community.services-icinga.dita:<p>In the <b>Host</b> column, click the icon next to the host IP (with tooltip that shows View Service Details For This Host) of the Swift storage node that you want to monitor.</p>
./community/community.services-icinga.dita:<p>The <b>Service Status Details For Host All Hosts</b> page opens.
./community/community.services-icinga.dita:<p>Click the target Swift node IP address to open the <b>Service Status Details For Host &lt;Swift node IP address &gt;</b> to view the disk usage of the selected Swift node.
./community/community.services-icinga.dita:<p>Click <b>Status</b> on the left panel and then click <b>Host Details</b>.
./community/community.services-icinga.dita:<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
./community/community.services-icinga.dita:<p>In the <b>Host</b> column, click the icon next to the host IP to see <b>Host Status Details For All Hosts</b>.
./community/community.services-icinga.dita:<li>Click the target Swift node IP address to open the <b>Service Status Details For Host <varname>IP Address</varname>
./community/community.services-icinga.dita:<p>Click <b>Status</b> on the left panel and then click <b>Host Details </b>.
./community/community.services-icinga.dita:<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
./community/community.services-icinga.dita:<p>In the Host column, click the icon next to the host IP of the Swift storage node to open <b>Service Status Details For Host <varname>All Hosts</varname>
./community/community.services-icinga.dita:<p>Click the Swift node IP address to open the <b>Service Status Details For Host <varname>Swift node IP address </varname>
./community/community.services-identity-configure-v3.dita:<codeph>OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
./community/community.services-identity-overview.dita:<p>A role defines set of rights and privileges that can be assigned to a user. A user assuming that role inherits those rights and privileges. A role is also called a <i>personality</i>. Each user name must be unique across all of your domains.</p>
./community/community.services-identity-overview.dita:url = ldap://localhost
./community/community.services-ironic-overview.dita:<p>HPE Helion OpenStack Community leverages the OpenStack Ironic service during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./community/community.services-overview.dita:<b>Ironic</b>. The Ironic service runs during the installation for provisioning of the controller and KVM compute host in a baremetal deployment.</p>
./community/community.services-overview.dita:**DVR**. Distributed Virtual Routing (DVR) allows you to define connectivity among different VNSs as well as connectivity between VNS hosts and the external network. HPE Helion OpenStack Community provides Distributed Virtual Routing to cloud users. 
./community/community.troubleshoot.dita:<li>UNSET 'Propagate NTP Time to Host'</li>
./community/community.troubleshoot.dita:<p>On the seed cloud host (the system on which the installer is run), the seed VM's networking will be bridged onto the external LAN.</p>
./community/community.troubleshoot.dita:<p>To revert this change, reboot the system then execute this command on the console of the host:</p>
./community/community.troubleshoot.dita:<p>If the <codeph>hp_ced_host_manager</codeph> fails to start the seed, execute the command again (a failure could be the result of a race condition in libvirt).</p>
./community/community.troubleshoot.dita:<p>If using the seed vm as your gateway (default setting), you will need to execute the following commands after you have run <codeph>hp_ced_host_manager.sh</codeph> on the host machine.</p>
./community/community.troubleshoot.dita:<codeph>curl "localhost:9200/_cat/indices?v"
./community/community.troubleshoot.dita:<codeph>curl -XDELETE "localhost:9200/logstash-&lt;DATE&gt;"
./community/community.verify.dita:<p>From the seed cloud host, connect to the undercloud Horizon console.</p>
./community/community.verify.dita:<p>Point your web browser on the seed cloud host to the undercloud Horizon console using the <codeph>UNDERCLOUD_IP_ADDRESS</codeph> obtained after the install.</p>
./community/community.verify.dita:<p>From the seed cloud host, connect to the overcloud Horizon console.</p>
./community/community.verify.dita:<p>Point your web browser on the seed cloud host to the overcloud Horizon console using the <codeph>OVERCLOUD_IP_ADDRESS</codeph> obtained after the instal.</p>
./community/community.verify.dita:<p>From the seed cloud host, you can connect to the demo VM using the following steps:</p>
./community/community.verify.dita:<p>To access the undercloud monitoring console, launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./community/community.verify.dita:<p>a. From the seed cloud host log in to the undercloud as super user:</p>
./community/community.verify.dita:<p>Launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
./helion/2021pdf.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Documentation PDF</title>
./helion/2021pdf.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/adding_moonshot_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding Moonshot Nodes</title>
./helion/administration/adding_moonshot_nodes.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/adding_moonshot_nodes.dita:    <p>The HPE Moonshot Servers are now verified hardware to run <keyword keyref="kw-hos"/>. The supported
./helion/administration/administration_index.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Administration Overview</title>
./helion/administration/administration_index.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/administration_index.dita:    <p>This section contains administration tasks for your <keyword keyref="kw-hos-phrase"/> cloud.</p>
./helion/administration/objectstorage/allocate_pac.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Allocating Proxy, Account, and Container
./helion/administration/objectstorage/allocate_pac.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/allocating_disk_drives.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Allocating Disk Drives for Object
./helion/administration/objectstorage/allocating_disk_drives.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/allocating_network.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding Swift Network and Service
./helion/administration/objectstorage/allocating_network.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/allocating_server.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Allocating Object Servers</title>
./helion/administration/objectstorage/allocating_server.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/creating_object_server_resource.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating Object Server Resource
./helion/administration/objectstorage/creating_object_server_resource.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/creating_pac_cluster.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating a Swift Proxy, Account, and Container
./helion/administration/objectstorage/creating_pac_cluster.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/creating_roles_swift_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating Roles for Swift Nodes</title>
./helion/administration/objectstorage/creating_roles_swift_nodes.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Customizing Swift Service Configuration Files </title>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:      <p><keyword keyref="kw-hos-phrase"/> enables you to modify various Swift service configuration
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            git:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            processor:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            directory:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            servers:<codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            git:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            processor:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            directory:<codeblock>cd ~/helion/hos/ansible
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/administration/objectstorage/modify_swift_service_config_files.dita:            servers:<codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/administration/objectstorage/modify_swift_service_config_files.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/administration/objectstorage/ring_specifications.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding Swift Ring Specifications</title>
./helion/administration/objectstorage/ring_specifications.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/ring_specifications.dita:        data required to build future rings. In <keyword keyref="kw-hos-phrase"/>, you will use the
./helion/administration/objectstorage/ring_specifications.dita:      <p>In <keyword keyref="kw-hos-phrase"/>, Swift supports erasure coded object rings as well as
./helion/administration/objectstorage/ring_specifications.dita:                ec_types in <keyword keyref="kw-hos-phrase"/> are: <ul>
./helion/administration/objectstorage/storage_policies.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Designing Storage Policies</title>
./helion/administration/objectstorage/storage_policies.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/swift_device_groups.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Swift Requirements for Device Group
./helion/administration/objectstorage/swift_device_groups.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/swift_input_model.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Modifying Example Configurations for Object
./helion/administration/objectstorage/swift_input_model.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/objectstorage/swift_zones.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Designing Swift Zones</title>
./helion/administration/shutdown_by_node_type.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Shutting Down and Restarting the Cloud Node by
./helion/administration/shutdown_by_node_type.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts logging-stop.yml</codeblock>
./helion/administration/shutdown_by_node_type.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts monasca-stop.yml
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts zookeeper-stop.yml</codeblock>
./helion/administration/shutdown_by_node_type.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts freezer-stop.yml</codeblock></sectiondiv>
./helion/administration/shutdown_by_node_type.dita:            then<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;compute node></codeblock>
./helion/administration/shutdown_by_node_type.dita:        Swift host <ol>
./helion/administration/shutdown_by_node_type.dita:            ~/scratch/ansible/next/hos/ansible<codeblock>ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;swift node></codeblock>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;vsa_node name></codeblock></li>
./helion/administration/shutdown_by_node_type.dita:            Ceph<codeblock>ansible-playbook -i hosts/verb_hosts ceph-stop.yml</codeblock></li>
./helion/administration/shutdown_by_node_type.dita:        nodes in your cloud running control plane services. <codeblock>for i in $(grep -w cluster-prefix ~/helion/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</codeblock>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;controller node></codeblock>
./helion/administration/shutdown_by_node_type.dita:            nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock></li>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;controller node></codeblock>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;controller node></codeblock>
./helion/administration/shutdown_by_node_type.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible/group_vars</codeblock>
./helion/administration/shutdown_by_node_type.dita:          <p>You will need to extract the list of RabbitMQ users and passwords from the file whose
./helion/administration/shutdown_by_node_type.dita:            ~/scratch/ansible/next/hos/ansible directory: </p>
./helion/administration/shutdown_by_node_type.dita:          <codeblock>$ ansible-playbook -i hosts/verb_hosts rabbitmq-deploy.yml</codeblock>
./helion/administration/shutdown_by_node_type.dita:          <codeblock>$ tail -F /var/log/rabbitmq/rabbitmq@&lt;hostname>.log
./helion/administration/shutdown_by_node_type.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;vsa_node name></codeblock></li>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname></codeblock>
./helion/administration/shutdown_by_node_type.dita:            observing that all the OSDs under the current host are reported as 'up')
./helion/administration/shutdown_by_node_type.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname></codeblock>
./helion/administration/shutdown_by_node_type.dita:        each Swift host <ol>
./helion/administration/shutdown_by_node_type.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;swift node> </codeblock>
./helion/administration/shutdown_by_node_type.dita:            nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock>
./helion/administration/shutdown_by_node_type.dita:            '@&lt;filename>' to process all hosts listed in the file.
./helion/administration/shutdown_by_node_type.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;compute node></codeblock>
./helion/administration/shutdown_by_node_type.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts freezer-start.yml</codeblock> Note that it
./helion/administration/shutdown_by_node_type.dita:        ~/scratch/ansible/next/hos/ansible<codeblock>ansible-playbook -i hosts/verb_hosts monasca-start.yml
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts zookeeper-start.yml</codeblock>
./helion/administration/shutdown_by_node_type.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:ansible-playbook -i hosts/verb_hosts logging-start.yml</codeblock>
./helion/administration/shutdown_by_node_type.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/shutdown_by_node_type.dita:        <codeblock>stack@helion-control-plane-cluster1-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ls *status*
./helion/administration/stop_restart.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Bringing Back a Cluster</title>
./helion/administration/stop_restart.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/administration/stop_restart.dita:        <li> If you are shutting down the cluster, in scratch/ansible/next/hos/ansible run hlm-stop
./helion/administration/stop_restart.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml</codeblock>
./helion/administration/stop_restart.dita:          nodes<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts bm-power-up.yml </codeblock></li>
./helion/administration/stop_restart.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml </codeblock>
./helion/administration/stop_restart.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml</codeblock>
./helion/administration/stop_restart.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml</codeblock>
./helion/administration/stop_restart.dita:          services:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/administration/stop_restart.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml</codeblock>
./helion/architecture/alternative/alternative_configurations.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Alternative Configurations</title>
./helion/architecture/alternative/alternative_configurations.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/alternative_configurations.dita:                <li><xref keyref="without_dvr">Configuring <keyword keyref="kw-hos"/> without DVR</xref></li>
./helion/architecture/alternative/alternative_configurations.dita:                <li><xref keyref="without_l3agent">Configuring <keyword keyref="kw-hos"/> with Provider VLANs and Physical Routers
./helion/architecture/alternative/entryscale_kvm_ceph_twonetwork.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with Ceph Model with Two
./helion/architecture/alternative/entryscale_kvm_ceph_twonetwork.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/entryscale_kvm_ceph_twonetwork.dita:            <p><image href="../../../media/hos.docs/entry_scale_kvm_ceph_two_network.png"/></p>
./helion/architecture/alternative/entryscale_kvm_ceph_twonetwork.dita:            <p><xref href="../../../media/hos.docs/entry_scale_kvm_ceph_two_network_lg.png" scope="external"
./helion/architecture/alternative/entryscale_kvm_ceph_twonetwork.dita:  hostname-suffix: osd
./helion/architecture/alternative/rhel_compute_model.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>RHEL Compute Nodes</title>
./helion/architecture/alternative/rhel_compute_model.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/standalone_deployer.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Using a Standalone Lifecycle-Manager Node</title>
./helion/architecture/alternative/standalone_deployer.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/standalone_deployer.dita:            <p>All of the examples described above host the lifecycle manager on one of the control nodes.
./helion/architecture/alternative/standalone_deployer.dita:                configuration data on a separate server from those that users of the cloud connect to
./helion/architecture/alternative/standalone_deployer.dita:                host the lifecycle manager service. Note that, in addition to adding the new cluster, you
./helion/architecture/alternative/standalone_deployer.dita:            <p>This specifies a single node of role <codeph>LIFECYCLE-MANAGER-ROLE</codeph> hosting the
./helion/architecture/alternative/standalone_deployer.dita:            <p>The snippet below shows the insertion of an additional server used for hosting the
./helion/architecture/alternative/standalone_deployer.dita:                i.e., the node where you have installed the <keyword keyref="kw-hos"/> ISO.</p>
./helion/architecture/alternative/twosystems.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Considerations When Installing Two Systems on One Subnet</title>
./helion/architecture/alternative/twosystems.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/twosystems.dita:    <p>If you wish to install two separate <ph conkeyref="HOS-conrefs/product-title"/> systems
./helion/architecture/alternative/twosystems.dita:        conkeyref="HOS-conrefs/product-title"/> systems with VRRP traffic on different
./helion/architecture/alternative/twosystems.dita:        conkeyref="HOS-conrefs/product-title"/> system by changing the
./helion/architecture/alternative/twosystems.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/architecture/alternative/twosystems.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/architecture/alternative/twosystems.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/architecture/alternative/twosystems.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/architecture/alternative/twosystems.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/architecture/alternative/twosystems.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/architecture/alternative/twosystems.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-reconfigure.yml</codeblock>
./helion/architecture/alternative/without_dvr.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring <keyword keyref="kw-hos"/> without DVR</title>
./helion/architecture/alternative/without_dvr.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/alternative/without_l3agent.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring <keyword keyref="kw-hos"/> with Provider VLANs
./helion/architecture/alternative/without_l3agent.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/architecture_index.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Architecture Overview</title>
./helion/architecture/architecture_index.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Changes to Entry-scale KVM with VSA Model</title>
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:          # HOS will look for a file with this name in the config/tls/certs directory.
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:          # into HOS anyway by copying the file into the directory
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:          # be processed by HOS.
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname-suffix: guest
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname-suffix: guest
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname-suffix: mgmt
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname: true
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname-suffix: mgmt
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:      hostname: true
./helion/architecture/examples/changes_entryscale_kvm_vsa.dita:          host_routes:
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale ESX, KVM with VSA Model</title>
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:        <p>This example shows how to integrate <keyword keyref="kw-hos"/> with ESX, KVM with VSA in
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:                <keyword keyref="kw-hos"/> as part of activating the vCenter cluster, and are
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_esx_kvm_vsa_1000.png"
./helion/architecture/examples/entryscale_esx_kvm_vsa.dita:        <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_esx_kvm_vsa_5000.png"
./helion/architecture/examples/entryscale_esx_kvm_vsa_mml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale ESX, KVM with VSA Model with
./helion/architecture/examples/entryscale_esx_kvm_vsa_mml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_esx_kvm_vsa_mml.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_esx_kvm_vsa_mml_1000.png"
./helion/architecture/examples/entryscale_esx_kvm_vsa_mml.dita:        <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_esx_kvm_vsa_mml_5000.png"
./helion/architecture/examples/entryscale_ironic.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale Cloud with Ironic Flat
./helion/architecture/examples/entryscale_ironic.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_ironic.dita:    <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_ironic_1000.png"/></p>
./helion/architecture/examples/entryscale_ironic.dita:    <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_ironic_5000.png"
./helion/architecture/examples/entryscale_kvm_ceph.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with Ceph Model</title>
./helion/architecture/examples/entryscale_kvm_ceph.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_kvm_ceph.dita:    <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_ceph_1000.png"/></p>
./helion/architecture/examples/entryscale_kvm_ceph.dita:    <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_ceph_5000.png"
./helion/architecture/examples/entryscale_kvm_ceph.dita:  hostname-suffix: osdc
./helion/architecture/examples/entryscale_kvm_ceph.dita:  hostname-suffix: osdi
./helion/architecture/examples/entryscale_kvm_ceph_onenetwork.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with Ceph Model</title>
./helion/architecture/examples/entryscale_kvm_ceph_onenetwork.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_kvm_dedicated.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with VSA model with Dedicated
./helion/architecture/examples/entryscale_kvm_dedicated.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_kvm_dedicated.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_vsa_mml_1000.png"
./helion/architecture/examples/entryscale_kvm_dedicated.dita:        <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_vsa_mml_5000.png"
./helion/architecture/examples/entryscale_kvm_vsa.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with VSA Model</title>
./helion/architecture/examples/entryscale_kvm_vsa.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_kvm_vsa.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_vsa_1000.png"/></p>
./helion/architecture/examples/entryscale_kvm_vsa.dita:        <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_vsa_5000.png"
./helion/architecture/examples/entryscale_kvm_vsa.dita:        <p>If you are using <keyword keyref="kw-hos"/> to install the operating system, then an
./helion/architecture/examples/entryscale_swift.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale Swift Model</title>
./helion/architecture/examples/entryscale_swift.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/entryscale_swift.dita:    <p>This example shows how <keyword keyref="kw-hos"/> can be configured to provide a Swift-only
./helion/architecture/examples/entryscale_swift.dita:    <p>If you are using <keyword keyref="kw-hos"/> to install the operating system, then an
./helion/architecture/examples/esx_examples.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>ESX Examples</title>
./helion/architecture/examples/esx_examples.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/example_config_changes.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Changes to Example Configurations</title>
./helion/architecture/examples/example_config_changes.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/example_config_changes.dita:          <p>This document details the changes in the example configurations in <keyword keyref="kw-hos-phrase"/> and the
./helion/architecture/examples/example_config_changes.dita:            impact these changes have on a fresh install of 3.0 as well as the implications for upgrade from <keyword keyref="kw-hos-phrase-21"/>.</p>
./helion/architecture/examples/example_config_changes.dita:            Alternatively, you can also re-use your existing version 2 configurations for a fresh installation of <keyword keyref="kw-hos-phrase"/> - 
./helion/architecture/examples/example_config_changes.dita:          <p>The structure of the example configurations provided in <keyword keyref="kw-hos-phrase"/> has changed
./helion/architecture/examples/example_config_changes.dita:          <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to enable support
./helion/architecture/examples/example_config_changes.dita:          <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to enable support
./helion/architecture/examples/example_config_changes.dita:        <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to enable support
./helion/architecture/examples/example_config_changes.dita:        <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to migrate to the new disk models, you need
./helion/architecture/examples/example_config_changes.dita:        <p id="esx_summary">The Entry-Scale ESX model has changed in <keyword keyref="kw-hos-phrase"/> to be based on <codeph>vxlan</codeph> and includes configuration data for KVM nodes as well,
./helion/architecture/examples/example_config_changes.dita:        <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to migrate to a hybrid cloud, you 
./helion/architecture/examples/example_config_changes.dita:        <p  id="esx_mml_summary">A new example configuration has been added in <keyword keyref="kw-hos-phrase"/> with a dedicated cluster  for Metering, Monitoring and Logging. For more information, see
./helion/architecture/examples/example_config_changes.dita:        <p>As this is a new model, there is no upgrade path from <keyword keyref="kw-hos-phrase-21"/>.</p>
./helion/architecture/examples/example_config_changes.dita:        <p id="glance_summary">Glance Image Caching has been added in <keyword keyref="kw-hos-phrase"/> in a commented-out section in the disk models - see <xref keyref="changes_entryscale_kvm_vsa/glance">here</xref>
./helion/architecture/examples/example_config_changes.dita:        <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to enable image caching, you will need to 
./helion/architecture/examples/example_config_changes.dita:        <p>The L2 gateway is not part of the example configurations in <keyword keyref="kw-hos-phrase"/>. To enable this
./helion/architecture/examples/example_config_changes.dita:        <p id="audit_summary">Support for audit configuation has been added in <keyword keyref="kw-hos-phrase"/> using  the <codeph>audit-settings</codeph>
./helion/architecture/examples/example_config_changes.dita:          <keyword keyref="kw-hos-phrase"/>.
./helion/architecture/examples/example_config_changes.dita:        <p>If you are upgrading from <keyword keyref="kw-hos-phrase-21"/> and want to enable TLS for Internal API Endpoints,
./helion/architecture/examples/example_config_changes.dita:          <keyword keyref="kw-hos-phrase"/>. If you want to enable this functionality, you need to follow the instructions
./helion/architecture/examples/ironic_examples.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Ironic Examples</title>
./helion/architecture/examples/ironic_examples.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/kvm_examples.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>KVM Examples</title>
./helion/architecture/examples/kvm_examples.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/midscale_kvm_vsa.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Mid-scale KVM with VSA Model</title>
./helion/architecture/examples/midscale_kvm_vsa.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/examples/midscale_kvm_vsa.dita:                keyref="kw-hos"/> for increased scale. The controller services are distributed
./helion/architecture/examples/midscale_kvm_vsa.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/midscale_kvm_vsa_1000.png"/></p>
./helion/architecture/examples/midscale_kvm_vsa.dita:        <p><image href="../../../media/hos.docs/exampleconfigs/midscale_kvm_vsa_notes_1000.png"
./helion/architecture/examples/midscale_kvm_vsa.dita:        <p><xref href="../../../media/hos.docs/exampleconfigs/midscale_kvm_vsa_c5000.png"
./helion/architecture/examples/swift_examples.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Swift Examples</title>
./helion/architecture/examples/swift_examples.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/example_configurations.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Example Configurations</title>
./helion/architecture/example_configurations.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/example_configurations.dita:    <p>The <keyword keyref="kw-hos-phrase"/> system ships with a collection of pre-qualified example
./helion/architecture/example_configurations.dita:    <p>The <keyword keyref="kw-hos"/> input model allows a wide variety of configuration parameters
./helion/architecture/example_configurations.dita:    <section id="example_configs"><title><keyword keyref="kw-hos"/> Example Configurations</title>
./helion/architecture/example_configurations.dita:      <!--<p><keyword keyref="kw-hos-phrase"/> ships with two classes of sample cloud models: examples
./helion/architecture/example_configurations.dita:      <p>The following pre-qualified examples are shipped with <keyword keyref="kw-hos-phrase"
./helion/architecture/example_configurations.dita:          keyref="kw-hos"/> model.</p>
./helion/architecture/example_configurations.dita:      <p>In <keyword keyref="kw-hos-phrase"/> there are alternative configurations that we recommend
./helion/architecture/example_configurations.dita:        <li><xref keyref="without_dvr">Configuring <keyword keyref="kw-hos"/> without DVR</xref></li>
./helion/architecture/example_configurations.dita:        <li><xref keyref="without_l3agent">Configuring <keyword keyref="kw-hos"/> with Provider VLANs and Physical Routers
./helion/architecture/input_model/concepts/cloud.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Cloud</title>
./helion/architecture/input_model/concepts/cloud.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/cloud.dita:        cloud, the host prefix, details of external services (NTP, DNS, SMTP) and the firewall
./helion/architecture/input_model/concepts/concepts.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Concepts</title>
./helion/architecture/input_model/concepts/concepts.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/concepts.dita:    <p>An <keyword keyref="kw-hos-phrase"/> cloud is defined by a declarative model that is described in a
./helion/architecture/input_model/concepts/concepts.dita:        significance to <keyword keyref="kw-hos"/>, rather it is the relationships between them that
./helion/architecture/input_model/concepts/concepts.dita:        discussed above, combines it with the service definitions provided by <keyword keyref="kw-hos"/>
./helion/architecture/input_model/concepts/concepts.dita:    <p>The relationship between the file systems on the <keyword keyref="kw-hos"/> deployment server and
./helion/architecture/input_model/concepts/concepts.dita:        directories that are maintained by <keyword keyref="kw-hos"/>.</p>
./helion/architecture/input_model/concepts/configuration_data.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Data</title>
./helion/architecture/input_model/concepts/configuration_data.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/controlplanes.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Control Planes</title>
./helion/architecture/input_model/concepts/controlplanes.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/controlplanes.dita:        <keyword keyref="kw-hos-phrase"/> only supports a single control-plane). Services that need to consume
./helion/architecture/input_model/concepts/controlplanes.dita:        to host the OpenStack services that manage the cloud such as API servers, database servers,
./helion/architecture/input_model/concepts/controlplanes.dita:        host the scale-out OpenStack services such as Nova-Compute or Swift-Object services. This is
./helion/architecture/input_model/concepts/controlplanes.dita:        same <uicontrol>control-plane</uicontrol> then all of those instances will work as a single
./helion/architecture/input_model/concepts/controlplanes_regions.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Control Planes and Regions</title>
./helion/architecture/input_model/concepts/controlplanes_regions.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/diskmodel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Disk Model</title>
./helion/architecture/input_model/concepts/diskmodel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/diskmodel.dita:        specify. The <keyword keyref="kw-hos"/> examples provide some typical configurations. As this is
./helion/architecture/input_model/concepts/diskmodel.dita:        an area that varies with respect to the services that are hosted on a server and the number
./helion/architecture/input_model/concepts/firewallconfiguration.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Firewall Configuration</title>
./helion/architecture/input_model/concepts/firewallconfiguration.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/interfacemodel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Interface Model</title>
./helion/architecture/input_model/concepts/interfacemodel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/loadbalancers.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Load Balancers</title>
./helion/architecture/input_model/concepts/loadbalancers.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/loadbalancers.dita:        <keyword keyref="kw-hos"/> supplied service definition files.</p>
./helion/architecture/input_model/concepts/loadbalancers.dita:        endpoint. <keyword keyref="kw-hos"/> services are configured to only connect to other services via
./helion/architecture/input_model/concepts/loadbalancers.dita:        public and internal access, <keyword keyref="kw-hos"/> will not allow a single
./helion/architecture/input_model/concepts/loadbalancers.dita:    <p>The following diagram shows a possible configuration in which the hostname associated with
./helion/architecture/input_model/concepts/loadbalancers.dita:        the cloud. Within the cloud, <keyword keyref="kw-hos"/> services are configured to use the
./helion/architecture/input_model/concepts/networkgroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Network Groups</title>
./helion/architecture/input_model/concepts/networkgroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/networkgroups.dita:        are used when defining <uicontrol>control-planes</uicontrol>. <keyword keyref="kw-hos"/> also
./helion/architecture/input_model/concepts/networkgroups.dita:        in the service definition files provided by <keyword keyref="kw-hos"/>.</p>
./helion/architecture/input_model/concepts/networkgroups.dita:        consideration. In <keyword keyref="kw-hos"/>, routing is controlled at the
./helion/architecture/input_model/concepts/networkgroups.dita:    <p>As part of the <keyword keyref="kw-hos"/> deployment, networks are configured to act as the
./helion/architecture/input_model/concepts/networkgroups.dita:    <p>Note that <keyword keyref="kw-hos"/> will configure the routing rules on the servers it deploys
./helion/architecture/input_model/concepts/networking.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Networking</title>
./helion/architecture/input_model/concepts/networking.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/networks.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Networks</title>
./helion/architecture/input_model/concepts/networks.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/networktags.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Network Tags</title>
./helion/architecture/input_model/concepts/networktags.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/networktags.dita:    <p>Network tags are defined by some <keyword keyref="kw-hos"/>
./helion/architecture/input_model/concepts/nicmapping.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>NIC Mapping</title>
./helion/architecture/input_model/concepts/nicmapping.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servergroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Groups</title>
./helion/architecture/input_model/concepts/servergroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servergroups.dita:    <p>In the <keyword keyref="kw-hos"/> model we support this configuration by allowing you to define a
./helion/architecture/input_model/concepts/servergroups_failurezones.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Groups and Failure Zones</title>
./helion/architecture/input_model/concepts/servergroups_failurezones.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servergroups_networks.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Groups and Networks</title>
./helion/architecture/input_model/concepts/servergroups_networks.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servergroups_networks.dita:        <keyword keyref="kw-hos"/> model via <uicontrol>server-groups</uicontrol>, each group lists zero or more
./helion/architecture/input_model/concepts/serverroles.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Roles</title>
./helion/architecture/input_model/concepts/serverroles.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servers.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Servers</title>
./helion/architecture/input_model/concepts/servers.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/servers.dita:        your cloud. In addition, in this definition file you can either provide <keyword keyref="kw-hos"/>
./helion/architecture/input_model/concepts/servers.dita:    <p>The address specified for the server will be the one used by <keyword keyref="kw-hos"/> for
./helion/architecture/input_model/concepts/servers.dita:        using <keyword keyref="kw-hos"/> to install the operating system this network must be an untagged
./helion/architecture/input_model/concepts/servers.dita:        VLAN. The first server must be installed manually from the <keyword keyref="kw-hos"/> ISO and this
./helion/architecture/input_model/concepts/services.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Services</title>
./helion/architecture/input_model/concepts/services.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/concepts/services.dita:        <keyword keyref="kw-hos"/>.</p>
./helion/architecture/input_model/concepts/services.dita:    <p>When specifying your <keyword keyref="kw-hos"/> cloud you have to decide where components will
./helion/architecture/input_model/concepts/services.dita:        services? The <keyword keyref="kw-hos"/> supplied examples provide solutions for some typical
./helion/architecture/input_model/configobj/bonddata.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Bond Data</title>
./helion/architecture/input_model/configobj/bonddata.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/bonddataoptions.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Bond Data Options for the "linux" Provider</title>
./helion/architecture/input_model/configobj/bonddataoptions.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/bonddataoptions.dita:    <p>Options used in the <keyword keyref="kw-hos"/> examples are:</p><table frame="all" rowsep="1"
./helion/architecture/input_model/configobj/cloud.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Cloud Configuration</title>
./helion/architecture/input_model/configobj/cloud.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/cloud.dita:        some global values for the <keyword keyref="kw-hos"/> Cloud, as described in the table below.</p>
./helion/architecture/input_model/configobj/cloud.dita:                    <entry>hostname-data (optional)</entry>
./helion/architecture/input_model/configobj/cloud.dita:                                <li>host-prefix - default is to use the cloud name (above)</li>
./helion/architecture/input_model/configobj/clusters.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Clusters</title>
./helion/architecture/input_model/configobj/clusters.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/clusters.dita:                        <entry>The cluster prefix is used in the hostname (see <xref
./helion/architecture/input_model/configobj/configurationdata.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Data</title>
./helion/architecture/input_model/configobj/configurationdata.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/configurationdata.dita:          host_routes:
./helion/architecture/input_model/configobj/configurationdata_ironic.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Data (Ironic)</title>
./helion/architecture/input_model/configobj/configurationdata_ironic.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/configurationdata_neutron.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Data (Neutron)</title>
./helion/architecture/input_model/configobj/configurationdata_neutron.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/configurationdata_neutron.dita:                        <entry>host_routes  (optional)</entry>
./helion/architecture/input_model/configobj/configurationdata_octavia.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Data (Octavia)</title>
./helion/architecture/input_model/configobj/configurationdata_octavia.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/configurationobjects.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Objects</title>
./helion/architecture/input_model/configobj/configurationobjects.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/controlplane.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Control Plane</title>
./helion/architecture/input_model/configobj/controlplane.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/controlplane.dita:                        <entry>The control-plane-prefix is used as part of the hostname (see <xref
./helion/architecture/input_model/configobj/devicegroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Device Groups</title>
./helion/architecture/input_model/configobj/devicegroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/diskmodels.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Disk Models</title>
./helion/architecture/input_model/configobj/diskmodels.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/diskmodels.dita:    <p>If the operating system has been installed by the <keyword keyref="kw-hos"/> installation process
./helion/architecture/input_model/configobj/firewallrules.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Firewall Rules</title>
./helion/architecture/input_model/configobj/firewallrules.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/interfacemodels.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Interface Models</title>
./helion/architecture/input_model/configobj/interfacemodels.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/mtu.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>MTU (Maximum Transmission Unit)</title>
./helion/architecture/input_model/configobj/mtu.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/mtu.dita:            device or bond will be set to the highest MTU value in those groups.  </p>
./helion/architecture/input_model/configobj/networkgroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Network Groups</title>
./helion/architecture/input_model/configobj/networkgroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/networkgroups.dita:       hostname-suffix: extapi
./helion/architecture/input_model/configobj/networkgroups.dita:        hostname-suffix: guest
./helion/architecture/input_model/configobj/networkgroups.dita:        hostname-suffix: mgmt
./helion/architecture/input_model/configobj/networkgroups.dita:        hostname: true
./helion/architecture/input_model/configobj/networkgroups.dita:                        <entry>hostname (optional)</entry>
./helion/architecture/input_model/configobj/networkgroups.dita:                            group will be used to set the hostname of the server.</entry>
./helion/architecture/input_model/configobj/networkgroups.dita:                        <entry>hostname-suffix (optional)</entry>
./helion/architecture/input_model/configobj/networkgroups.dita:                            balancer provides TLS-terminated virtual IP addresses for. In <keyword keyref="kw-hos-phrase-30"/>,
./helion/architecture/input_model/configobj/networks.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Networks</title>
./helion/architecture/input_model/configobj/networks.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/networks.dita:                            from which server addresses may be allocated. The default value is the first host address 
./helion/architecture/input_model/configobj/networks.dita:                            value is the first host address within the <i>CIDR</i> (e.g. the .1
./helion/architecture/input_model/configobj/networks.dita:                            value is the last host address within the <i>CIDR</i> (e.g. the .254 address of a
./helion/architecture/input_model/configobj/networktags.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Network Tags</title>
./helion/architecture/input_model/configobj/networktags.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/networktags.dita:    <p><keyword keyref="kw-hos"/> supports a small number of network tags which may be used to convey
./helion/architecture/input_model/configobj/nicmappings.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>NIC Mappings</title>
./helion/architecture/input_model/configobj/nicmappings.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/nicmappings.dita:        managed by <keyword keyref="kw-hos"/>. An excerpt from <codeph>nic_mappings.yml</codeph>
./helion/architecture/input_model/configobj/nicmappings.dita:                        <entry>The type of port. <keyword keyref="kw-hos-phrase"/> supports "simple-port" and
./helion/architecture/input_model/configobj/passthrough.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Pass Through</title>
./helion/architecture/input_model/configobj/passthrough.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/resources.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Resources</title>
./helion/architecture/input_model/configobj/resources.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/rule.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Rule</title>
./helion/architecture/input_model/configobj/rule.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/servergroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Server Groups</title>
./helion/architecture/input_model/configobj/servergroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/serverroles.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Server Roles</title>
./helion/architecture/input_model/configobj/serverroles.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/servers.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Servers</title>
./helion/architecture/input_model/configobj/servers.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/servers.dita:                        <entry>A string of additional variables to be set when defining the server as a host in Ansible.   
./helion/architecture/input_model/configobj/servers.dita:                            <note type="important">RHEL is only supported for KVM compute hosts</note>
./helion/architecture/input_model/configobj/volumegroups.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/><!-- Configuration Object -  -->Volume Groups</title>
./helion/architecture/input_model/configobj/volumegroups.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/configobj/volumegroups.dita:    <p>The <keyword keyref="kw-hos"/> operating system installation automatically creates a volume-group
./helion/architecture/input_model/configobj/volumegroups.dita:                            by the <keyword keyref="kw-hos"/> operating system install process, the volume group
./helion/architecture/input_model/cpinfofiles/address_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>address_info.yml</title>
./helion/architecture/input_model/cpinfofiles/address_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/clouddiagram_txt.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>CloudDiagram.txt</title>
./helion/architecture/input_model/cpinfofiles/clouddiagram_txt.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/cpinfofiles.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuration Processor Information Files</title>
./helion/architecture/input_model/cpinfofiles/cpinfofiles.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>explain.txt</title>
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster nova-api roles: ['public'] vip-port: 8774 host-port: 8774
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster glance-api roles: ['public'] vip-port: 9292 host-port: 9292
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster keystone-api roles: ['public'] vip-port: 5000 host-port: 5000
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster swift-proxy roles: ['public'] vip-port: 8080 host-port: 8080
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster monasca-api roles: ['public'] vip-port: 8070 host-port: 8070
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster heat-api-cfn roles: ['public'] vip-port: 8000 host-port: 8000
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster ops-console-web roles: ['public'] vip-port: 9095 host-port: 9095
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster heat-api roles: ['public'] vip-port: 8004 host-port: 8004
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster nova-novncproxy roles: ['public'] vip-port: 6080 host-port: 6080
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster neutron-server roles: ['public'] vip-port: 9696 host-port: 9696
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster heat-api-cloudwatch roles: ['public'] vip-port: 8003 host-port: 8003
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster ceilometer-api roles: ['public'] vip-port: 8777 host-port: 8777
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster freezer-api roles: ['public'] vip-port: 9090 host-port: 9090
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster horizon roles: ['public'] vip-port: 443 host-port: 80
./helion/architecture/input_model/cpinfofiles/explain_txt.dita:          10.0.1.5: ip-cluster cinder-api roles: ['public'] vip-port: 8776 host-port: 8776</codeblock>
./helion/architecture/input_model/cpinfofiles/firewall_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>firewall_info.yml</title>
./helion/architecture/input_model/cpinfofiles/firewall_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>net_info.yml</title>
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:              hostname:  &#60;Hostname alias of address allocated for the cluster>
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:          hosts: (list)
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:              hostname:  &#60;Hostname of server in the cluster>
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:              hostname: helion-cp1-vsa-VSA-BLK-mgmt
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:          hosts:
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:          -   hostname: helion-cp1-vsa0001-VSA-BLK-mgmt
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:          -   hostname: helion-cp1-vsa0002-VSA-BLK-mgmt
./helion/architecture/input_model/cpinfofiles/net_info_yml.dita:          -   hostname: helion-cp1-vsa0003-VSA-BLK-mgmt
./helion/architecture/input_model/cpinfofiles/password_change_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>password_change.yml</title>
./helion/architecture/input_model/cpinfofiles/password_change_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/password_change_yml.dita:        <p>This file provides details equivalent to those in private_data_metadata.yml for passwords 
./helion/architecture/input_model/cpinfofiles/password_change_yml.dita:            <keyword keyref="kw-hos"/> documentation</p>
./helion/architecture/input_model/cpinfofiles/private_data_metadata_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>private_data_metadata.yml</title>
./helion/architecture/input_model/cpinfofiles/private_data_metadata_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/route_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>route_info.yml</title>
./helion/architecture/input_model/cpinfofiles/route_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/route_info_yml.dita:        data; this file shows which routes will actually be used. <keyword keyref="kw-hos"/> will
./helion/architecture/input_model/cpinfofiles/route_info_yml.dita:                     &#60;list of hosts using this route></codeblock>
./helion/architecture/input_model/cpinfofiles/server_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>server_info.yml</title>
./helion/architecture/input_model/cpinfofiles/server_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/server_info_yml.dita:         hostname: &#60;hostname of the server>
./helion/architecture/input_model/cpinfofiles/server_info_yml.dita:         hostname: helion-cp1-c1-m1-mgmt
./helion/architecture/input_model/cpinfofiles/service_info_yml.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>service_info.yml</title>
./helion/architecture/input_model/cpinfofiles/service_info_yml.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/cpinfofiles/service_info_yml.dita:               &#60;list of hosts></codeblock>
./helion/architecture/input_model/input_model.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Input Model</title>
./helion/architecture/input_model/input_model.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/input_model.dita:                <li><xref keyref="concepts"><keyword keyref="kw-hos-phrase"/> Concepts</xref>
./helion/architecture/input_model/input_model.dita:                <li><xref keyref="configurationobjects"><keyword keyref="kw-hos-phrase"/> Configuration
./helion/architecture/input_model/input_model_changes30.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Changes to the Input Model for <keyword keyref="kw-hos-phrase-30"/></title>
./helion/architecture/input_model/input_model_changes30.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/input_model_changes30.dita:        <p>The following is a list of links to all the changes in the Input Model for <keyword keyref="kw-hos-phrase-30"/>.</p>
./helion/architecture/input_model/input_model_introduction.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Introduction to the Input Model</title>
./helion/architecture/input_model/input_model_introduction.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/input_model_introduction.dita:    <p>This document describes how the <keyword keyref="kw-hos"/> input model can be used to define and
./helion/architecture/input_model/input_model_introduction.dita:    <p><keyword keyref="kw-hos"/> ships with a set of example input models that can be used as starting
./helion/architecture/input_model/input_model_introduction.dita:            approach used in <keyword keyref="kw-hos-phrase"/> and the core concepts used in describing that
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Neutron Provider VLANs</title>
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:        physical network infrastructure is outside the scope of the <keyword keyref="kw-hos-phrase"/>
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:    <p><keyword keyref="kw-hos-phrase"/> automates the server networking configuration and the Neutron
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:        <keyword keyref="kw-hos"/> <uicontrol>network</uicontrol>, because that VLAN must span all compute
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:        <keyword keyref="kw-hos"/> <uicontrol>network</uicontrol> must be defined with <codeph>tagged-vlan:
./helion/architecture/input_model/other_topics/configneutronprovidervlans.dita:    <p>When the cloud is deployed, <keyword keyref="kw-hos-phrase"/> will create the appropriate bridges on
./helion/architecture/input_model/other_topics/namegeneration.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Name Generation</title>
./helion/architecture/input_model/other_topics/namegeneration.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/namegeneration.dita:        be assigned as the hostname for a server via the network-group configuration (see <xref
./helion/architecture/input_model/other_topics/namegeneration.dita:                    <entry>Comes from the hostname-data section of the <uicontrol>cloud</uicontrol> object
./helion/architecture/input_model/other_topics/namegeneration.dita:                    <entry>comes from the hostname-data section of the <uicontrol>cloud</uicontrol> object
./helion/architecture/input_model/other_topics/namegeneration.dita:                    <entry>comes from the <uicontrol>hostname-suffix</uicontrol> of the network group to
./helion/architecture/input_model/other_topics/namegeneration.dita:                    <entry>Comes from the hostname-data section of the <uicontrol>cloud</uicontrol> object
./helion/architecture/input_model/other_topics/namegeneration.dita:                    <entry>comes from the <uicontrol>hostname-suffix</uicontrol> of the network group to
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Network Route Validation</title>
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:        <keyword keyref="kw-hos"/> release.</p>
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:    hostname-suffix: intapi
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:         hostname-suffix: mgmt
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:         hostname: true
./helion/architecture/input_model/other_topics/networkroutevalidation.dita:         hostname-suffix: iscsi
./helion/architecture/input_model/other_topics/othertopics.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Other Topics</title>
./helion/architecture/input_model/other_topics/othertopics.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/persisteddata.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Persisted Data</title>
./helion/architecture/input_model/other_topics/persisteddata.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/persisteddata.dita:        <keyword keyref="kw-hos"/> will only persist data when the administrator confirms that they are about to
./helion/architecture/input_model/other_topics/persisteddata.dita:    <p>Imagine you have completed your <keyword keyref="kw-hos"/> deployment with servers A, B, and C
./helion/architecture/input_model/other_topics/persisteddata.dita:    <codeblock>cd ~/helion/hos/ansible
./helion/architecture/input_model/other_topics/persisteddata.dita:ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y"</codeblock>
./helion/architecture/input_model/other_topics/persisteddata.dita:    <codeblock>cd ~/helion/hos/ansible
./helion/architecture/input_model/other_topics/persisteddata.dita:ansible-playbook -i hosts/localhost config-processor-run.yml -e free_unused_addresses="y"</codeblock>
./helion/architecture/input_model/other_topics/serverallocation.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Allocation</title>
./helion/architecture/input_model/other_topics/serverallocation.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/servernetworkselection.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Server Network Selection</title>
./helion/architecture/input_model/other_topics/servernetworkselection.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/input_model/other_topics/servernetworkselection.dita:        <li>Looking to see which <uicontrol>network-group</uicontrol> each of those components is
./helion/architecture/input_model/other_topics/servernetworkselection.dita:            <uicontrol>service-component</uicontrol> running on this server, and if so, adding those
./helion/architecture/input_model/other_topics/standalonedeployer.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Standalone Lifecycle Manager</title>
./helion/architecture/input_model/other_topics/standalonedeployer.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/modify/customizing_inputmodel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Customizing the Input Model</title>
./helion/architecture/modify/customizing_inputmodel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/modify/customizing_inputmodel.dita:                separate file systems for the various services that are co-hosted on the controllers in the
./helion/architecture/modify/localizing_inputmodel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Localizing the Input Model</title>
./helion/architecture/modify/localizing_inputmodel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/architecture/modify/localizing_inputmodel.dita:                from PCI bus address to a user specified name. <keyword keyref="kw-hos"/> uses the prefix
./helion/architecture/modify/localizing_inputmodel.dita:                <li><b>Logical name</b> - <keyword keyref="kw-hos"/> uses the form
./helion/architecture/modify/localizing_inputmodel.dita:                <li><b>Type</b> - Only simple-port types are supported in <keyword keyref="kw-hos-phrase"
./helion/architecture/modify/localizing_inputmodel.dita:     hostname-suffix: extapi
./helion/architecture/modify/localizing_inputmodel.dita:                    keyref="input_model"><keyword keyref="kw-hos-phrase"/> Input Model</xref>.</p>
./helion/architecture/modify/localizing_inputmodel.dita:                <xref keyref="input_model"><keyword keyref="kw-hos-phrase"/> Input Model</xref>.</p>
./helion/architecture/modify/modify_entryscale_kvm_vsa.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Modifying the Entry-scale KVM with VSA Model for Your Environment</title>
./helion/architecture/modify/modify_entryscale_kvm_vsa.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/backup_limitations.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Backup and Restore Limitations</title>
./helion/bura/backup_limitations.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/bura_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Planning Your Backup and Restore
./helion/bura/bura_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/bura_overview.dita:    <p><keyword keyref="kw-hos-phrase"/> supports backup and restore of control plane services. It
./helion/bura/cloud_control_plane_backup.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Backing up the Cloud Control Plane </title>
./helion/bura/cloud_control_plane_backup.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/cloud_control_plane_backup.dita:    <section><title>Default Backup and Restore</title><p>As part of the installation procedure in <keyword keyref="kw-hos-phrase"/>, automatic
./helion/bura/cloud_control_plane_backup.dita:      <p>By default, during <keyword keyref="kw-hos-phrase"/> deployment, backup jobs
./helion/bura/cloud_control_plane_backup.dita:        Restore jobs are also deployed for convenience. It is more secure to store those backups
./helion/bura/cloud_control_plane_backup.dita:cd ~/helion/hos/ansible/
./helion/bura/cloud_control_plane_backup.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/bura/cloud_control_plane_backup.dita:ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/bura/cloud_control_plane_backup.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/bura/cloud_control_plane_backup.dita:ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock>This
./helion/bura/disable_bura_before_deployment.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Disabling Backup/Restore before Deployment</title>
./helion/bura/disable_bura_before_deployment.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/disable_bura_before_deployment.dita:      <p>Alternatively, you can disable the creation of those jobs while launching the deployment
./helion/bura/disable_bura_before_deployment.dita:        follows:<codeblock outputclass="nomaxheight">ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_create_backup_jobs": false }' -e '{ "freezer_create_restore_jobs": false }'</codeblock>
./helion/bura/disable_bura_before_deployment.dita:      restore job for each of those backup jobs will also be created by default. These jobs can be
./helion/bura/disable_bura_before_deployment.dita:    <ol><li>Make modifications similar to those discussed above in <codeph>
./helion/bura/disable_bura_before_deployment.dita:          <pre>cd /home/stack/helion/hos/ansible/
./helion/bura/disable_bura_before_deployment.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</pre></li><li>Run the ready deployment playbook. (This will update the scratch/... directories with all of the
./helion/bura/disable_bura_before_deployment.dita:          modifications).<codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li><li> Change directories to scratch
./helion/bura/disable_bura_before_deployment.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible </codeblock></li><li>Run
./helion/bura/disable_bura_before_deployment.dita:          _freezer_manage_jobs.yml<codeblock>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li></ol>
./helion/bura/disable_bura_before_deployment.dita:        <codeblock outputclass="nomaxheight">~/helion/hos/ansible/_freezer_manage_jobs.yml</codeblock></p>
./helion/bura/disable_bura_before_deployment.dita:          <li>To disable the lifecycle manager's jobs, comment out all localhost paragraphs, as
./helion/bura/disable_bura_before_deployment.dita: - hosts: FRE-AGN
./helion/bura/disable_bura_before_deployment.dita:#- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:- hosts: FND-MDB
./helion/bura/disable_bura_before_deployment.dita:- hosts: SWF-PRX[0]
./helion/bura/disable_bura_before_deployment.dita:#- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:- hosts: FRE-AGN
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:#- hosts: FND-MDB
./helion/bura/disable_bura_before_deployment.dita:- hosts: SWF-PRX[0]
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:- hosts: FRE-AGN
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:- hosts: FND-MDB
./helion/bura/disable_bura_before_deployment.dita:#- hosts: SWF-PRX[0]
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:        scripts:<codeblock outputclass="nomaxheight">~/helion/hos/ansible/_freezer_manage_jobs.yml</codeblock></p>
./helion/bura/disable_bura_before_deployment.dita:- hosts: FRE-AGN
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/disable_bura_before_deployment.dita:- hosts: FND-MDB
./helion/bura/disable_bura_before_deployment.dita:- hosts: SWF-PRX[0]
./helion/bura/disable_bura_before_deployment.dita:- hosts: localhost
./helion/bura/freezer_agent.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding the Freezer Agent</title>
./helion/bura/freezer_agent.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/freezer_agent.dita:            <p>For <keyword keyref="kw-hos-phrase"/>,
./helion/bura/freezer_agent.dita:                </ul></p> For <keyword keyref="kw-hos-phrase"/>, you can create a backup using only
./helion/bura/freezer_agent.dita:            "freezer_action" key. For <keyword keyref="kw-hos-phrase"/>, we can define the schedule
./helion/bura/freezer_agent.dita:        <section><title>Restoring your Data</title><p> For <keyword keyref="kw-hos-phrase"/>, 
./helion/bura/freezer_agent.dita:                    <li>restore-from-host</li>
./helion/bura/freezer_agent.dita:                <codeblock>  - freezer-agent --action restore --mode fs --storage swift --restore-abs-path /home/user/tmp --container tmp_backups --backup-name backup1 --restore-from-host ubuntu</codeblock></p>
./helion/bura/freezer_agent.dita:                "hostname": "ubuntu"
./helion/bura/freezer_faq.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>FAQ on Backup and Restore</title>
./helion/bura/freezer_faq.dita:    <p conkeyref="HOS-conrefs/applies-to"/>  
./helion/bura/freezer_faq.dita:    data on those nodes by manually creating backup jobs. Because the Freezer agen and scheduler are installed, you do not have to install them 
./helion/bura/freezer_scheduler.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding the Freezer Scheduler</title>
./helion/bura/freezer_scheduler.dita:      <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/freezer_scheduler.dita:      <section><title>Freezer Scheduler Client-ID</title><p>In <keyword keyref="kw-hos-phrase"/>,
./helion/bura/freezer_scheduler.dita:                nodes.</p><p>There is a client_id for each node and its corresponds to the hostname.
./helion/bura/freezer_scheduler.dita:|         client_id         |          hostname         | description |
./helion/bura/freezer_scheduler.dita:            <p>The scheduler can be used to restore from a different node using the hostname
./helion/bura/freezer_scheduler.dita:                "hostname": "test_machine_1",
./helion/bura/freezer_scheduler.dita:                "hostname": "test_machine_1",
./helion/bura/freezer_ui.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Backing up Via the Horizon UI</title>
./helion/bura/freezer_ui.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/freezer_ui.dita:        <li>List the various nodes ( hosts/servers) on which the freezer-scheduler and freezer agent
./helion/bura/freezer_ui.dita:            End dates and times in creating a job. Please refrain from using those fields. </li>
./helion/bura/restore_previous_backup.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restore from Previous Backup</title>
./helion/bura/restore_previous_backup.dita:              <codeph>&lt;tenant-id>_&lt;hostname></codeph>.</p>
./helion/bura/restore_previous_backup.dita:            <codeph>target-name</codeph>, <codeph>container-name</codeph>), backup_name, hostname.
./helion/bura/restore_shared_controller.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restoring a Shared Controller</title>
./helion/bura/restore_shared_controller.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/helion/hos/ansible 
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
./helion/bura/restore_shared_controller.dita:        <li> Configure the necessary keys used for the database etc: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</codeblock>
./helion/bura/restore_shared_controller.dita:            <codeblock>sudo ssh-keygen -f "/home/dbadmin/.ssh/known_hosts" -R {{ip_of_node_to_replace}}</codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
./helion/bura/restore_shared_controller.dita:          otherwise you need to specify the hostname of your first proxy and the hostname of the
./helion/bura/restore_shared_controller.dita:          replaced.<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname>,&lt;first-proxy-hostname></codeblock>
./helion/bura/restore_shared_controller.dita:        <li> If the node being replaced does not host any swift services then you only need to use
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/helion/hos/ansible/ansible-playbook -i hosts/localhost _deployer_restore_helper.yml -e '{ "old_deployer_hostname": "&lt;here put the hostname of the server that was your deployer>" }'</codeblock>
./helion/bura/restore_shared_controller.dita:      PM HOS 3.0 Page 1 Question: When deployer is down, we can't access either compute or
./helion/bura/restore_shared_controller.dita:        <li>Run the following command to change the host name
./helion/bura/restore_shared_controller.dita:          <codeblock>hostname &lt;here put the hostname of the server that was your deployer></codeblock>
./helion/bura/restore_shared_controller.dita:          Edit the /etc/hosts and replace the default hLinux install name (hlm) with the old
./helion/bura/restore_shared_controller.dita:        <li> Change the hostname of the server so it points to the hostname of the server that was
./helion/bura/restore_shared_controller.dita:          your deployer and update /etc/hosts file according On the lifecycle manager: Become root
./helion/bura/restore_shared_controller.dita:        <li>List jobs <codeblock>freezer-scheduler -c &lt;hostname> job-list</codeblock>
./helion/bura/restore_shared_controller.dita:        <li> Question>> backup.osrc file points to HOSTNAME of the VIP address. <p/>
./helion/bura/restore_shared_controller.dita:        <li>When deployer is reinstalled it won't have details of the HOSTNAME of the VIP in the
./helion/bura/restore_shared_controller.dita:          /etc/hosts file. This command would fail, example <p/>
./helion/bura/restore_shared_controller.dita:          <codeblock>curl http://FoxHOS20HDP-ccp1-vip-KEY-API-mgmt:5000 curl
./helion/bura/restore_shared_controller.dita:Could not resolve host: FoxHOS20HDP-ccp1-vip-KEY-API-mgmt </codeblock>
./helion/bura/restore_shared_controller.dita:        <li>We need to add to the document to edit the /etc/hosts file of the deployer and add the
./helion/bura/restore_shared_controller.dita:          VIP hostname/IP address details. </li>
./helion/bura/restore_shared_controller.dita:          <codeblock>freezer-scheduler -c &lt;hostname> job-stop -j &lt;job-id></codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>freezer-scheduler -c &lt;hostname> job-start -j &lt;job-id></codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock>
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/helion/hos/ansible/
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/localhost _deployer_restore_helper.yml -e '{ "old_deployer_hostname": "&lt;here put the hostname of the server that was your deployer>"}'</codeblock>
./helion/bura/restore_shared_controller.dita:        <li>Run the following command to change the host name
./helion/bura/restore_shared_controller.dita:          <codeblock>hostname &lt;here put the hostname of the server that was your deployer></codeblock></li>
./helion/bura/restore_shared_controller.dita:        <li>Edit the /etc/hosts and replace the default hLinux install name (hlm) with the old
./helion/bura/restore_shared_controller.dita:          deployer name which modified in above step iii. Change the hostname of the server so it
./helion/bura/restore_shared_controller.dita:          points to the hostname of the server that was your deployer and update /etc/hosts file
./helion/bura/restore_shared_controller.dita:          <p>Copy the /etc/hosts file from other surviving controller to replaced controller
./helion/bura/restore_shared_controller.dita:            node</p><p> </p><p><b>Copying the /etc/hosts file from other controller, would address
./helion/bura/restore_shared_controller.dita:              the below concern as well</b></p><p><i>Question>> backup.osrc file points to HOSTNAME
./helion/bura/restore_shared_controller.dita:              the</i></p><p><i>HOSTNAME of the VIP in the /etc/hosts file. This command would fail,
./helion/bura/restore_shared_controller.dita:              example</i></p><p><i>curl <xref href="http://FoxHOS20HDP-ccp1-vip-KEY-API-mgmt:5000"
./helion/bura/restore_shared_controller.dita:              >http://FoxHOS20HDP-ccp1-vip-KEY-API-mgmt:5000</xref></i></p><p><i>curl: (6) Could not
./helion/bura/restore_shared_controller.dita:              resolve host: FoxHOS20HDP-ccp1-vip-KEY-API-mgmt</i></p><p><i>We need to add to the
./helion/bura/restore_shared_controller.dita:              document to edit the /etc/hosts file of the deployer and add the VIP hostname/IP
./helion/bura/restore_shared_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/bura/restore_shared_controller.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock>
./helion/bura/restore_shared_controller.dita:        <li> Restore from an SSH backup HOS 3.0 Page 2 </li>
./helion/bura/start_stop_freezer_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enabling and Disabling Freezer Services</title>
./helion/bura/start_stop_freezer_services.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/start_stop_freezer_services.dita:ansible-playbook -i hosts/verb_hosts freezer-stop.yml</codeblock></p>
./helion/bura/start_stop_freezer_services.dita:ansible-playbook -i hosts/verb_hosts freezer-start.yml</codeblock></p>
./helion/bura/start_stop_freezer_services.dita:          <li>Connect to the concerned host.</li>
./helion/bura/start_stop_freezer_services.dita:          <li>Connect to the concerned host.</li>
./helion/bura/supported_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Default Automatic Backup Jobs</title>
./helion/bura/supported_services.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/troubleshooting.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Freezer Services</title>
./helion/bura/troubleshooting.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/bura/troubleshooting.dita:          dates and times in creating a job. Please refrain from using those fields. </li>
./helion/ceph_twonetworks_config.dita:    <p><image href="../media/hos.docs/entry_scale_kvm_ceph_two_network.png"/></p>
./helion/ceph_twonetworks_config.dita:    <p><xref href="../media/hos.docs/entry_scale_kvm_ceph_two_network_lg.png" scope="external"
./helion/ceph_twonetworks_config.dita:  hostname-suffix: osd
./helion/disk.dita:      <xref href="../hos-html/diskCalc.html" format="html">Try the disk sizing tool</xref></section>
./helion/documentation_updates.dita:  <title>Updates to <keyword keyref="kw-hos"/> Documentation</title>
./helion/documentation_updates.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/documentation_updates.dita:      <p>The <keyword keyref="kw-hos"/> documentation team continuously updates the documents in
./helion/enhancements_to_openstack.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enhancements to OpenStack</title>
./helion/enhancements_to_openstack.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/enhancements_to_openstack.dita:    <section><title>How does <keyword keyref="kw-hos"/> differ from a pure OpenStack cloud
./helion/enhancements_to_openstack.dita:        implementation?</title>There are a number of enhancements in <keyword keyref="kw-hos"/> that
./helion/enhancements_to_openstack.dita:    <section>During installation of <keyword keyref="kw-hos"/>, the core OpenStack services, such as
./helion/enhancements_to_openstack.dita:        keyref="kw-hos"/> lifecycle manager, a collection of Ansible playbooks, script together
./helion/enhancements_to_openstack.dita:      those integrations for you. The playbooks read YAML files that describe your cloud
./helion/enhancements_to_openstack.dita:      based on those configuration settings. In addition, these files, collectively referred to as
./helion/enhancements_to_openstack.dita:    <section><keyword keyref="kw-hos"/> lifecyle manager responsibilities go beyond installation and
./helion/enhancements_to_openstack.dita:        keyref="kw-hos"/> as well using Monasca integration.</section>
./helion/enhancements_to_openstack.dita:    <section>Additionally, the following security features are built into <keyword keyref="kw-hos"
./helion/esx/enable_new_cluster_compute_resource.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enabling a New Cluster as a Compute Resource </title>
./helion/esx/enable_new_cluster_compute_resource.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/eon_logging.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Debug Logging</title>
./helion/esx/eon_logging.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/eon_logging.dita:            <codeblock>~/helion/hos/ansible/roles/eon-common/templates/eon.conf.j2</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>~/helion/hos/ansible/roles/eon-common/templates/logging.conf.j2</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/verb_hosts eon-reconfigure.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>~/helion/hos/ansible/roles/neutron-common/templates/ovsvapp-agent-logging.conf.j2</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/esx/eon_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/esx/eon_logging.dita:          <li>Deploy your changes, specifying the hostname for your OVSAPP host: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/esx/eon_logging.dita:ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml --limit &lt;hostname></codeblock>
./helion/esx/eon_logging.dita:            <p>The hostname of the node can be found in the list generated from the output of the
./helion/esx/eon_logging.dita:            <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock>
./helion/esx/eon_service.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>ESX ON-Boarding Service (EON)</title>
./helion/esx/eon_service.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/eon_service.dita:    <p>The <keyword keyref="kw-hos-phrase"/> EON (ESX on boarding) service is an inventory service, which
./helion/esx/eon_service.dita:      a particular cluster to be a compute resource in <keyword keyref="kw-hos-phrase"/>, EON provisions the
./helion/esx/eon_service.dita:      Nova proxy service VM for each cluster, and OVSvApp service VM, for each ESX host in that
./helion/esx/eon_service.dita:    <p>The integration of EON with <keyword keyref="kw-hos-phrase"/> automates the steps for moving/adding a
./helion/esx/eon_service.dita:| b8f21b2a-ba3f-4279-84e9-3a5e1c7e2287 | HOS-Cluster | domain-c21  | 6542e282-00ac-47a5-9c84-d107604bfb09 | UNSET      | UNSET | esxcluster | imported |
./helion/esx/eon_service.dita:| b8f21b2a-ba3f-4279-84e9-3a5e1c7e2287 | HOS-Cluster | domain-c21  | 6542e282-00ac-47a5-9c84-d107604bfb09 | UNSET      | UNSET | esxcluster | imported |
./helion/esx/eon_service.dita:| b8f21b2a-ba3f-4279-84e9-3a5e1c7e2287 | HOS-Cluster | domain-c21 | 6542e282-00ac-47a5-9c84-d107604bfb09 | UNSET      | UNSET | esxcluster | imported |
./helion/esx/eon_service.dita:        VM per host, does the network plumbing as mentioned in the activation JSON and updates the
./helion/esx/eon_service.dita:          <li><b>Resource Update</b><p>Add additional ESXi hosts to an already activated cluster
./helion/esx/eon_service.dita:              (Host
./helion/esx/eon_service.dita:              Commissioning).<codeblock>eon resource-update &lt;RESOURCE_ID> --action add_host --server-group &lt;SERVER_GROUP></codeblock></p><p>Where
./helion/esx/eon_service.dita:              <codeblock># eon resource-update 9ab2196d-37d2-4006-a2c7-08946b5194ea --action add_host --server-group RACK01 
./helion/esx/eon_service.dita:| state           | host-commission-initiated            |
./helion/esx/eon_service.dita:        <li><xref href="esx_host_commissioning.dita#topic_nfp_xtf_rt"/></li>
./helion/esx/eon_service.dita:        <li><xref href="removing_esx_host_from_cluster.dita#topic_srg_d5h_rt"/></li>
./helion/esx/esx_host_commissioning.dita:        <title><ph conkeyref="HOS-conrefs/product-title"/>ESX Host Commissioning</title>
./helion/esx/esx_host_commissioning.dita:                <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/esx_host_commissioning.dita:                <p>ESX host commissioning is a functionality provided to a user to add additional
./helion/esx/esx_host_commissioning.dita:                        ESX hosts to an already-activated cluster. </p>
./helion/esx/esx_host_commissioning.dita:                <p>The following steps detail how to commission a new a host to an already activated
./helion/esx/esx_host_commissioning.dita:                                <li>Add the host in the maintenance mode to the already activated
./helion/esx/esx_host_commissioning.dita:                                                  href="../../media/esx/eon_service_host1.png"
./helion/esx/esx_host_commissioning.dita:                                                  id="image_it1_3bp_lt"/></p> Any host that is in
./helion/esx/esx_host_commissioning.dita:                                        new host to be activated. <note>You can also activate
./helion/esx/esx_host_commissioning.dita:                                                multiple hosts.</note></li>
./helion/esx/esx_host_commissioning.dita:                                        cluster:<codeblock><codeph># eon resource-update &lt;RESOURCE_ID> --action add_host --server-group &lt;SERVER_GROUP></codeph></codeblock>This
./helion/esx/esx_host_commissioning.dita:                                        process moves the host to a folder in the Datacenter, spawns
./helion/esx/esx_host_commissioning.dita:                                        execution, the host is moved back to the cluster. <p>For
./helion/esx/esx_host_commissioning.dita:                                                example:</p><codeblock># eon resource-update 9ab2196d-37d2-4006-a2c7-08946b5194ea --action add_host --server-group RACK01
./helion/esx/esx_host_commissioning.dita:| state           | host-commission-initiated            |
./helion/esx/esx_host_commissioning.dita:                                                following image shows the host in folder during
./helion/esx/esx_host_commissioning.dita:                                                  href="../../media/esx/eon_service_host2.png"
./helion/esx/esx_host_commissioning.dita:                                        shows the host after successful completion of Host
./helion/esx/esx_host_commissioning.dita:                                                  href="../../media/esx/eon_service_host3.png"
./helion/esx/network_esx_ovsvapp.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Networking for ESXi Hypervisor
./helion/esx/network_esx_ovsvapp.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/network_esx_ovsvapp.dita:    <p>To provide the network as a service for tenant VM's hosted on ESXi Hypervisor, a service VM
./helion/esx/network_esx_ovsvapp.dita:      process. You can mitigate data path traffic loss for VMs on the failed ESX host in that
./helion/esx/network_esx_ovsvapp.dita:      cluster by putting the failed ESX host in the maintenance mode. This, in turn, triggers the
./helion/esx/network_esx_ovsvapp.dita:      vCenter DRS migrates tenant VMs to other ESX hosts within the same cluster. This ensures data
./helion/esx/network_esx_ovsvapp.dita:            at least one round of host mitigation has happened.</p><p>Example:</p><p>
./helion/esx/network_esx_ovsvapp.dita:            the host, goes down before finishing the task). In this case, the cluster will be
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Cluster from the Compute Resource
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:              cluster:<codeblock># nova list --host &lt;hostname>
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:                <p><b>hostname</b>: Specifies hostname of the compute proxy present in that cluster.
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:                  <!--<b>Is of the form &lt;*esx-comp000#-mgmt> (how to get hostname??)</b>--></p>
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:            services:<codeblock>ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname>;
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname>;</codeblock><p>where:
./helion/esx/remove_existing_cluster_compute_resource_pool.dita:                <p><b>hostname</b>: Specifies hostname of the compute proxy present in that
./helion/esx/removing_esx_host_from_cluster.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing an ESX Host from a Cluster</title>
./helion/esx/removing_esx_host_from_cluster.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/removing_esx_host_from_cluster.dita:    <p>This topic describes how to remove an existing ESX host from a cluster.<note>Before
./helion/esx/removing_esx_host_from_cluster.dita:        hosts in that same cluster.</note><ol id="ol_ilq_k5h_rt">
./helion/esx/removing_esx_host_from_cluster.dita:        <li>Right-click and put the host in the maintenance mode. This will automatically migrate
./helion/esx/removing_esx_host_from_cluster.dita:            <b>Host</b>, and then click <b>Enter Maintenance Mode</b>.<?oxy_custom_end?><p><image
./helion/esx/scale_config_changes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Making Scale Configuration Changes</title>
./helion/esx/scale_config_changes.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/scale_config_changes.dita:          directories:<codeblock>cd /home/stack/helion/hos/ansible/roles/nova-common/templates</codeblock></li>
./helion/esx/scale_config_changes.dita:          configuration:<codeblock>cd ~/helion/hos/ansible
./helion/esx/scale_config_changes.dita:          deployment:<codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml;
./helion/esx/scale_config_changes.dita:cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
./helion/esx/scale_config_changes.dita:          playbook:<codeblock>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/esx/vcenter_credential_update.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Updating vCenter Credentials</title>
./helion/esx/vcenter_credential_update.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/esx/vcenter_credential_update.dita:      EON service if any of those values are changed. </p>
./helion/eula_30.dita:<topic xml:lang="en-us" id="eulahos30">
./helion/eula_30.dita:  <title><keyword keyref="kw-hos-tm"/> <keyword keyref="kw-hos-version-30"/>: Software License Terms</title>
./helion/eula_30.dita:      <xref type="section" href="#eulahos30/eula">Part 1: HPE End User License Agreement</xref>
./helion/eula_30.dita:      <xref type="section" href="#eulahos30/auth">Part 2: Additional License Authorizations for Software</xref>
./helion/eula_30.dita:      <xref type="section" href="#eulahos30/ancilliary">Part 3: Ancillary and Open Source Software for Software</xref>
./helion/eula_30.dita:      <keyword keyref="kw-hos-tm"/> Software ("Software"), unless it is subject to a separate agreement between you and
./helion/eula_30.dita:            href="http://docs.hpcloud.com/pdf/static/HPE_HOS_3.0_OpenSource_and_3rd_Party_Licenses.pdf"
./helion/eula_30.dita:            >http://docs.hpcloud.com/pdf/static/HPE_HOS_3.0_OpenSource_and_3rd_Party_Licenses.pdf</xref>.</li>
./helion/eula_30.dita:              terms, then those third party license terms or open source license terms shall govern
./helion/eula_30.dita:              Physical Servers are licensed and those licenses are properly assigned, you may run
./helion/eula_30.dita:              deploy your Cloud Fabric, you must assign to each Physical Server running the host
./helion/eula_30.dita:              license is a licensed host server.</li>
./helion/eula_30.dita:          href="http://docs.hpcloud.com/pdf/static/HPE_HOS_3.0_OpenSource_and_3rd_Party_Licenses.pdf"
./helion/eula_30.dita:          scope="external" format="pdf">http://docs.hpcloud.com/pdf/static/HPE_HOS_3.0_OpenSource_and_3rd_Party_Licenses.pdf</xref>.</p>
./helion/faqs.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Frequently Asked Questions</title>
./helion/faqs.dita:            <entry>How do I move subsets of VMs and volumes vs. the entire host?</entry>
./helion/faqs.dita:              portgroups. hosts and uplinks , then activate. Alternatively, you could make sure that
./helion/faqs.dita:              https://wiki.hpcloud.net/display/iaas/ESX+Scenarios+HOS+3.0</entry>
./helion/faqs.dita:              together with the required portgroups, hosts and uplinks and migrate the service
./helion/faqs.dita:              there tools in HPE Helion OpenStack to check the state of those storage controllers
./helion/faqs.dita:            <entry>When new OSD node is added, host would be added to the deafult root bucket. You
./helion/faqs.dita:              those users?</entry>
./helion/faqs.dita:            <entry>You can run <codeph>nova list --host &lt;hostname of failed compute node>
./helion/faqs.dita:              instances. There is a filter to list the VMs running on particular host. The CLI
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Frequently Asked Questions</title>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Why is HOS better than straight OpenStack? </entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              HOS</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              HOS?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>My HOS installation with HDP does not have direct or indirect access to the
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              Does HOS's Keystone support fernet?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              old backups and delete the data associated with those.</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/identity/identity_ldap.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How to integrate HOS with a CDN</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>My cloud is using a non-HPE Cinder device hosted elsewhere in my enterprise. How
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              switches that are appropriate for use with HOS NIC bonded nodes?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              there tools in HOS to check the state of those storage controllers and attached disks
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              supplied with HOS (eg I want Nuage) ?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/installation/installation_verification.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/security/generate_own_certs.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/installation/installation_overview.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>"http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/architecture/input_model/input_model.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/architecture/example_configurations.html"</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How can I install HOS on a single machine?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>"http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/administration/objectstorage/swift_input_model.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/hardware.html The answer
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/planning/high_availability.html#HP3.0HA__highly_available_app_workloads</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/installation/designate/designate_install_overview.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What guidelines are available within a HOS cloud for creating instances with the
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Can I implement two factor authentication for logging into HOS infrastructure
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What tools / examples are available in HOS for setting up Keystone Federation
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What tools / examples are available in HOS for setting up Keystone Federation
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Our company uses SASL based single sign on. Can HOS be integrated into our
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Does HOS support the concept of OpenStack Cells?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Does HOS support Tricircle?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do you architect a distributed multi-cloud with HOS?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>HOS/ESX How do I setup AZs when using ESX compute nodes ?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do take the advantage of HOS Failures Zones in Ceph OSD Data
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry> For admin (e.g. ssh) access to hosts</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/identity/identity_roles.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Can I get finer grained security with HOS? In other clouds I can control user
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What auditing features are available in HOS? My company requires that an audit
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/compute/enabling_resize.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/installation/configure_vsa.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>"http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/adding_node.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/removing_node.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/live_migration.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I move sub sets of VMs and volumes vs entire host?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/live_migration.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I upgrade/update HOS without disrupting my apps/instances?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_rolling.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I gracefully bring down the entire HOS cloud (anticipating a power
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_down.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I bring up the entire HOS cloud after a power stoppage (planned and
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/restarting_controller_nodes.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_down.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/blockstorage/ceph/add_osd_datadisk.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/start_stop_services.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/objectstorage/add_swift_object_node.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I restore a host post migration?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/bura/cloud_control_plane_backup.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/bura/cloud_control_plane_backup.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_rolling.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_rolling.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>"http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/start_stop_services.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>"http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/recover_compute_node.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/reboot_cloud_rolling.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/recovering_controller_nodes.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How do I recover from a compute failure? (Nova and all hosts are dead)</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              I go to find out if this applies to HOS? </entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              there a way to enable VM live migration across all my servers in HOS?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How can I collect iLO event logs automatically from a HOS node that has
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/objectstorage/validating_swift_recon.html"</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry> http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/alarms.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/objectstorage/validating_swift_recon.html"</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              those users?"</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/objectstorage/validating_swift_recon.html"</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/managing_notificationmethods.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/administration/objectstorage/ring_specifications.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/opsconsole/alarm_explorer.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/opsconsole/alarm_explorer.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/security/arcsight.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/alarms.html</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>http://docs-staging.hpcloud.com/hos.docs-hos-30/#helion/operations/managing_services.html
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What tools can I use with my HOS cloud to monitor the hardware? Is OneView
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:              servers but I don't see that in HOS. What's my alternatives?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>How can I gather OVS or neutron networking troubleshooting information from HOS
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>What tools are available in HOS to help me diagnose a network connectivity issue?
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>Is Jumbo frames supported for HOS networking?</entry>
./helion/FrequentlyAskedQuestionsEXTRAcontent.dita:            <entry>I seem to be getting poor network performance to many of my HOS hosts systems and
./helion/hardware.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Hardware and Software Support Matrix</title>
./helion/hardware.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/hardware.dita:          keyref="kw-hos-phrase"/></p>
./helion/hardware.dita:      <p><keyword keyref="kw-hos-phrase"/> services have been updated to the <xref
./helion/hardware.dita:      <p>For information about the supported hardware in <keyword keyref="kw-hos-phrase"/>, see
./helion/hardware.dita:      <p><keyword keyref="kw-hos-phrase"/> supports the following hardware configurations for a
./helion/hardware.dita:        <p><keyword keyref="kw-hos-phrase"/> supports Fibre Channel and FCoE boot from SAN in
./helion/hardware.dita:        <li><xref href="../hos-html/diskCalcGood.html" format="html">Open the disk calculator.</xref></li>
./helion/hardware.dita:            services than those set by default, then you must increase the number in the <b>Logging
./helion/hardware.dita:            those fields have the following values in the diagram: 60 GB, 64 GB, and 92 GB.</p></li>
./helion/hardware.dita:              <entry>32 GB (memory must be sized based on the virtual machine instances hosted on
./helion/hardware.dita:                support. The CPU cores must be sized based on the VM instances hosted by the Compute
./helion/hardware.dita:      <p><keyword keyref="kw-hos-phrase"/> currently supports the following ESXi versions:</p>
./helion/hardware.dita:              the ESXi hosts)</li>
./helion/hardware.dita:              <entry>32 GB (memory must be sized based on the virtual machine instances hosted on
./helion/hardware.dita:                support. The CPU cores must be sized based on the VM instances hosted by the Compute
./helion/hardware.dita:        Nova compute virtual machine on <keyword keyref="kw-hos-phrase"/>.</p>
./helion/hardware.dita:        vendor as a Nova compute virtual machine on <keyword keyref="kw-hos-phrase"/>.</p>
./helion/hardware.dita:        bare metal instance on <keyword keyref="kw-hos-phrase"/>.</p>
./helion/hardware.dita:        vendor as a bare metal instance on <keyword keyref="kw-hos-phrase"/>.</p>
./helion/horizon/horizon_domainadmin.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Domain Admin Activities with the
./helion/horizon/horizon_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Horizon Service Overview</title>
./helion/horizon/horizon_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/horizon/horizon_overview.dita:    <p>Horizon is the OpenStack service that serves as the basis for the <keyword keyref="kw-hos"/>
./helion/horizon/horizon_overview.dita:    <p>The dashboards provide a web-based user interface to <keyword keyref="kw-hos"/> services
./helion/horizon/horizon_overview.dita:      administrator, you should ask the cloud admin for the host name or public IP address of the
./helion/horizon/horizon_overview.dita:        <li>In the address bar, enter the host name or IP address for the dashboard.<note> If a
./helion/horizon/horizon_overview.dita:        Host tab lists all of the currently configured physical hosts, which Availability Zone each
./helion/horizon/horizon_overview.dita:        host is configured for, the status, and total consumed resources.</p>
./helion/horizon/horizon_overview.dita:          <b>Host aggregates </b>are a grouping of physical servers or hosts with associated
./helion/horizon/horizon_overview.dita:          metadata. A host can belong to multiple aggregates.</li>
./helion/horizon/horizon_overview.dita:        <li>Common use cases for host aggregates include supporting the scheduling of instances to a
./helion/horizon/horizon_overview.dita:          subset of hosts that have a specific capability or flavor such as a specific type of
./helion/horizon/horizon_overview.dita:          storage, lots of RAM, and/or large numbers of processors. <p>Another use case of host
./helion/horizon/horizon_overview.dita:            aggregates is to support the arrangement of hosts into logical groups for load balancing
./helion/horizon/horizon_overview.dita:            and instance distribution. Host aggregates are configured and only viewable by system
./helion/horizon/horizon_overview.dita:            admins. The end user view of a host aggregate is called an Availability Zone.
./helion/horizon/horizon_overview.dita:            Availability zones are created via the Nova API and the host aggregates function. End
./helion/horizon/horizon_overview.dita:            could be deployed on hosts in multiple availability zones. A load balancer can then be
./helion/horizon/horizon_overview.dita:          <p>Additional information on host aggregates and availability zones is available at <xref
./helion/horizon/horizon_overview.dita:              href="http://blog.russellbryant.net/2013/05/21/availability-zones-and-host-aggregates-in-openstack-compute-nova/"
./helion/horizon/horizon_overview.dita:              >http://blog.russellbryant.net/2013/05/21/availability-zones-and-host-aggregates-in-openstack-compute-nova/</xref>
./helion/horizon/horizon_overview.dita:          projects, the images applied to those instances, the status of each instance, flavor type,
./helion/horizon/horizon_overview.dita:          host the volume is hosted on, and the ability for the admin to manage each volume.</li>
./helion/horizon/horizon_overview.dita:          compute host. Flavors are assigned during the creation of a new instance. This panel
./helion/horizon/horizon_overview.dita:          edit or delete flavors. Hosting organizations often use flavors as a form of billing unit;
./helion/horizon/identity/identity_admin.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Administering the Identity Service</title>
./helion/horizon/identity/identity_admin.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/horizon/identity/identity_admin.dita:        lifecycle manager. Also, <keyword keyref="kw-hos"/> installs convenient *.osrc files
./helion/index.dita:  <title><keyword keyref="kw-hos"/>
./helion/index.dita:    <keyword keyref="kw-hos-version-30"/>
./helion/index.dita:    <searchtitle><keyword keyref="kw-hos-phrase"/>: Documentation Home</searchtitle>
./helion/index.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/index.dita:    <p><keyword keyref="kw-hos"/> is a commercial-grade distribution of <tm tmtype="reg"
./helion/index.dita:      clouds on a resilient, maintainable platform. <keyword keyref="kw-hos"/> enables
./helion/index.dita:      <title>Get Started with <keyword keyref="kw-hos"/></title>
./helion/index.dita:          keyref="kw-hos-phrase"/> cloud.</p>
./helion/index.dita:        <li><xref href="overview.dita"><keyword keyref="kw-hos-phrase"/> Overview</xref></li>
./helion/installation/blockstorageconfig_troubleshooting.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Block Storage Backend
./helion/installation/blockstorageconfig_troubleshooting.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/blockstorageconfig_troubleshooting.dita:FATAL: all hosts have already failed -- aborting</codeblock>
./helion/installation/brocade_zone_manager.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Brocade Fibre Channel Zone
./helion/installation/brocade_zone_manager.dita:      host and storage nodes that need to communicate. Zoning allows nodes to communicates with each
./helion/installation/brocade_zone_manager.dita:    <p><keyword keyref="kw-hos-phrase"/> supports Brocade FC Zone Manager. Block Storage (Cinder)
./helion/installation/brocade_zone_manager.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/brocade_zone_manager.dita:                  <li> Detach all volumes from host that were pre-zoned.</li>
./helion/installation/brocade_zone_manager.dita:                scope="external"/> - Parallel volume attach to different compute host fails</li>
./helion/installation/brocade_zone_manager.dita:            <li><keyword keyref="kw-hos"/> Cloud must be successfully deployed. </li>
./helion/installation/brocade_zone_manager.dita:                repository</xref>:<codeblock>cd ~/helion/hos/ansible
./helion/installation/brocade_zone_manager.dita:                processor:<codeblock>cd ~/helion/hos/ansible
./helion/installation/brocade_zone_manager.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock><note>While
./helion/installation/brocade_zone_manager.dita:              directory.<codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/brocade_zone_manager.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/brocade_zone_manager.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/installation/cloudinstallation_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Cloud Installation Overview</title>
./helion/installation/cloudinstallation_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/cloudinstallation_overview.dita:                get an overview of the sample configurations <keyword keyref="kw-hos-phrase"/> offers. We
./helion/installation/configure_3par.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for 3PAR Block Storage
./helion/installation/configure_3par.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/configure_3par.dita:            <codeph>~/helion/hos/ansible/roles/multipath/README.md</codeph> file on the lifecycle
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:          <li>Run the configuration processor: <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/installation/configure_3par.dita:              the Ansible group_vars and host_vars that it produces for subsequent deploy runs. You
./helion/installation/configure_3par.dita:              <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock>
./helion/installation/configure_3par.dita:              then commit those changes to git using the instructions above.</p></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_3par.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_3par.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/installation/configure_ceph.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for Ceph Object and Block Storage
./helion/installation/configure_ceph.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/configure_ceph.dita:          replaces Swift as is the default backend option in <keyword keyref="kw-hos-tm"/>. If you
./helion/installation/configure_ceph.dita:          will then need to re-upload the images to your Glance repository in order for those images
./helion/installation/configure_ceph.dita:        <p><b>Ceph and RADOS:</b> The default <keyword keyref="kw-hos-tm"/> Entry-scale KVM Ceph
./helion/installation/configure_ceph.dita:        <p>The <keyword keyref="kw-hos-tm"/> deployment of the RADOS Gateway features include:</p>
./helion/installation/configure_ceph.dita:            <keyword keyref="kw-hos-tm"/> recommends storing the Ceph RADOS Gateway OSD
./helion/installation/configure_ceph.dita:            journal disk crashes, you might lose your data on those disks. Also, too many journals
./helion/installation/configure_ceph.dita:          <p>Ceph OSD journal size defaults to 5120MB (i.e. 5GB) in <keyword keyref="kw-hos-tm"/>.
./helion/installation/configure_ceph.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</codeblock>
./helion/installation/configure_ceph.dita:        <p>In <keyword keyref="kw-hos-tm"/>, the Ceph is installed on two dedicated cluster nodes
./helion/installation/configure_ceph.dita:          servers. You can install the RADOS Gateway on the cluster node(s) hosting the Ceph Monitor
./helion/installation/configure_ceph.dita:          service or on the <keyword keyref="kw-hos-tm"/> controller nodes. For information on these
./helion/installation/configure_ceph.dita:                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/other_topics/persisteddata.html"
./helion/installation/configure_ceph.dita:                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/concepts/concepts.html#concept_serverroles"
./helion/installation/configure_ceph.dita:                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/concepts/concepts.html#concept_servergroups"
./helion/installation/configure_ceph.dita:                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/configobj/nicmappings.html"
./helion/installation/configure_ceph.dita:              <li><codeph>~/helion/hos/ansible/roles/_CEP-CMN/defaults/main.yml</codeph>
./helion/installation/configure_ceph.dita:                  pools created by <keyword keyref="kw-hos-tm"/>.</p>
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
./helion/installation/configure_ceph.dita:              <li>Snapshot or delete all Nova instances using those images.</li>
./helion/installation/configure_ceph.dita:            </ol><p>After you have finished the Ceph configuration you will need to re-add those
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/installation/configure_ceph.dita:      <sectiondiv outputclass="insideSection"> The RADOS Gateway service can also be co-hosted along
./helion/installation/configure_ceph.dita:        with other <keyword keyref="kw-hos-tm"/> services as listed in the following alternatives
./helion/installation/configure_ceph.dita:          <li>Installing RADOS Gateway on (dedicated) cluster node(s) that host Ceph Monitor
./helion/installation/configure_ceph.dita:          node(s) that host Ceph Monitor service</b><p>The RADOS Gateway can be configured to
./helion/installation/configure_ceph.dita:          install on dedicated cluster node(s) hosting the Ceph Monitor service as follows:</p><ol
./helion/installation/configure_ceph.dita:          that host Ceph Monitor service. Additional RADOS Gateway servers cannot be added if the
./helion/installation/configure_ceph.dita:                <codeblock>cd ~/helion/hos/ansible 
./helion/installation/configure_ceph.dita:                <codeblock>cd ~/helion/hos/ansible 
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_ceph.dita:                <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
./helion/installation/configure_ceph.dita:                    playbooks. Perform these configuration tasks each time either of those playbooks
./helion/installation/configure_ceph.dita:        <p>An <keyword keyref="kw-hos-tm"/> cloud can be deployed with Ceph RADOS Gateway as a
./helion/installation/configure_ceph.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:        <!--<b>Replace swift with rados gateway in pre-deployed Helion cloud </b><p>To replace OpenStack Swift with Ceph RADOS Gateway in a running HOS cloud, perform the steps listed below: </p><ol id="ol_qmq_sgf_sv"><li>Login to controller node. </li><li>Identify the Swift and Ceph service IDs using following commands. Look for Name <codeph>swift</codeph> and <codeph>ceph</codeph> and note the associated ID for eac: <codeblock>source ~/keystone.osrc 
./helion/installation/configure_ceph.dita:openstack service delete &lt;ceph-service-id></codeblock></li><li>Update the service type for Ceph service, by logging in to the deployer node and modifying the <codeph>rgw_keystone_service_type</codeph> in file <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> as follows: <codeblock>rgw_keystone_service_type: object-store</codeblock></li><li>Commit your configuration <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:git commit -m "&lt;commit message>" </codeblock></li><li>Execute the ready-deployment playbook: <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/localhost ready-deployment.yml </codeblock></li><li>Execute the ceph-reconfigure playbook: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li><li>Execute the <codeph>swift-stop playbook</codeph> to stop the Swift service: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_ceph.dita:ansible-playbook -i hosts/verb_hosts swift-stop.yml </codeblock><note><ul id="ul_wng_jhf_sv"><li>The above process does not migrate the existing objects in the Swift store to the Ceph Object Store. So, existing objects will not be available after migrating a running cluster to Ceph Object Store.</li><li>The Swift services should be manually uninstalled and its data should be deleted from the node(s), otherwise services like Monasca will report failure for Swift services, logging services will have logs of Swift services, etc.</li></ul></note></li></ol>-->
./helion/installation/configure_lbaas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Load Balancer as a Service</title>
./helion/installation/configure_lbaas.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/configure_lbaas.dita:      <p>The <keyword keyref="kw-hos"/> Neutron LBaaS service supports several load balancing
./helion/installation/configure_lbaas.dita:          keyref="kw-hos"/>.</p>
./helion/installation/configure_lbaas.dita:          keyref="kw-hos-phrase-30"/>. Also note that the Octavia load balancer provider is listed
./helion/installation/configure_lbaas.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock></li>
./helion/installation/configure_lbaas.dita:      <p id="summary_p">The Octavia Load balancing provider bundled with <keyword keyref="kw-hos-phrase-30"/> is an
./helion/installation/configure_lbaas.dita:        Certificate Authority (CA) certificates included with <keyword keyref="kw-hos-phrase-30"/>
./helion/installation/configure_lbaas.dita:        in <codeph>/home/stack/scratch/ansible/next/hos/ansible/roles/octavia-common/files</codeph>
./helion/installation/configure_lbaas.dita:        to boot those VM's called <codeph>octavia-amphora-haproxy</codeph>. <note type="warning"
./helion/installation/configure_lbaas.dita:      <p>You can download those images form the software depot. Once the image is downloaded it
./helion/installation/configure_lbaas.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible/
./helion/installation/configure_lbaas.dita:$ ansible-playbook -i hosts/verb_hosts -e service_package=&lt;image path/name&gt; service-guest-image.yml</codeblock></li>
./helion/installation/configure_lbaas.dita:| e64cb914-15d2-4ad8-a63c-b7c60a6c232e | octavia-amphora-x64-haproxy_hos-3.0.0 |
./helion/installation/configure_lbaas.dita:| name             | octavia-amphora-x64-haproxy_hos-3.0.0 |
./helion/installation/configure_vsa.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for VSA Block Storage
./helion/installation/configure_vsa.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/configure_vsa.dita:              keyref="kw-hos"/> and comes with a free trial which allows a maximum limit of 50 TB
./helion/installation/configure_vsa.dita:      <p>The process for configuring HPE StoreVirtual VSA for <keyword keyref="kw-hos"/> involves
./helion/installation/configure_vsa.dita:          keyref="kw-hos"/>. Clusters created using CMC manually cannot be reconfigured using the
./helion/installation/configure_vsa.dita:        administer those clusters.</note>
./helion/installation/configure_vsa.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/installation/configure_vsa.dita:ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</codeblock></li>
./helion/installation/configure_vsa.dita:            will make the cluster automation playbook ignore those clusters for
./helion/installation/configure_vsa.dita:          of the VSA hosts in the cluster. You will use these values later when configuring your
./helion/installation/configure_vsa.dita:          <p>This step displays each of the IP addresses for the VSA hosts in the cluster:</p>
./helion/installation/configure_vsa.dita:        <p>In <keyword keyref="kw-hos"/>, cluster creation using HPE StoreVirtual Centralized
./helion/installation/configure_vsa.dita:              is not supported in <keyword keyref="kw-hos"/>. Instead, you must manually add each
./helion/installation/configure_vsa.dita:              notice telling you that your hostnames are not unique. This can be ignored by clicking
./helion/installation/configure_vsa.dita:hplefthand_api_url: https://&lt;vsa-cluster-vip>:8081/lhos
./helion/installation/configure_vsa.dita:                        have chosen to encrypt this password, enter the value in this format: <codeblock>hplefthand_password: {{ '&#60;encrypted vsa-cluster-password>' | hos_user_password_decrypt }}</codeblock>
./helion/installation/configure_vsa.dita:                      <entry>If you set this option as <b>true</b> then the hosts will not be able
./helion/installation/configure_vsa.dita:              <p>[OPTIONAL] <keyword keyref="kw-hos-phrase"/> supports VSA deployment for KVM
./helion/installation/configure_vsa.dita:add the value and the hos_user_password_decrypt filter like so:
./helion/installation/configure_vsa.dita:san_password= {{ '&lt;encrypted san_password>' | hos_user_password_decrypt }}
./helion/installation/configure_vsa.dita:                  encryption for your <codeph>san_password</codeph> is supported. If you chose to
./helion/installation/configure_vsa.dita:                <codeblock>san_password= {{ '&#60;encrypted san_password>' | hos_user_password_decrypt }}</codeblock>
./helion/installation/configure_vsa.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_vsa.dita:              HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph> See <xref
./helion/installation/configure_vsa.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_vsa.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/configure_vsa.dita:          directory:<codeblock>cd ~/helion/hos/ansible
./helion/installation/configure_vsa.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/configure_vsa.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/configure_vsa.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/installation/designate/designate_install_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>DNS Service Installation Overview</title>
./helion/installation/designate/designate_install_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/designate_install_overview.dita:      <p>The <keyword keyref="kw-hos"/> DNS Service supports several different backends for domain
./helion/installation/designate/designate_install_overview.dita:          <keyword keyref="kw-hos"/> install is completed. By default, the example models will
./helion/installation/designate/designate_install_overview.dita:                  <keyword keyref="kw-hos"/> install.</entry>
./helion/installation/designate/designate_install_overview.dita:              <entry>The authoritative DNS server itself is external to <keyword keyref="kw-hos"/>,
./helion/installation/designate/designate_install_overview.dita:              <entry>The authoritative DNS server itself is external to <keyword keyref="kw-hos"/>,
./helion/installation/designate/designate_install_overview.dita:              <entry>Customers hosting public facing services.</entry>
./helion/installation/designate/install_designate_Akamai.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Install the DNS Service with Akamai </title>
./helion/installation/designate/install_designate_Akamai.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/install_designate_Akamai.dita:      <p><keyword keyref="kw-hos"/>DNS Service can be installed to use the <b>Akamai's FastDNS</b>
./helion/installation/designate/install_designate_Akamai.dita:diff --git a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2 b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_Akamai.dita:--- a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_Akamai.dita:+++ b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_BIND.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Install the DNS Service with BIND </title>
./helion/installation/designate/install_designate_BIND.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/install_designate_BIND.dita:      <p><keyword keyref="kw-hos"/> DNS Service and BIND can be installed together instead of the
./helion/installation/designate/install_designate_DynECT.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Install the DNS Service with DynECT </title>
./helion/installation/designate/install_designate_DynECT.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/install_designate_DynECT.dita:      <p><keyword keyref="kw-hos"/>DNS Service can be installed to use the <b>DynECT</b> backend
./helion/installation/designate/install_designate_DynECT.dita:                   username="mycorp-hos-designate",
./helion/installation/designate/install_designate_DynECT.dita:diff --git a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2 b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_DynECT.dita:--- a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_DynECT.dita:+++ b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_DynECT.dita:+                   username="mycorp-hos-designate",
./helion/installation/designate/install_designate_InfoBlox.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Install the DNS Service with InfoBlox </title>
./helion/installation/designate/install_designate_InfoBlox.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/install_designate_InfoBlox.dita:      <p><keyword keyref="kw-hos"/> DNS Service can be installed with the <b>InfoBlox</b> backend
./helion/installation/designate/install_designate_InfoBlox.dita:          server will be deployed onto the <keyword keyref="kw-hos"/> nodes. Instead, zones will be
./helion/installation/designate/install_designate_InfoBlox.dita:          hosted on the <b>InfoBlox</b> servers.</note></p>
./helion/installation/designate/install_designate_InfoBlox.dita:                     username="hos-designate",
./helion/installation/designate/install_designate_InfoBlox.dita:diff --git a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2 b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_InfoBlox.dita:--- a/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_InfoBlox.dita:+++ b/hos/ansible/roles/designate-pool-manager/templates/pool-manager.conf.j2
./helion/installation/designate/install_designate_InfoBlox.dita:+                     username="hos-designate",
./helion/installation/designate/install_designate_powerDNS.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Install the DNS Service with PowerDNS </title>
./helion/installation/designate/install_designate_powerDNS.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/designate/install_designate_powerDNS.dita:      <p><keyword keyref="kw-hos"/> DNS Service defaults to the PowerDNS Backend if another backend
./helion/installation/existing_v2_input_model.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Re-Using an Existing Version 2 Configuration for a
./helion/installation/existing_v2_input_model.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/existing_v2_input_model.dita:            <p>The example configurations have changed significantly  in <keyword keyref="kw-hos-phrase"/> compared to those 
./helion/installation/existing_v2_input_model.dita:                <keyword keyref="kw-hos-phrase"/>, it may be easier to 
./helion/installation/existing_v2_input_model.dita:                deployment, you can re-use these files when performing a clean <keyword keyref="kw-hos-phrase"/> installation,
./helion/installation/existing_v2_input_model.dita:            <p>Note that your new cloud will not have the services that are newly supported in <keyword keyref="kw-hos-phrase"/>.
./helion/installation/existing_v2_input_model.dita:            <p>When <keyword keyref="kw-hos-phrase"/> installs, it will automatically uninstall <codeph>ceiloemeter-agent-central</codeph> and
./helion/installation/existing_v2_input_model.dita:                the <keyword keyref="kw-hos-phrase-30"/> installation.  Refer to the
./helion/installation/existing_v2_input_model.dita:            <p>The example configuration for Entry-scale KVM with Ceph has changed in <keyword keyref="kw-hos-phrase"/> to use a three network model by default.
./helion/installation/existing_v2_input_model.dita:            <p>RADOS Gateway nodes have been added to the Entry-scale KVM with Ceph model in <keyword keyref="kw-hos-phrase"/>. If you choose to re-use your existing configuration, 
./helion/installation/gui_installer.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installing via the GUI </title>
./helion/installation/gui_installer.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/gui_installer.dita:    <section><keyword keyref="kw-hos-phrase"/> comes with a UI installer for installing your cloud
./helion/installation/gui_installer.dita:      installation of <keyword keyref="kw-hos"/> are run for you behind the scenes. </section>
./helion/installation/gui_installer.dita:            included in the <keyword keyref="kw-hos-phrase"/> package. You may find the instructions
./helion/installation/gui_installer.dita:          <note>When following the guidance for operating system installation on those pages, stop
./helion/installation/gui_installer.dita:            <!--  still true ??????????????????/ -->The GUI installer will run those steps, and in
./helion/installation/gui_installer.dita:            ~/helion/hos/ansible/hlm-deploy.yml</b> to uncomment the line containing eon-deploy.yml.
./helion/installation/gui_installer.dita:          need and those that are optional.<table id="table_e5h_wnk_5v">
./helion/installation/gui_installer.dita:        decided to perform those installations yourself, the GUI will perform all deployment tasks
./helion/installation/gui_installer.dita:          <b>http://localhost:8080/dayzero</b></p> As you proceed through the GUI, you need only
./helion/installation/gui_installer.dita:        Example URL looks like this: http://&lt;lifecycle manager-host-ip>:79/dayzero When you
./helion/installation/gui_installer.dita:          <li><xref keyref="input_model"><keyword keyref="kw-hos-phrase"/> Input
./helion/installation/installation_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation Overview</title>
./helion/installation/installation_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/installation_overview.dita:        get an overview of the sample configurations <keyword keyref="kw-hos-phrase"/> offers. We
./helion/installation/installation_troubleshooting.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Installation</title>
./helion/installation/installation_troubleshooting.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/installation_troubleshooting.dita:      <p><b>Issue: Running the hos-init.bash script when configuring your lifecycle manager does not
./helion/installation/installation_troubleshooting.dita:      <p>Part of what the <codeph>~/hos-3.0.0/hos-init.bash</codeph> script does is install git and
./helion/installation/installation_troubleshooting.dita:failed: [localhost] => {"failed": true}
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:          definitions to Cobbler: <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
./helion/installation/installation_troubleshooting.dita:failed: [localhost] => {"failed": true}
./helion/installation/installation_troubleshooting.dita:FATAL: all hosts have already failed -- aborting</codeblock>
./helion/installation/installation_troubleshooting.dita:          keyref="kw-hos"/> ISO, then you should be able to remove all of your nodes from Cobbler
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/installation/installation_troubleshooting.dita:      <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0044,cpn-0045</codeblock>
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:"#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes: destination '192.168.10.0/24' is not defined as a Network in the input model. Add 'external: True' to this host_route if this is for an external network.",
./helion/installation/installation_troubleshooting.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/installation/installation_troubleshooting.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/verb_hosts wipe_disks.yml</codeblock>
./helion/installation/installation_troubleshooting.dita:        nodes then you will need to remove the <codeph>/etc/hos/osconfig-ran</codeph> file on each
./helion/installation/installation_troubleshooting.dita:      <codeblock>sudo rm /etc/hos/osconfig-ran</codeblock>
./helion/installation/installation_troubleshooting.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/verb_hosts monasca-vertica-dbclean.yml</codeblock>
./helion/installation/installation_troubleshooting.dita:FATAL: all hosts have already failed -- aborting</codeblock>
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/installation_troubleshooting.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/installation/installation_troubleshooting.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
./helion/installation/installation_verification.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Verifying the Installation</title>
./helion/installation/installation_verification.dita:    <!--suggested edit to first sentence: <keyword keyref="kw-hos-phrase"/> provides a tool, Tempest,
./helion/installation/installation_verification.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/installation_verification.dita:      <p><keyword keyref="kw-hos-phrase"/> provides a deployment verification tool called Tempest
./helion/installation/installation_verification.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_verification.dita:ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
./helion/installation/installation_verification.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_verification.dita:ansible-playbook -i hosts/verb_hosts tempest-run.yml</codeblock>
./helion/installation/installation_verification.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_verification.dita:ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
./helion/installation/installation_verification.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_verification.dita:ansible-playbook -i hosts/verb_hosts tempest-run.yml  -e run_filter &lt;run_filter_name></codeblock></li>
./helion/installation/installation_verification.dita:      <codeblock>ansible-playbook -i hosts/verb_hosts tempest-run.yml -e run_filter=ci</codeblock>
./helion/installation/installation_verification.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/installation_verification.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock></li>
./helion/installation/installation_verification.dita:          nodes and that you restarted the deploy to include those nodes. However, the nodes are not
./helion/installation/install_entryscale_esx_kvm_vsa.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for Helion Entry Scale ESX, KVM
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:      and adding more ESX hosts to an already activated cluster.</p>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:          <li>It is recommended to have a common shared storage for all the ESXi hosts in a
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            is disabled only for OVSvApp. This is done so that it does not move to a different host.
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            it only for OVSvApp. As a result DRS/HA can migrate OVSvApp to different host, which
./helion/installation/install_entryscale_esx_kvm_vsa.dita:        <!--The configuration files for editing are available at <codeph>~/helion/my_cloud/definition/</codeph>. Refer to the <b><xref href="input_model.dita">Helion OpenStack 2.0 Input Model</xref></b> document for assistance with the configuration files. <note type="important">If you chose to use your first controller node as your deployer, ensure that your <codeph>servers.yml</codeph> file contains the <codeph>is-deployer: true</codeph> notation in your controller options. If you are using a dedicated deployer node you can omit this. Here is an example snippet of a <codeph>servers.yml</codeph> file where a user is using their first controller node as their deployer: <codeblock># Controllers
./helion/installation/install_entryscale_esx_kvm_vsa.dita:          <!--Modify <codeph>~/helion/hos/ansible/hlm-deploy.yml</codeph> to uncomment the line containing <codeph>eon-deploy.yml</codeph>. You must comment the line containing <codeph>ceph-deploy.yml</codeph>, <codeph>vsa-deploy.yml</codeph>, and <codeph>cmc-deploy.yml</codeph>. remove per DOCS2776-->
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            repo</xref>, as follows: <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_esx_kvm_vsa.dita:        <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/localhost bm-power-status.yml</codeblock></li>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:        <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:      of your configuration files is correct for your environment and then commit those changes to
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            location:<codeblock>location /media/cdrom/hos/ hlm-shell-vm.ova</codeblock></li>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <p>vCenter provides centralized management of virtual host and virtual machines from a single
./helion/installation/install_entryscale_esx_kvm_vsa.dita:                hosts in the cluster are part of same RACK.</li>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:        one OVSvApp VM per host and configures the networking as defined in the JSON template. This
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <codeblock>cd /home/stack/helion/hos/ansible;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/localhost ready-deployment.yml;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <codeblock>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*esx-ovsvapp:*esx-compute'
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP</codeblock>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:    <p>This process spawns one compute proxy VM per cluster and one OVSvApp VM per host and
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            :<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock><p>OR<codeblock>cd /home/stack/helion/my_cloud/config/cinder</codeblock></p><p>It
./helion/installation/install_entryscale_esx_kvm_vsa.dita:#vmware_host_ip = &lt;ip_address_of_vcenter>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:#vmware_host_username = &lt;vcenter_username>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:#vmware_host_password = &lt;password>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            :<codeblock>cd /home/stack/helion/hos/ansible;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:ansible-playbook -i hosts/localhost ready-deployment.yml;
./helion/installation/install_entryscale_esx_kvm_vsa.dita:cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:        OVSvApps:<codeblock>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></p>
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| Id  | Binary           | Host                         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 3   | nova-conductor   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 63  | nova-scheduler   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:43.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 66  | nova-conductor   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:48.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 111 | nova-conductor   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 129 | nova-scheduler   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 132 | nova-consoleauth | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:44.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 135 | nova-scheduler   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
./helion/installation/install_entryscale_esx_kvm_vsa.dita:| 138 | nova-compute     | esxhos-joh-esx-comp0001-mgmt | nova     | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
./helion/installation/install_entryscale_kvm.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for Helion Entry-scale Cloud with
./helion/installation/install_entryscale_kvm.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/install_entryscale_kvm.dita:        <li>Unpack the tarball that is in the <codeph>/media/cdrom/hos/</codeph> directory:
./helion/installation/install_entryscale_kvm.dita:            <p>Example for <keyword keyref="kw-hos-tm"/>
./helion/installation/install_entryscale_kvm.dita:            <keyword keyref="kw-hos-version-30"/> Release Candidate 1:</p>
./helion/installation/install_entryscale_kvm.dita:          <codeblock>tar xvf /media/cdrom/hos/hos-3.0.0-20160405T104542Z.tar</codeblock>
./helion/installation/install_entryscale_kvm.dita:        <li>Run the hos-init.bash script which is included in the build: <p>Example for <keyword
./helion/installation/install_entryscale_kvm.dita:              keyref="kw-hos-tm"/>
./helion/installation/install_entryscale_kvm.dita:            <keyword keyref="kw-hos-version-30"/>:</p>
./helion/installation/install_entryscale_kvm.dita:          <codeblock>~/hos-3.0.0/hos-init.bash</codeblock>
./helion/installation/install_entryscale_kvm.dita:              <codeph>hos-init.bash</codeph>. This passphrase is used to protect the key used by
./helion/installation/install_entryscale_kvm.dita:            by setting the <codeph>HOS_INIT_AUTO</codeph> environment variable before running
./helion/installation/install_entryscale_kvm.dita:              <codeph>hos-init.bash</codeph>, like this:</p>
./helion/installation/install_entryscale_kvm.dita:          <codeblock>export HOS_INIT_AUTO=y</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hos/ansible directory
./helion/installation/install_entryscale_kvm.dita:helion/hos/                    Directory contains files used by the installer
./helion/installation/install_entryscale_kvm.dita:<!--        <li>If your <keyword keyref="kw-hos"/> environment is behind a firewall, you need to set the
./helion/installation/install_entryscale_kvm.dita:              <codeblock>~/helion/hos/ansible/roles/logging-common/defaults/main.yml</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:        <li id="hosencrypt">[OPTIONAL] - You can use the <codeph>hosencrypt.py</codeph> script to encrypt your iLo
./helion/installation/install_entryscale_kvm.dita:            <li>Change to the Ansible directory: <codeblock>cd ~/helion/hos/ansible</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:              <codeblock>export HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeblock></li>
./helion/installation/install_entryscale_kvm.dita:              want to encrypt. <codeblock>./hosencrypt.py</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:                  <codeph>export HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption
./helion/installation/install_entryscale_kvm.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:        operating system installation process provided by <keyword keyref="kw-hos"/> or you can use
./helion/installation/install_entryscale_kvm.dita:          <keyword keyref="kw-hos-version"/> then the requirements that have to be met using the
./helion/installation/install_entryscale_kvm.dita:        <li>The operating system must be installed via the HPE Linux for <keyword keyref="kw-hos"/>
./helion/installation/install_entryscale_kvm.dita:      <p>If you chose this method for installing your baremetal hardware, skip forward to the <xref
./helion/installation/install_entryscale_kvm.dita:          <keyword keyref="kw-hos-version"/> then complete all of the steps below.</p>
./helion/installation/install_entryscale_kvm.dita:      <p><b>Using the Automated Operating System Installation Provided by <keyword keyref="kw-hos"
./helion/installation/install_entryscale_kvm.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/localhost bm-power-status.yml</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/localhost bm-reimage.yml [-e nodelist=node1,node2,node3]</codeblock>
./helion/installation/install_entryscale_kvm.dita:        and the ansible <codeph>group_vars</codeph> and <codeph>host_vars</codeph> that it produces
./helion/installation/install_entryscale_kvm.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/installation/install_entryscale_kvm.dita:        <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock>
./helion/installation/install_entryscale_kvm.dita:        each of your configuration files is correct for your environment and then commit those
./helion/installation/install_entryscale_kvm.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/verb_hosts wipe_disks.yml</codeblock></li>
./helion/installation/install_entryscale_kvm.dita:        <li>Run the <codeph>site.yml</codeph> playbook below: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/install_entryscale_kvm.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
./helion/installation/install_entryscale_kvm.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</codeblock>
./helion/installation/install_entryscale_kvm.dita:            <codeph>/etc/hosts</codeph> file from one of the controller nodes.</li>
./helion/installation/install_entryscale_kvm.dita:      <p><keyword keyref="kw-hos"/> supports VSA, 3PAR, and Ceph as block storage backend options.
./helion/installation/install_entryscale_kvm_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>KVM</title>
./helion/installation/install_entryscale_kvm_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/install_entryscale_kvm_overview.dita:            <keyword keyref="kw-hos"/> Entry-scale cloud models that utilize the KVM hypervisor on the
./helion/installation/install_entryscale_swift.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for HPE Helion Entry-scale Cloud
./helion/installation/install_entryscale_swift.dita:    <p conkeyref="HOS-conrefs/applies-to"/>    
./helion/installation/install_entryscale_swift.dita:      hostname-suffix: swift
./helion/installation/install_entryscale_swift.dita:                    keyref="kw-hos-phrase"/> Input Model</xref>.</p>
./helion/installation/install_entryscale_swift.dita:          <p>If you chose to use your first controller node as your lifecycle manager, ensure that your
./helion/installation/install_entryscale_swift.dita:          <p>Optionally, you can use the <codeph>hosencrypt.py</codeph> script to encrypt your iLo
./helion/installation/install_entryscale_swift.dita:            <li>Change to the Ansible directory: <codeblock>cd ~/helion/hos/ansible</codeblock></li>
./helion/installation/install_entryscale_swift.dita:              <codeblock>export HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeblock></li>
./helion/installation/install_entryscale_swift.dita:              want to encrypt. <codeblock>hosencrypt.py</codeblock></li>
./helion/installation/install_entryscale_swift.dita:              HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeph></note></li>
./helion/installation/install_entryscale_swift.dita:        follows: <codeblock>cd ~/helion/hos/ansible
./helion/installation/install_esx_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>ESX Installation Overview</title>
./helion/installation/install_esx_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/install_l2_gateway.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Installing the L2 Gateway Agent for the Networking
./helion/installation/install_l2_gateway.dita:  <body><!--not tested-->    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/install_l2_gateway.dita:        keyref="kw-hos-phrase"/>, is a plug-in to the Neutron networking service that allows two L2
./helion/installation/install_l2_gateway.dita:        agent either during installation of <keyword keyref="kw-hos-phrase"/>, while performing an
./helion/installation/install_l2_gateway.dita:        upgrade to <keyword keyref="kw-hos-phrase"/>, or later by running neutron reconfigure on any
./helion/installation/install_l2_gateway.dita:        existing <keyword keyref="kw-hos-phrase"/> installation.</p>L2 Gateway agent installation
./helion/installation/install_l2_gateway.dita:ovsdb_hosts = hardware_vtep:10.7.31.1:6630</codeblock>
./helion/installation/install_l2_gateway.dita:      <title>Perform standard <keyword keyref="kw-hos"/> installation steps</title></section>
./helion/installation/install_swift_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Swift Installation Overview
./helion/installation/install_swift_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/config_drives.dita:            integrity of a Configuration Drive beyond the initial boot of a host as an administrative user 
./helion/installation/ironic/install_entryscale_ironic.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for HPE Helion Entry-scale Cloud
./helion/installation/ironic/install_entryscale_ironic.dita:        <p conkeyref="HOS-conrefs/applies-to"/>    
./helion/installation/ironic/install_entryscale_ironic.dita:                <li conkeyref="install_entryscale_kvm/hosencrypt"/>
./helion/installation/ironic/install_entryscale_ironic.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/installation/ironic/install_entryscale_ironic.dita:ansible-playbook -i hosts/verb_hosts ironic-cloud-configure.yml
./helion/installation/ironic/install_entryscale_ironic.dita:| d4e2a0ff-9575-4bed-ac5e-5130a1553d93 | ir-deploy-iso-HOS3.0     |
./helion/installation/ironic/install_entryscale_ironic.dita:| b759a1f0-3b33-4173-a6cb-be5706032124 | ir-deploy-kernel-HOS3.0  |
./helion/installation/ironic/install_entryscale_ironic.dita:| ce5f4037-e368-46f2-941f-c01e9072676c | ir-deploy-ramdisk-HOS3.0 |
./helion/installation/ironic/ironic_configuration.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Ironic Flat Network Configuration</title>
./helion/installation/ironic/ironic_configuration.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/ironic_features.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Supported Ironic Features </title>
./helion/installation/ironic/ironic_features.dita:        <p conkeyref="HOS-conrefs/applies-to"/>   
./helion/installation/ironic/ironic_provisioning.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Provisioning Baremetal Nodes</title>
./helion/installation/ironic/ironic_provisioning.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/ironic_provisioning.dita:            <!--  https://etherpad.hpcloud.net/p/HOSKiloIronicHLM2_0-DOC -->
./helion/installation/ironic/ironic_provisioning.dita:| b9499494-7db3-4448-b67f-233b86489c1f | ir-deploy-iso-HOS3.0     |
./helion/installation/ironic/ironic_provisioning.dita:| 8bee92b7-98ae-4242-b80e-1201a475361a | ir-deploy-kernel-HOS3.0  |
./helion/installation/ironic/ironic_provisioning.dita:| 0c889803-469d-4aad-8cf6-3501e39c532c | ir-deploy-ramdisk-HOS3.0 |
./helion/installation/ironic/ironic_provisioning.dita:            The <codeph>ir-deploy-ramdisk-HOS3.0</codeph> image is a traditional boot ramdisk 
./helion/installation/ironic/ironic_provisioning.dita:            <codeph>ir-deploy-iso-HOS3.0</codeph> is an ISO image 
./helion/installation/ironic/ironic_provisioning.dita:            that is supplied as virtual media to the host when using the <codeph>agent_ilo</codeph> driver.
./helion/installation/ironic/ironic_provisioning.dita:| OS-EXT-SRV-ATTR:host                 | -                                             |
./helion/installation/ironic/ironic_provisioning.dita:| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                             |
./helion/installation/ironic/ironic_provisioning.dita:| hostId                               |                                               |
./helion/installation/ironic/ironic_tls.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>TLS Certificates with Ironic Python Agent (IPA) Images</title>
./helion/installation/ironic/ironic_tls.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/ironic_tls.dita:        <p>As part of <keyword keyref="kw-hos-phrase"/>, Ironic Python Agent, better known as IPA in the OpenStack community, 
./helion/installation/ironic/ironic_tls.dita:            an ISO image that is supplied as virtual media to the host when using the agent_ilo driver.
./helion/installation/ironic/ironic_tls.dita:            remote node, and ensure that the TLS endpoints being connected to in <keyword keyref="kw-hos"/> can be trusted.  This 
./helion/installation/ironic/ironic_troubleshooting.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Ironic Installation</title>
./helion/installation/ironic/ironic_troubleshooting.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/ironic_troubleshooting.dita:            <title>No valid host was found. There are not enough hosts available.</title>
./helion/installation/ironic/ironic_troubleshooting.dita:| OS-EXT-SRV-ATTR:host                 | -                                                                                                                                                                                                                                           |
./helion/installation/ironic/ironic_troubleshooting.dita:| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                                                                                                                                                                                                           |
./helion/installation/ironic/ironic_troubleshooting.dita:| fault                                | {"message": "<b>No valid host was found. There are not enough hosts available.</b>", "code": 500, "details": "  File \<b>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/site-packages/nova/conductor/manager.py\"</b>, line 739, in build_instances |
./helion/installation/ironic/ironic_troubleshooting.dita:| hostId                               |                                                                                                                                                                                                                                             |
./helion/installation/ironic/ironic_troubleshooting.dita:cd ~/helion/hos/ansible
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/localhost config-processor-run.yml 
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/localhost ready-deployment.yml 
./helion/installation/ironic/ironic_troubleshooting.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml    
./helion/installation/ironic/ironic_troubleshooting.dita:cd ~/helion/hos/ansible
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/localhost config-processor-run.yml 
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/localhost ready-deployment.yml 
./helion/installation/ironic/ironic_troubleshooting.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/installation/ironic/ironic_troubleshooting.dita:ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml    
./helion/installation/ironic/node_cleaning.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Node Cleaning</title>
./helion/installation/ironic/node_cleaning.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/node_cleaning.dita:cd ~/helion/hos/ansible
./helion/installation/ironic/node_cleaning.dita:cd ~/helion/hos/ansible
./helion/installation/ironic/node_cleaning.dita:ansible-playbook -i hosts/localhost config-processor-run.yml 
./helion/installation/ironic/node_cleaning.dita:ansible-playbook -i hosts/localhost ready-deployment.yml 
./helion/installation/ironic/node_cleaning.dita:cd ~/scratch/ansible/next/hos/ansible 
./helion/installation/ironic/node_cleaning.dita:ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
./helion/installation/ironic/node_config.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Node Configuration</title>
./helion/installation/ironic/node_config.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/node_config.dita:        Drive beyond the initial boot of a host as an administrative user with-in a deployed instance can potentially
./helion/installation/ironic/system_details.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>View Ironic System Details</title>
./helion/installation/ironic/system_details.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/ironic/system_details.dita:| OS-EXT-SRV-ATTR:host                 | helion-cp1-ir-compute0001-mgmt                           |
./helion/installation/ironic/system_details.dita:| OS-EXT-SRV-ATTR:hypervisor_hostname  | ea7246fd-e1d6-4637-9699-0b7c59c22e67                     |
./helion/installation/ironic/system_details.dita:| hostId                               | ecafa4f40eb5f72f7298de0ef51eb7769e3bad47cbc01aa0a076114f |
./helion/installation/ironic/system_details.dita:| ID  | Hypervisor hostname                  | State | Status  |
./helion/installation/ironic/system_details.dita:| host_ip                 | 192.168.12.6                         |
./helion/installation/ironic/system_details.dita:| hypervisor_hostname     | ea7246fd-e1d6-4637-9699-0b7c59c22e67 |
./helion/installation/ironic/system_details.dita:| service_host            | helion-cp1-ir-compute0001-mgmt       |
./helion/installation/ironic/system_details.dita:| Id | Binary           | Host                           | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/installation/multipath_boot_from_san.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Boot from SAN and Multipath Configuration</title>
./helion/installation/multipath_boot_from_san.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/multipath_boot_from_san.dita:                       Boot from SAN support using QLogic-based FlexFabric adapters is latent in <keyword keyref="kw-hos-phrase"/> but 
./helion/installation/multipath_boot_from_san.dita:                       <keyword keyref="kw-hos-phrase"/> customers to not use that configuration. The aim is to fix this outstanding 
./helion/installation/multipath_boot_from_san.dita:                       issue in a patch to <keyword keyref="kw-hos-phrase"/> in the near future. This issue also affects the use of QLogic 
./helion/installation/multipath_boot_from_san.dita:                   In order to allow <keyword keyref="kw-hos-phrase"/> use volumes from a SAN,
./helion/installation/multipath_boot_from_san.dita:        <title>Installing the <keyword keyref="kw-hos-phrase"/> iso for nodes that supports Boot from SAN</title>
./helion/installation/postinstall_tasks.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Common Post-Installation Tasks</title>
./helion/installation/postinstall_tasks.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/postinstall_tasks.dita:          <codeph>~/scratch/ansible/next/hos/ansible/group_vars/</codeph> directory you will find
./helion/installation/postinstall_tasks.dita:        <li><p>Run the following command, which will replace <codeph>/etc/hosts</codeph> on the
./helion/installation/postinstall_tasks.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/postinstall_tasks.dita:ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml</codeblock>
./helion/installation/postinstall_tasks.dita:          <p>As the <codeph>/etc/hosts</codeph> file no longer has entries for Helion lifecycle
./helion/installation/postinstall_tasks.dita:            complete, add "hlm" after "127.0.0.1 localhost". The result will look like this:</p>
./helion/installation/postinstall_tasks.dita:# Localhost Information
./helion/installation/postinstall_tasks.dita:127.0.0.1 localhost hlm</codeblock></li>
./helion/installation/post_install_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Post-Installation Overview</title>
./helion/installation/post_install_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/preinstall_checklist.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Pre-Installation Checklist</title>
./helion/installation/preinstall_checklist.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/preinstall_checklist.dita:      the example configuration you chose for your cloud.</p>
./helion/installation/preinstall_checklist.dita:      <p>Before installing <keyword keyref="kw-hos"/>, the following networks must be provisioned and
./helion/installation/preinstall_checklist.dita:      <p>The IP router used with <keyword keyref="kw-hos"/> must support the updated of its ARP table
./helion/installation/preinstall_checklist.dita:                <entry>VSA host servers</entry>
./helion/installation/preinstall_checklist.dita:      <p>The management network is the backbone used for the majority of <keyword keyref="kw-hos"/>
./helion/installation/preinstall_checklist.dita:        hosts, and Cinder backends through this network. In addition to the control flows, the
./helion/installation/preinstall_checklist.dita:      <p>This server contains the <keyword keyref="kw-hos"/> installer, which is based on Git, Ansible,
./helion/installation/preinstall_checklist.dita:      <p>The Control Plane consists of three servers, in a highly available cluster, that host the
./helion/installation/preinstall_checklist.dita:        core <keyword keyref="kw-hos"/> services including Nova, Keystone, Glance, Cinder, Heat, Neutron,
./helion/installation/preinstall_checklist.dita:    <section id="compute"><title>Compute Hosts</title>
./helion/installation/preinstall_checklist.dita:      <p>One or more KVM Compute servers will be used as the compute host targets for instances.</p>
./helion/installation/preinstall_checklist.dita:      <p>Table to record your Compute host details:</p>
./helion/installation/preinstall_checklist.dita:    <section id="vsa"><title>VSA Hosts</title>
./helion/installation/preinstall_checklist.dita:                  drives with that amount of space)<p>- The VSA appliance deployed on a host is
./helion/installation/preinstall_checklist.dita:                    expected to consume ~40 GB of disk space from the host root disk for ephemeral
./helion/installation/preinstall_checklist.dita:      <p>Table to record your VSA host details:</p>
./helion/installation/preinstall_config.dita:    <title><keyword keyref="kw-hos-version-30"/>Pre-installation Service and Feature Configuration</title>
./helion/installation/preinstall_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Pre-installation Overview</title>
./helion/installation/preinstall_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/preinstall_overview.dita:        <section id="about"><title><keyword keyref="kw-hos-phrase"/> Installation</title>
./helion/installation/preinstall_overview.dita:                full power and flexibility of <keyword keyref="kw-hos-phrase"/>.</p>
./helion/installation/rhel/install_rhel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Using LifeCycle Manager to Deploy RHEL Compute Nodes</title>
./helion/installation/rhel/install_rhel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/rhel/install_rhel_uefi.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>RHEL Compute Node Installation on UEFI Node</title>
./helion/installation/rhel/install_rhel_uefi.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/rhel/install_rhel_uefi.dita:            <p>If you want to use <keyword keyref="kw-hos-phrase"/> lifecycle manager to install RHEL 7.2 on UEFI nodes,
./helion/installation/rhel/install_rhel_uefi.dita:                <li>Edit <codeph>~/helion/hos/ansible/roles/cobbler/templates/cobbler.dhcp.template.j2</codeph> and 
./helion/installation/rhel/install_rhel_uefi.dita:cd ~/helion/hos/ansible
./helion/installation/rhel/install_rhel_uefi.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/installation/rhel/install_rhel_uefi.dita:cd ~/helion/hos/ansible
./helion/installation/rhel/install_rhel_uefi.dita:ansible-playbook -i hosts/localhost bm-reimage.yml [-e nodelist=node1,node2,node3]
./helion/installation/rhel/install_rhel_uefi.dita:                <li>Edit <codeph>~/helion/hos/ansible/roles/cobbler/templates/cobbler.dhcp.template.j2</codeph> and 
./helion/installation/rhel/install_rhel_uefi.dita:cd ~/helion/hos/ansible
./helion/installation/rhel/install_rhel_uefi.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/installation/rhel/provisioning_rhel.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Provisioning RHEL Yourself</title>
./helion/installation/rhel/provisioning_rhel.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/rhel/provisioning_rhel.dita:                it can be added to a new or existing <keyword keyref="kw-hos-phrase"/> cloud.</p>
./helion/installation/rhel/provisioning_rhel.dita:    inet 127.0.0.1/8 scope host lo
./helion/installation/rhel/provisioning_rhel.dita:    inet6 ::1/128 scope host
./helion/installation/rhel/provisioning_rhel.dita:# Disable "ssh hostname sudo &lt;cmd>", because it will show the password in clear.
./helion/installation/rhel/provisioning_rhel.dita:#         You have to run "ssh -t hostname sudo &lt;cmd>".
./helion/installation/rhel/provisioning_rhel.dita:            <p>You need to set up a yum repository, either external or local, containing a <keyword keyref="kw-hos"/>
./helion/installation/rhel/rhel_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>RHEL Compute Node Installation Overview</title>
./helion/installation/rhel/rhel_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/rhel/rhel_overview.dita:                <p><keyword keyref="kw-hos-phrase"/> supports RHEL compute nodes, specifically RHEL 7.2. 
./helion/installation/rhel/rhel_overview.dita:                    HPE does not ship a RedHat iso with <keyword keyref="kw-hos"/> so you will need to 
./helion/installation/rhel/rhel_overview.dita:                <p>There are two approaches for deploying RHEL compute nodes in <keyword keyref="kw-hos"/>:
./helion/installation/rhel/rhel_preinstall.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>RHEL Pre-Installation Checks</title>
./helion/installation/rhel/rhel_preinstall.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/rhel/rhel_preinstall.dita:                <keyword keyref="kw-hos-phrase"/> uses iptables to secure access to lifecycle manager network interfaces and on 
./helion/installation/rhel/rhel_preinstall.dita:                This sample configuration is inappropriate for <keyword keyref="kw-hos"/> operation and the node will not be able to run HOS 
./helion/installation/rhel/rhel_preinstall.dita:                <keyword keyref="kw-hos-phrase"/> install will prevent the installation of the sample files.                
./helion/installation/rhel/rhel_preinstall.dita:                However, if these files do exist, there are a number of steps that you must follow before you install <keyword keyref="kw-hos-phrase"/>.
./helion/installation/rhel/rhel_preinstall.dita:-A INPUT -j REJECT --reject-with icmp-host-prohibited
./helion/installation/rhel/rhel_preinstall.dita:-A FORWARD -j REJECT --reject-with icmp-host-prohibited
./helion/installation/rhel/rhel_preinstall.dita:                <keyword keyref="kw-hos"/> components and OpenStack components manage <i>interface specific</i> rules.
./helion/installation/rhel/rhel_preinstall.dita:                            interfaces not used by <keyword keyref="kw-hos"/>), you will need to reboot the system to activate the new settings.
./helion/installation/rhel/rhel_preinstall.dita:                <li>Ensure that any remaining rules are limited to interfaces not used by <keyword keyref="kw-hos"/>.  To delete
./helion/installation/rhel/rhel_preinstall.dita:iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited
./helion/installation/rhel/rhel_toc.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>RHEL Installation Overview</title>
./helion/installation/rhel/rhel_toc.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/using_git.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using Git for Configuration
./helion/installation/using_git.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/installation/using_git.dita:    <p>In <keyword keyref="kw-hos-phrase"/>, a local git repository is used to track configuration changes
./helion/installation/using_git.dita:      <p>On a system new to <keyword keyref="kw-hos-phrase"/>, the lifecycle manager will prepare a git repository
./helion/installation/using_git.dita:          <codeph>ansible-playbook -i hosts/localhost git-00-initialise.yml</codeph>.</p>
./helion/installation/using_git.dita:            <dt>hos</dt>
./helion/installation/using_git.dita:            <dd>This branch begins life as a copy of the first 'hos' drop. It is onto this branch
./helion/installation/using_git.dita:              ansible playbooks need. This includes the <codeph>verb_hosts</codeph> file that
./helion/installation/using_git.dita:                <codeph>~/scratch/ansible/next/hos/ansible</codeph> from which the main deployment
./helion/installation/using_git.dita:            <dd>This branch hosts the most recent commit that will be appended to the ansible
./helion/installation/using_git.dita:            <dd>This branch hosts the most recent commit that will be appended to the cp-persistent
./helion/installation/using_git.dita:          <li>ansible-playbook -i hosts/localhost</li>
./helion/installation/using_git.dita:          <li>-i hosts/localhost git-01-receive-new.yml</li>
./helion/installation/using_git.dita:        </ol> The third playbook puts the new content directly onto the 'hos' branch. After it runs,
./helion/installation/using_git.dita:        you will notice a new commit on the 'hos' branch with the latest upstream deployer content
./helion/installation/using_git.dita:        introduction of the git workflow. The hos branch will be merged to the site branch; if there
./helion/installation/using_git.dita:        <codeblock>ansible-playbook -i hosts/localhost config-proessor-run.yml </codeblock> However,
./helion/installation/using_git.dita:      <p>The user readies a deployment area by running ansible-playbook -i hosts/localhost
./helion/installation/using_git.dita:        -i hosts/verb_hosts site.yml </p>
./helion/installation/using_git.dita:            processor:<codeblock>cd ~/helion/hos/ansible
./helion/installation/using_git.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/installation/using_git.dita:            to produce the required configuration processor output from those changes. Review the
./helion/installation/using_git.dita:            <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/installation/using_git.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/installation/using_git.dita:ansible-playbook -i hosts/verb_hosts site.yml
./helion/introduction.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Overview</title>
./helion/introduction.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/introduction.dita:        <p>This section provides general information on the <keyword keyref="kw-hos-phrase"/> release.</p>
./helion/knownissues30.dita:FATAL: all hosts have already failed -- aborting
./helion/knownissues30.dita:                generated by earlier versions of <keyword keyref="kw-hos"/> could prevent hLinux 4.4 
./helion/knownissues30.dita:            <p>During the <keyword keyref="kw-hos"/> upgrade this file will be re-generated with 
./helion/knownissues30.dita:            <p><b>Live Migration between KVM and RHEL Compute Hosts Isn't Supported</b></p>
./helion/knownissues30.dita:            <p>If you are using both Linux for HPE Helion (KVM) and RHEL compute hosts, you cannot
./helion/knownissues30.dita:                live migrate instances between them. Instances on KVM hosts can only be live
./helion/knownissues30.dita:                migrated to other KVM hosts and the same for RHEL hosts. For more details about live
./helion/knownissues30.dita:            <p><!-- DOCS-2878 --> Although <keyword keyref="kw-hos-version-30"/> supports encrypted
./helion/knownissues30.dita:                PKI tokens. <keyword keyref="kw-hos-phrase-30"/> does not support using PKI 
./helion/knownissues30.dita:            <p>In <keyword keyref="kw-hos"/> 2.X and <keyword keyref="kw-hos-phrase-30"/> the
./helion/knownissues30.dita:                <codeblock>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
./helion/knownissues30.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock>
./helion/knownissues30.dita:            <p>In Helion OpenStack version 2.x, the keystone role for submitting cross-tenant metrics was called  'monasca-agent'. In HOS 3.0, the keystone role for submitting cross-tenant metrics is now called 'monitoring-delegate'. Any user who submits cross-tenant metrics without the 'monitoring-delegate' role will now receive a '403 Forbidden' http response.</p>
./helion/knownissues30.dita:            <p>You will need to change your input model if your system includes RHEL nodes that are configured to boot from a multipath device (boot from SAN). Otherwise those nodes will not be able to access their boot devices when you reboot them.
./helion/knownissues30.dita:                Most of the example models supplied with HOS 3.0 include a blacklist with wildcards like this:</p>
./helion/knownissues30.dita:                disabled, this feature will generate false positives and place hosts into maintenance mode.</p>
./helion/knownissues30.dita:Fact cache entry for host COMPUTE-0002 isn't valid, deleting and failing
./helion/knownissues30.dita:Fact cache entry for host COMPUTE-0001 isn't valid, deleting and failing
./helion/knownissues30.dita:            <p>The Sherpa Service is no longer provided with <keyword keyref="kw-hos-phrase-30"/>.</p>
./helion/liberty_features.dita:<topic id="HOS3LibertyFeatures">
./helion/liberty_features.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>OpenStack Liberty Features</title>
./helion/liberty_features.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/liberty_features.dita:      <title><keyword keyref="kw-hos-phrase"/> Liberty Features</title>
./helion/liberty_features.dita:      <!-- Original Source Material - https://wiki.hpcloud.net/display/core/OpenStack+Upstream+Features+in+HOS+V3.0-->
./helion/liberty_features.dita:        identified below) are enabled in the <keyword keyref="kw-hos-phrase"/> standard settings and
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/NovaFeatures">Compute Service
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/NeutronFeatures">Networking
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/GlanceFeatures">Image Service
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/SwiftFeatures">Object Storage
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/CinderFeatures">Block Storage
./helion/liberty_features.dita:        <!-- <li><xref href="liberty_features.dita#HOS3LibertyFeatures/CephFeatures">Ceph Features</xref></li> -->
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/KeystoneFeatures">Identity Service
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/CeilometerFeatures">Data
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/HeatFeatures">Orchestration
./helion/liberty_features.dita:         <li><xref href="liberty_features.dita#HOS3LibertyFeatures/IronicFeatures">Bare Metal
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/HorizonFeatures">Dashboard
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/MonascaFeatures">Monitoring
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/FreezerFeatures">Backup, Restore,
./helion/liberty_features.dita:        <li><xref href="liberty_features.dita#HOS3LibertyFeatures/BarbicanFeatures">Key Manager
./helion/liberty_features.dita:                <entry>Migration to Specified Host</entry>
./helion/liberty_features.dita:                <entry>Mark Host Down *</entry>
./helion/liberty_features.dita:                <entry>Libvirt KVM (x86) - Host Linux Only </entry>
./helion/liberty_features.dita:      <p>Freezer is all new for the <keyword keyref="kw-hos"/> Liberty release.</p>
./helion/liberty_features.dita:      <p>Barbican is all new for the <keyword keyref="kw-hos"/> Liberty release.</p>
./helion/metering/metering.dita:      Sample API is disabled by default in <keyword keyref="kw-hos-phrase-30"/> and is configured with a pipeline
./helion/metering/metering.dita:      meters to the API pipeline. Ensure that only those meters are added to this pipeline which are
./helion/metering/metering.dita:      each of those APIs in Ceilometer's policy.json. (<b>/etc/ceilometer/policy.json</b> on
./helion/metering/metering_apis.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering APIs</title>
./helion/metering/metering_apis.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_apis.dita:      Post Sample API is disabled by default in <keyword keyref="kw-hos-phrase"/> and its configured
./helion/metering/metering_apis.dita:      Exercise caution when adding meters to the API pipeline. Ensure that only those meters are
./helion/metering/metering_apis.dita:      each of those APIs in Ceilometer's policy.json. (<b>/etc/ceilometer/policy.json</b> on
./helion/metering/metering_apis.dita:      <keyword keyref="kw-hos"/> uses the Apache2 Web Server to provide API access. It is possible
./helion/metering/metering_bestpractice.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Optimizing the Ceilometer Metering
./helion/metering/metering_bestpractice.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_bestpractice.dita:            <p><keyword keyref="kw-hos"/> uses Apache2 Web Server to serve the API access. It is possible
./helion/metering/metering_bestpractice.dita:&lt;VirtualHost *:8777>
./helion/metering/metering_bestpractice.dita:&lt;/VirtualHost></codeblock>
./helion/metering/metering_bestpractice.dita:            <p>In <keyword keyref="kw-hos-phrase"/> we have adopted a strategy to reduce the amount of data that is sent
./helion/metering/metering_bestpractice.dita:            <p>Post Sample API is disabled by default in <keyword keyref="kw-hos-phrase"/> and its configured
./helion/metering/metering_bestpractice.dita:                those meters are added to this pipeline which are already present in the
./helion/metering/metering_bestpractice.dita:                <li>Custom rule hp_disabled_rule:not_implemented is added to each of those APIs in
./helion/metering/metering_bestpractice.dita:backend_url = &lt;IP address of Zookeeper host: port> (port is usually 2181 as a zookeeper default)
./helion/metering/metering_bestpractice.dita:                notifications, then the recommendation is to use a separate source for those events,
./helion/metering/metering_components.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Service
./helion/metering/metering_components.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_components.dita:    <p>The Metering service is installed during the <keyword keyref="kw-hos"/> installation process. During
./helion/metering/metering_failover_ha.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Failover HA
./helion/metering/metering_failover_ha.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_failover_ha.dita:    <section> In the <keyword keyref="kw-hos"/> environment, the Ceilometer metering service supports
./helion/metering/metering_failover_ha.dita:backend_url = &lt;IP address of Zookeeper host: port> (port is usually 2181 as a zookeeper default)
./helion/metering/metering_failover_ha.dita:        the recommendation is to use a separate source for those events, especially if the expected
./helion/metering/metering_metertypes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Available Meter
./helion/metering/metering_metertypes.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_metertypes.dita:      <title><keyword keyref="kw-hos"/> Default Meters</title>
./helion/metering/metering_metertypes.dita:      <p>These meters are installed and enabled by default during an <keyword keyref="kw-hos"/>
./helion/metering/metering_notifications.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Service Notifications</title>
./helion/metering/metering_notifications.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_notifications.dita:    <section> In <keyword keyref="kw-hos-phrase"/> we have adopted a strategy to reduce the amount
./helion/metering/metering_notifications.dita:      strategy. You should run <b>ceilometer-reconfigure.yml</b> to make those changes stick. <p>As
./helion/metering/metering_notifications.dita:        environment, those need to be verified appropriately. </p>The pipeline configuration file is
./helion/metering/metering_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Metering Service (Ceilometer) Overview</title>
./helion/metering/metering_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_overview.dita:      <title>New Metering Functionality in <keyword keyref="kw-hos-phrase"/></title>
./helion/metering/metering_overview.dita:          overall <keyword keyref="kw-hos"/> performance prior to deploying any Ceilometer
./helion/metering/metering_overview.dita:        <li>The Ceilometer Alarms API is disabled by default. <keyword keyref="kw-hos-phrase"/>
./helion/metering/metering_rbac.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Setting Role-based
./helion/metering/metering_rbac.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_reconfig.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceilometer Metering Service Config and
./helion/metering/metering_reconfig.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/metering/metering_reconfig.dita:    <section id="deploy"><title>Deployment</title> <keyword keyref="kw-hos-phrase"/> automatically deploys
./helion/metering/metering_reconfig.dita:&lt;VirtualHost *:8777>
./helion/metering/metering_reconfig.dita:&lt;/VirtualHost>&lt;/ipaddress></codeblock>
./helion/metering/metering_reconfig.dita:      file is configured during <keyword keyref="kw-hos"/> deployment and preferably should be changed by
./helion/metering/metering_reconfig.dita:      box in <keyword keyref="kw-hos-phrase"/>. The list of meters supported out of the box for these services
./helion/metering/metering_reconfig.dita:      lifecycle manager distribution. For Ceilometer, the playbook included with <keyword keyref="kw-hos"/>
./helion/metering/metering_reconfig.dita:      <codeblock>cd /home/stack/helion/hos/ansible/
./helion/metering/metering_reconfig.dita:ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock>
./helion/metering/metering_reconfig.dita:      <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock> Run
./helion/metering/metering_reconfig.dita:      <codeblock>cd /home/stack/scratch/ansible/next/hos/ansible/
./helion/metering/metering_reconfig.dita:ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</codeblock>
./helion/networking/configure_mtu.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Maximum Transmission Units in Neutron</title>
./helion/networking/configure_mtu.dita:  <body><!--not tested-->    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/configure_mtu.dita:        In <keyword keyref="kw-hos-phrase-30"/>, the DHCP server advertises to all
./helion/networking/configure_mtu.dita:     hostname-suffix: guest
./helion/networking/configure_mtu.dita:          processor:<codeblock>cd ~/helion/hos/ansible
./helion/networking/configure_mtu.dita:ansible-playbook -i hosts/verb_hosts config-processor-run.yml</codeblock>and
./helion/networking/configure_mtu.dita:          deployment:<codeblock>ansible-playbook -i hosts/verb_hosts ready-deployment.yml</codeblock>Then
./helion/networking/configure_mtu.dita:          playbook:<codeblock>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</codeblock>followed
./helion/networking/configure_mtu.dita:          nova-reconfigure.yml:<codeblock>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml </codeblock>Then
./helion/networking/configure_mtu.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</codeblock>Note:
./helion/networking/create_ha_router.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating a Highly Available Router</title>
./helion/networking/create_ha_router.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/create_ha_router.dita:        <codeblock>$ neutron l3-agent-list-hosting-router &lt;router_id&gt;</codeblock></li>
./helion/networking/create_ha_router.dita:        <li>Within 10 seconds, check again to see which L3 agent is hosting the router
./helion/networking/create_ha_router.dita:        <codeblock>$ neutron l3-agent-list-hosting-router &lt;router_id&gt;</codeblock></li>
./helion/networking/fwaas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using Firewall as a Service (FWaaS)</title>
./helion/networking/fwaas.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/fwaas.dita:      <p><keyword keyref="kw-hos"/> must be installed. </p>
./helion/networking/fwaas.dita:      <title><keyword keyref="kw-hos-phrase"/> FWaaS Configuration</title>
./helion/networking/fwaas.dita:            attempted by hostile attackers. Using <b>deny</b> will drop all of the packets, making
./helion/networking/lbaas_admin.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Load Balancing as a Service
./helion/networking/lbaas_admin.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/lbaas_admin.dita:    <p><b><keyword keyref="kw-hos-phrase"/> LBaaS Configuration</b></p>
./helion/networking/lbaas_admin.dita:    <p><keyword keyref="kw-hos-phrase"/> can support either LBaaS v1 or LBaaS v2 to allow for wide ranging
./helion/networking/lbaas_admin.dita:      support will be needed from Helion Professional Services and your chosen load balancer
./helion/networking/lbaas_admin.dita:      HAProxy.  Octavia is the default load balancing provider in <keyword keyref="kw-hos-phrase-30"
./helion/networking/lbaas_admin.dita:      v2 is installed by default with <keyword keyref="kw-hos"/> and requires minimal configuration to
./helion/networking/lbaas_admin.dita:      <p><b><keyword keyref="kw-hos"/> LBaaS v1</b></p>
./helion/networking/lbaas_admin.dita:          appropriate installation preparations during <keyword keyref="kw-hos"/> installation since LBaaS v2
./helion/networking/lbaas_admin.dita:          <keyword keyref="kw-hos-phrase"/>: Using Git for Configuration Management</xref> 
./helion/networking/lbaas_admin.dita:      <p><b><keyword keyref="kw-hos"/> LBaaS v2</b></p>
./helion/networking/lbaas_admin.dita:        <li><keyword keyref="kw-hos"/> must be installed for LBaaS v2.</li>
./helion/networking/multinetwork.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Multiple Provider Networks</title>
./helion/networking/multinetwork.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/multinetwork.dita:    <p><keyword keyref="kw-hos-phrase"/> Multiple Provider Network Configuration</p>
./helion/networking/multinetwork.dita:        keyref="kw-hos-phrase"/> Multiple Provider Network.</p>
./helion/networking/multinetwork.dita:      <p><keyword keyref="kw-hos"/> must be installed. </p>
./helion/networking/multinetwork.dita:      <p><keyword keyref="kw-hos-phrase"/> Multiple Provider Network Configuration</p>
./helion/networking/multinetwork.dita:          keyref="kw-hos-phrase"/> software.</p>
./helion/networking/multinetwork.dita:      <p><keyword keyref="kw-hos-phrase"/> automates the server networking configuration and the
./helion/networking/multinetwork.dita:      <p>A network group used for provider VLANs may contain only a single <keyword keyref="kw-hos"
./helion/networking/multinetwork.dita:        nodes/controllers (i.e. it is a single L2 segment). The <keyword keyref="kw-hos"/> network
./helion/networking/multinetwork.dita:      <p>When the cloud is deployed, <keyword keyref="kw-hos-phrase"/> will create the appropriate
./helion/networking/networking_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Networking Service Overview</title>
./helion/networking/networking_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/networking_overview.dita:    <p><keyword keyref="kw-hos"/> Networking is a virtual networking service that leverages the OpenStack
./helion/networking/networking_overview.dita:      Neutron service to provide network connectivity and addressing to <keyword keyref="kw-hos"/> Compute
./helion/networking/networking_overview.dita:ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</codeblock>
./helion/networking/networking_overview.dita:          <keyword keyref="kw-hos"/> Dashboard</xref> to work with the Networking service.</p>
./helion/networking/networking_overview.dita:        OpenStack Cloud is put together. However, the <keyword keyref="kw-hos"/> takes care of these
./helion/networking/neutron_provider_networks.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Neutron Provider Networks</title>
./helion/networking/neutron_provider_networks.dita:        <li>host_routes which will require a destination and nexthop </li>
./helion/networking/neutron_provider_networks.dita:              <entry>host_routes</entry>
./helion/networking/neutron_provider_networks.dita:              <entry>See host_routes table below</entry>
./helion/networking/neutron_provider_networks.dita:      <title>HOST_ROUTES Details</title>
./helion/networking/neutron_provider_networks.dita:      <p>The following table explains host route settings.</p>
./helion/networking/neutron_provider_networks.dita:          host_routes:
./helion/networking/neutron_troubleshooting.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Neutron</title>
./helion/networking/neutron_troubleshooting.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/neutron_troubleshooting.dita:      This situation is specific to when L3 HA is configured and a network failure occurs to the node hosting the currently active l3 agent. 
./helion/networking/neutron_troubleshooting.dita:      L3 HA is intended to provide HA in situations where the l3-agent crashes or the node hosting an l3-agent crashes/restarts. In the case
./helion/networking/octavia_admin.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Load Balancer: Octavia Driver
./helion/networking/octavia_admin.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/octavia_admin.dita:      <note type="warning">Changes might be lost during <keyword keyref="kw-hos"/> upgrades.</note>
./helion/networking/octavia_admin.dita:      <p>Reasons to enable a load balancing spare pool in <keyword keyref="kw-hos"/>
./helion/networking/octavia_admin.dita:      <note type="important">In <keyword keyref="kw-hos-phrase-30"/> the spare pool canâ€™t be used to
./helion/networking/octavia_admin.dita:        speed up fail overs. If a load balancer fails in <keyword keyref="kw-hos"/>, Octavia will
./helion/networking/octavia_admin.dita:          <keyword keyref="kw-hos-phrase-30"/>.</p>
./helion/networking/octavia_admin.dita:        automatically generate new certificates and deploy them to the controller hosts.</p>
./helion/networking/octavia_admin.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/networking/octavia_admin.dita:ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</codeblock></p>
./helion/networking/rbac.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Role-based Access Control in Neutron</title>
./helion/networking/rbac.dita:| host_routes       |                                            |
./helion/networking/rbac.dita:| dns_assignment        | {"hostname": "host-10-0-1-10", "ip_address": "10.0.1.10", "fqdn": "host-10-0-1-10.openstacklocal."} |
./helion/networking/rbac.dita:| hostId                               |                                                            |
./helion/networking/rbac.dita:| dns_assignment        | {"hostname": "host-10-0-1-11", "ip_address": "10.0.1.11", "fqdn": "host-10-0-1-11.openstacklocal."} |
./helion/networking/using_ipam.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using IPAM Drivers in the Networking
./helion/networking/using_ipam.dita:        <codeblock>cd ~/helion/hos/ansible
./helion/networking/using_ipam.dita:ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/networking/using_ipam.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/networking/using_ipam.dita:ansible-playbook -i hosts/verb_hosts site.yml
./helion/networking/using_ipam.dita:        database tables and Neutron currently can't make those changes. So <i>reconfiguration </i>of
./helion/networking/using_ipam.dita:            grid_master_host = 1.2.3.4
./helion/networking/using_ipam.dita:              <entry>Current Value in HOS</entry>
./helion/networking/using_ipam.dita:              <entry>Current Value in HOS</entry>
./helion/networking/using_ipam.dita:                <p><b>No change needed for HOS</b>.</p>
./helion/networking/using_ipam.dita:              <entry>grid_master_host</entry>
./helion/networking/using_ipam.dita:                grid_master_host</entry>
./helion/networking/using_ipam.dita:          pluggable IPAM driver. Currently, Neutron in HOS by default uses the non-pluggable IPAM
./helion/networking/using_ipam.dita:            keyref="kw-hos-phrase"/> using any pluggable IPAM driver is not supported.</li>
./helion/networking/using_ipam.dita:        <li> Recofiguration from <keyword keyref="kw-hos-phrase"/> using non-pluggable IPAM
./helion/networking/using_ipam.dita:          configuration to <keyword keyref="kw-hos-phrase"/> using any pluggable IPAM driver is not
./helion/networking/vpnaas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using VPN as a Service (VPNaaS)</title>
./helion/networking/vpnaas.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/networking/vpnaas.dita:    <p><b><keyword keyref="kw-hos-phrase"/> VPNaaS Configuration</b></p>
./helion/networking/vpnaas.dita:        keyref="kw-hos-phrase"/> Virtual Private Network (VPN) as a Service module.</p>
./helion/networking/vpnaas.dita:        <li><keyword keyref="kw-hos"/> must be installed. </li>
./helion/networking/vpnaas.dita:          extend access between private networks across two different <keyword keyref="kw-hos"/>
./helion/networking/vpnaas.dita:          clouds or between a <keyword keyref="kw-hos"/> cloud and a non-cloud network. VPNaaS is
./helion/newfeatures30.dita:            <title><keyword keyref="kw-hos-phrase"/> Cloud Scaling</title>
./helion/newfeatures30.dita:            <p><keyword keyref="kw-hos-phrase"/> supports a total of 8000 virtual machines across a total of 200 compute nodes.</p>
./helion/newfeatures30.dita:            <p><keyword keyref="kw-hos-phrase"/> supports 100 Bare Metal Ironic nodes in a single region. </p>
./helion/newfeatures30.dita:                first Swift proxy server. In <keyword keyref="kw-hos-version-30"/> this will now be
./helion/newfeatures30.dita:            <p>In <keyword keyref="kw-hos-version-30"/>, the Entry-scale KVM with Ceph model uses
./helion/newfeatures30.dita:            <p><!-- DOCS-2832 -->In <keyword keyref="kw-hos-version-30"/>, Ceph OSD disks are
./helion/newfeatures30.dita:                    keyref="kw-hos-phrase-30"/> versus installation by a separate installer as in
./helion/newfeatures30.dita:                previous versions of <keyword keyref="kw-hos"/>. It requires configuration before
./helion/newfeatures30.dita:                the <keyword keyref="kw-hos-phrase-30"/> installation, therefore please refer to the
./helion/newfeatures30.dita:            <p>Identity federation enables the ability to configure <keyword keyref="kw-hos"/> to
./helion/newfeatures30.dita:                    keyref="kw-hos"/> clouds. Each cloud can be configured to trust the
./helion/newfeatures30.dita:                API Endpoints</title><p> With <keyword keyref="kw-hos-phrase"/>, data transmission
./helion/newfeatures30.dita:                Cinder block storage with <keyword keyref="kw-hos-phrase"/>. </p></section>
./helion/newfeatures30.dita:                <keyword keyref="kw-hos-phrase"/> is now PCI (Payment Card Industry) ready, enabling
./helion/newfeatures30.dita:            <title>MTU Configurabiity</title><p> In <keyword keyref="kw-hos-phrase"/> you can
./helion/operations/adding_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Increasing Capacity by Adding Nodes</title>
./helion/operations/alarm_resolutions.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Suggested Service Alarm Resolutions</title>
./helion/operations/alarm_resolutions.dita:    <!-- LIST IS CURRENT AS OF HOS 3.0 BUILD 871 -->
./helion/operations/alarm_resolutions.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/alarm_resolutions.dita:                <entry>Host Status</entry>
./helion/operations/alarm_resolutions.dita:                <entry>Alarms when the specified host is down or not reachable.</entry>
./helion/operations/alarm_resolutions.dita:                <entry>The host is down, has been rebooted, or has network connectivity
./helion/operations/alarm_resolutions.dita:                <entry>Stop all the processes and restart the nova-api process on the affected host.
./helion/operations/alarm_resolutions.dita:                  the host specified by the <codeph>hostname</codeph> dimension.</entry>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts glance-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                    <li>Log in to the affected host via SSH.</li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ironic-stop.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ironic-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ironic-stop.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ironic-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  view/confirm this, you can also log into the host specified by the
./helion/operations/alarm_resolutions.dita:                    <codeph>hostname</codeph> dimension, and then run this command: <codeblock>sudo swiftlm-scan | python -mjson.tool</codeblock>
./helion/operations/alarm_resolutions.dita:                  <p>SSH to the affected host and restart the process with this
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                  <p>SSH to the affected host and restart the process with this
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                  <p>SSH to the affected host and restart the process with this
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts _swift-configure.yml --limit &#60;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>A daemon specified by the <codeph>component</codeph> dimension on the host
./helion/operations/alarm_resolutions.dita:                  specified by the <codeph>hostname</codeph> dimension has stopped running.</entry>
./helion/operations/alarm_resolutions.dita:                    <li>Run the Swift start playbook against the affected host:
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &#60;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>Alarms if the swift rings checksums do not match on all hosts.</entry>
./helion/operations/alarm_resolutions.dita:                    <li>Run the Swift start playbook against the affected host:
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>Swift host socket connect</entry>
./helion/operations/alarm_resolutions.dita:                  that the network between the host reporting the problem and the Keystone server or
./helion/operations/alarm_resolutions.dita:                    <li>Run the memcached start playbook against the affected host:
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts memcached-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                    server (use the steps below) and also look for alarms related to the host.
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  <p>Also look for alarms related to the host. An individual disk drive filling can
./helion/operations/alarm_resolutions.dita:                  <p>Restart Swift on that host using the <codeph>--limit</codeph> argument to
./helion/operations/alarm_resolutions.dita:                    target the host:</p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/helion/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/helion/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  see the description of the "Swift host socket connect" alarm for possible
./helion/operations/alarm_resolutions.dita:                  see the description of the "Swift host socket connect" alarm for possible
./helion/operations/alarm_resolutions.dita:                <entry>Cinder backup running &lt;hostname&gt; check</entry>
./helion/operations/alarm_resolutions.dita:                <entry>Cinder volume running &lt;hostname&gt; check</entry>
./helion/operations/alarm_resolutions.dita:                  node.<codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</codeblock></entry>
./helion/operations/alarm_resolutions.dita:                    <note>This alarm only pertains to those using HPE servers with
./helion/operations/alarm_resolutions.dita:                <entry>Log in to the reported host and run these commands to find out the status of
./helion/operations/alarm_resolutions.dita:                    OK.<note>This alarm only pertains to those using HPE servers with
./helion/operations/alarm_resolutions.dita:                    <li>SSH to the affected VSA host.</li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts vsa-start.yml --limit &lt;vsa_hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                    <li>SSH to the affected VSA host.</li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts vsa-start.yml --limit &lt;vsa_hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ceph-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      status:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts neutron-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                    <li>Make note of the failed service names and the affected hosts which you will
./helion/operations/alarm_resolutions.dita:                    <li>Using the affected hostname(s) from the previous output, run the Neutron
./helion/operations/alarm_resolutions.dita:                      start playbook to restart the services:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts neutron-start.yml --limit &lt;hostname></codeblock>
./helion/operations/alarm_resolutions.dita:                      <note>You can pass multiple hostnames with <codeph>--limit</codeph> option by
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts neutron-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                    <li>Once all services are back up, you can SSH to the affected host(s) and
./helion/operations/alarm_resolutions.dita:                    <li>SSH to the affected host(s).</li>
./helion/operations/alarm_resolutions.dita:                    <li>SSH to the affected host(s).</li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-ZMG'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-PMG'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-CEN'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-API'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-MDN'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeph>api_endpoint</codeph> and <codeph>monitored_host_types</codeph>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml --limit 'DES-API,DES-CEN'</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts powerdns-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts bind-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>component=keystone-api<p>monitored_host_type=vip</p><p>This check is
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>Review the logs on the alarming host in the following location for the cause: <codeblock>/var/log/ceilometer/ceilometer-agent-notification-json.log</codeblock>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>Review the logs on the alarming host in the following location for the cause: <codeblock>/var/log/ceilometer/ceilometer-polling-json.log</codeblock>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>The host defined in the <codeph>url</codeph> field is down or not
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                <entry>The host defined in the <codeph>host_type</codeph> and
./helion/operations/alarm_resolutions.dita:                  <p>If this occurs with a specific host with http_status in non-error for
./helion/operations/alarm_resolutions.dita:                    manager from the <codeph>~/scratch/ansible/next/hos/ansible</codeph>
./helion/operations/alarm_resolutions.dita:                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</codeblock>
./helion/operations/alarm_resolutions.dita:                    unassigned shards are on. To find the actual hostname, run:</p>
./helion/operations/alarm_resolutions.dita:                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</codeblock>
./helion/operations/alarm_resolutions.dita:                  <p>Once you find the hostname, you can try the following:</p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/helion/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/helion/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts logging-reconfigure.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  topics or on multiple hosts. Which alarms are firing can help diagnose likely
./helion/operations/alarm_resolutions.dita:                  causes, ie if all on one host it could be the host. If one topic across multiple
./helion/operations/alarm_resolutions.dita:                  hosts it is likely the consumers of that topic, etc.</entry>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags notification</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  hosts to find the root cause.<p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  hosts to find the root cause.<p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  hosts to find the root cause.<p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
./helion/operations/alarm_resolutions.dita:                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
./helion/operations/alarm_resolutions.dita:                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
./helion/operations/alarm_resolutions.dita:      <p>These alarms show under the System section and are setup per <codeph>hostname</codeph>
./helion/operations/alarm_resolutions.dita:              <entry>Log onto the reporting host and diagnose the heavy CPU usage.</entry>
./helion/operations/alarm_resolutions.dita:                host that generated the alarm to try to determine if a service or hardware caused
./helion/operations/alarm_resolutions.dita:              <entry>Host Status</entry>
./helion/operations/alarm_resolutions.dita:              <entry>test_type = ping<p>Alerts when a host is unreachable.</p></entry>
./helion/operations/alarm_resolutions.dita:              <entry>Host or network is down.</entry>
./helion/operations/alarm_resolutions.dita:              <entry>If a single host, attempt to restart the system. If multiple hosts, investigate
./helion/operations/alarm_resolutions.dita:              <entry>Log onto the reporting host to investigate high memory users.</entry>
./helion/operations/alarm_resolutions.dita:              <entry>Take this host out of service until the network can be fixed.</entry>
./helion/operations/alarm_resolutions.dita:              <entry>Log in to the reported host and check if the ntp service is running. <p>If it
./helion/operations/alarm_resolutions.dita:                  <li>Stop the service. <p>On a Linux for HPE Helion host:</p>
./helion/operations/alarm_resolutions.dita:                    <p>On a RHEL host:</p>
./helion/operations/alarm_resolutions.dita:                  <li>Restart the ntp service back up. <p>On a Linux for HPE Helion host:</p>
./helion/operations/alarm_resolutions.dita:                    <p>On a RHEL host:</p>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts barbican-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname></codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags vertica</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</codeblock></li>
./helion/operations/alarm_resolutions.dita:                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/alarm_resolutions.dita:ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</codeblock></li>
./helion/operations/audit_logs_backup.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Backing up and Restoring Audit Logs</title>
./helion/operations/audit_logs_backup.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/audit_logs_backup.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock>This
./helion/operations/audit_logs_backup.dita:          <codeblock>freezer-scheduler job-list -c `hostname`</codeblock>
./helion/operations/audit_logs_backup.dita:          <codeblock>freezer-scheduler job-start -c `hostname` -j &lt;id of the job></codeblock></li>
./helion/operations/audit_logs_backup.dita:      </ol> To restore the backup in another directory, or from another host, <ol id="ol_dkf_lfl_qv">
./helion/operations/audit_logs_backup.dita:hostname = &lt;hostname of the host you want to restore the backup from>          </codeblock>
./helion/operations/audit_logs_backup.dita:hostname = &lt;hostname of the host you want to restore the backup from>
./helion/operations/audit_logs_backup.dita:ssh_host = &lt;your ssh backup host>
./helion/operations/audit_logs_checklist.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Audit Logging Checklist</title>
./helion/operations/audit_logs_enable.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enable Audit Logging</title>
./helion/operations/audit_logs_enable.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ cd ~/helion/hos/ansible</codeblock>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/audit_logs_enable.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible</codeblock>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ source /opt/stack/venv/ansible-hos-3.0.0/bin/activate</codeblock>
./helion/operations/audit_logs_enable.dita:          configured, run: <codeblock>$ ansible -I hosts/verb_hosts KEY-API -a â€˜sudo rm -f /etc/hos/osconfig-ranâ€™</codeblock>
./helion/operations/audit_logs_enable.dita:            configured, you must remove the stub file /etc/hos/osconfig-ran before re-running the
./helion/operations/audit_logs_enable.dita:        <li>To run the playbook that enables auditing for a service, run: <codeblock>$ ansible-playbook -i hosts/verb_hosts osconfig-run.yml â€“limit KEY-API</codeblock>
./helion/operations/audit_logs_enable.dita:            <codeblock>$ ansible-playbook -i hosts/verb_hosts osconfig-run.yml â€“limit KEY-API:MON-API</codeblock></note>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible</codeblock>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ ansible-playbook -i hosts/verb_hosts &lt;service-name&gt;-reconfigure.yml</codeblock>
./helion/operations/audit_logs_enable.dita:          <codeblock>$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock>
./helion/operations/audit_logs_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Audit Logging Overview</title>
./helion/operations/audit_logs_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/blockstorage_index.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Block Storage Operations</title>
./helion/operations/blockstorage/blockstorage_index.dita:    environment.</shortdesc>This section contains operations tasks for your <keyword keyref="kw-hos-phrase-30"/>
./helion/operations/blockstorage/blockstorage_index.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Ceph Monitor Node</title>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:    <p>During your initial installation and deployment, if you defined separate hosts for the Ceph
./helion/operations/blockstorage/ceph/add_monitor_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:              keyref="persisteddata/persistedserverallocations"><keyword keyref="kw-hos-phrase"/>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          monitor node(s) to your Ceph cluster: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit &#60;hostname></codeblock>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          <p>The hostname of the newly added node can be found in the list generated from the output
./helion/operations/blockstorage/ceph/add_monitor_node.dita:          <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:      <p>The newly added Ceph monitor's hostname appears in the <codeph>quorum_names</codeph>
./helion/operations/blockstorage/ceph/add_monitor_node.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/add_monitor_node.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Data Disk to the Object Storage Daemon
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_datadisk.dita:ansible-playbook -i hosts/verb_hosts ceph-deploy.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding an Object Storage Daemon (OSD)
./helion/operations/blockstorage/ceph/add_osd_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/add_osd_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_node.dita:        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
./helion/operations/blockstorage/ceph/add_osd_node.dita:              keyref="persisteddata/persistedserverallocations"><keyword keyref="kw-hos-phrase"/>
./helion/operations/blockstorage/ceph/add_osd_node.dita:          node(s) to your Ceph cluster: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</codeblock>
./helion/operations/blockstorage/ceph/add_osd_node.dita:          <p>The hostname of the newly added node can be found in the list generated from the output
./helion/operations/blockstorage/ceph/add_osd_node.dita:          <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock></li>
./helion/operations/blockstorage/ceph/add_osd_node.dita:        Ceph cluster, the hostname of the newly added OSD should appear in the output.</p>
./helion/operations/blockstorage/ceph/add_osd_node.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/add_osd_node.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
./helion/operations/blockstorage/ceph/ceph_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Overview</title>
./helion/operations/blockstorage/ceph/ceph_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/ceph_overview.dita:            <p>The <keyword keyref="kw-hos-phrase"/> Ceph storage solution provides an unified,
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    <keyword keyref="kw-hos-phrase"/> and Ceph Firefly 0.80.7 running on the hlinux
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    keyref="kw-hos-phrase"/>, the supported version of Ceph is Firefly 0.80.7. In
./helion/operations/blockstorage/ceph/ceph_overview.dita:                upcoming <keyword keyref="kw-hos"/> releases, new versions of Ceph will be supported
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/ceph_overview.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/ceph_overview.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/ceph_overview.dita:                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/ceph_overview.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
./helion/operations/blockstorage/ceph/ceph_overview.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Deploying Ceph Monitor Services on Dedicated
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:        <p>In the <keyword keyref="kw-hos-phrase"/> example configurations, the Ceph monitor service
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <keyword keyref="kw-hos-phrase"/> does not support deployment transition. Once
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    configuration:<codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <codeblock>cd ~/helion/hos/ansible/
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <codeblock>cd ~/helion/hos/ansible/
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <codeblock>cd ~/helion/hos/ansible/
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <codeblock>cd ~/helion/hos/ansible/
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/deploy_monitor_standalone_node.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Reconfiguring Ceph Services</title>
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:                <li>Any additional configuration parameters for Ceph (beyond those listed in the
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:                    directory:<codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:                    services:<codeblock>$cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/reconfigure_ceph_services.dita:$ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Ceph RADOS Gateway Node</title>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:      <li>On (dedicated) cluster node(s) that host Ceph Monitor service. When installed in this
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:        <li>Stop the monitor service running on the specific host by running the following commands: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:ansible-playbook -i hosts/verb_hosts ceph-stop.yml --limit &lt;mon-node-to-remove></codeblock>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <p>The hostname of the node can be found in the list generated from the output of the
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          cluster:<codeblock>ceph mon remove &lt;ceph-mon-host&gt;</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:        <li>Stop the monitor service running on the specific host by running the following commands
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          :<codeblock>service ceph-mon@&lt;ceph-mon-host> stop</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          service:<codeblock>ceph mon remove &lt;ceph-mon-host></codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:        <li>Edit the <codeph>servers.yml</codeph> file and remove the host section from the
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:        restart the monasca-agent on each of those servers with this command:</p>
./helion/operations/blockstorage/ceph/remove_ceph_rados_node.dita:      <codeblock>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&#60;ceph monitor node deleted></codeblock>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Ceph Monitor Node</title>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:        <li>Stop the monitor service running on the specific host by running the following commands: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:ansible-playbook -i hosts/verb_hosts ceph-stop.yml --limit &lt;mon-node-to-remove></codeblock>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <p>The hostname of the node can be found in the list generated from the output of the
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          cluster:<codeblock>ceph mon remove &lt;ceph-mon-host&gt;</codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:        <li>Stop the monitor service running on the specific host by running the following commands
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          :<codeblock>service ceph-mon@&lt;ceph-mon-host> stop</codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          service:<codeblock>ceph mon remove &lt;ceph-mon-host></codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:        <li>Edit the <codeph>servers.yml</codeph> file and remove the host section from the
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:        restart the monasca-agent on each of those servers with this command:</p>
./helion/operations/blockstorage/ceph/remove_monitor_node.dita:      <codeblock>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&#60;ceph monitor node deleted></codeblock>
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Data Disk from a Ceph OSD
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_datadisk.dita:ansible-playbook -i hosts/verb_hosts ceph-status.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Ceph OSD Node</title>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:-2      3               host ceph-ccp-ceph0003-mgmt
./helion/operations/blockstorage/ceph/remove_osd_node.dita:<b>-4      3               host ceph-ccp-ceph0001-mgmt
./helion/operations/blockstorage/ceph/remove_osd_node.dita:-3      3               host ceph-ccp-ceph0002-mgmt
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                    <li>Run the following command to stop the OSDs running on the specific host:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_node.dita:ansible-playbook -i hosts/verb_hosts ceph-stop.yml --limit &lt;osd_node_to_remove></codeblock>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                            hostname of the node that you wish to remove, for example
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                    <li>From the same OSD node, remove the OSD host from the Ceph CRUSH map by
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                            command:<codeblock>ceph osd crush remove &lt;host-name></codeblock><note>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                            Replace the string <codeph>&lt;host-name></codeph> in the above
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                    <li>Log in to the lifecycle manager and remove the OSD host from the input model
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                        <note>If you need to determine the OSD hostname in Cobbler you can use
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/ceph/remove_osd_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                    <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove
./helion/operations/blockstorage/ceph/remove_osd_node.dita:                need to restart the monasca-agent on each of those servers with this command:</p>
./helion/operations/blockstorage/ceph/remove_osd_node.dita:            <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;ceph osd node deleted></codeblock>
./helion/operations/blockstorage/creating_voltype.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating a Volume Type for your Volumes</title>
./helion/operations/blockstorage/singleton_service_cinder.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing the Cinder Singleton Services</title>
./helion/operations/blockstorage/singleton_service_cinder.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/singleton_service_cinder.dita:      <p><keyword keyref="kw-hos"/> cloud must be successfully deployed. </p>
./helion/operations/blockstorage/singleton_service_cinder.dita:        these services are started on the first node in the Cinder volume and backup hostgroup. If
./helion/operations/blockstorage/singleton_service_cinder.dita:        needed, the volume and backup services can be migrated to a different node in the hostgroup.
./helion/operations/blockstorage/singleton_service_cinder.dita:        The following steps describe how to migrate the services to a specified host.<ol
./helion/operations/blockstorage/singleton_service_cinder.dita:          <li>Get the hostname of the node Cinder volume and backup are to be migrated to. <p>For
./helion/operations/blockstorage/singleton_service_cinder.dita:              example: <codeblock>$ hostname
./helion/operations/blockstorage/singleton_service_cinder.dita:            migrate to in the <codeph>migrate_host</codeph>
./helion/operations/blockstorage/singleton_service_cinder.dita:            argument:<codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/blockstorage/singleton_service_cinder.dita:ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --extra-vars "migrate_host=padawan-ccp-c1-m2-mgmt"</codeblock>After
./helion/operations/blockstorage/singleton_service_cinder.dita:                <codeph>migrate_host</codeph> value to
./helion/operations/blockstorage/singleton_service_cinder.dita:            using the <codeph>--limit &lt;hostname></codeph> option. <p>For example: </p><codeblock> cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/blockstorage/singleton_service_cinder.dita: ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit padawan-ccp-c1-m1-mgmt </codeblock>
./helion/operations/blockstorage/singleton_service_cinder.dita:            <p>The hostname of the node can be found in the list generated from the output of the
./helion/operations/blockstorage/singleton_service_cinder.dita:            <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock></li>
./helion/operations/blockstorage/singleton_service_cinder.dita:              volume hostgroup. </p></li>
./helion/operations/blockstorage/singleton_service_cinder.dita:              hostgroup.</p></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a VSA Node</title>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:      add those new hosts to your monitoring checks and we show how to do that as well.</p>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:              keyref="persisteddata/persistedserverallocations"><keyword keyref="kw-hos-phrase"/>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml</codeblock></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          the controller node out. <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit &#60;hostname_vsa_node,hostname_first_controller></codeblock>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <note>If you don't know the <codeph>&#60;hostname></codeph> already, you can get it by
./helion/operations/blockstorage/vsa/add_vsa_node.dita:            looking in the <codeph>~/scratch/ansible/next/hos/ansible/host_vars</codeph>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</codeblock></li>
./helion/operations/blockstorage/vsa/add_vsa_node.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/vsa/add_vsa_node.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring a Separate iSCSI Network to use
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:      hostname-suffix: iscsi
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:            processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:            directory.<codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:          <li>Run the <codeph>site.yml</codeph> playbook using the command below. <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/blockstorage/vsa/configure_vsa_separate_iscsi_network.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
./helion/operations/blockstorage/vsa/create_multiple_vsa_clusters.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating Multiple VSA Clusters</title>
./helion/operations/blockstorage/vsa/create_multiple_vsa_clusters.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/vsa/create_multiple_vsa_clusters.dita:    <p>The <keyword keyref="kw-hos-phrase"/> input model comes with one cluster and three VSA nodes.
./helion/operations/blockstorage/vsa/create_multiple_vsa_clusters.dita:              keyref="kw-hos-phrase"/> input model you will find only a
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a VSA Node For Maintenance</title>
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              <codeph>hosts/verb_hosts</codeph>
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:            file:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:ansible-playbook -i hosts/verb_hosts</codeblock></li>
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              <codeph>padawan-ccp-vsa001</codeph> under maintenance, you must use the host name
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              command:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock><p>In
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              the following example <b>offline-vsa</b> file is created with the VSA host
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              name:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit @offline-vsa</codeblock></p><p>The
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              command:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname></codeblock><p>For
./helion/operations/blockstorage/vsa/remove_vsa_maintenance.dita:              example:</p><codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit @offline-vsa</codeblock><p>When
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a VSA Node</title>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:    <p>Once you have completed the removal of your VSA node(s) you will also want to remove those
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:      hosts from your monitoring checks and we show how to do that as well.</p>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:              <codeblock>cd ~/helion/hos/ansible
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:            <codeph>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</codeph> file.</li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</codeblock></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                <p><image href="../../../../media/hos.docs/operations/remove_vsa_node_30_1.jpg"
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_2.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_3.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_4.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_5.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_6.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:                href="../../../../media/hos.docs/operations/remove_vsa_node_30_7.jpg"/></p></li>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:      <p>Once you have removed the VSA node from your cloud infrastructure, the Host Status alarm
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:        <li>Edit the <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove
./helion/operations/blockstorage/vsa/remove_vsa_node.dita: built_by: HostAlive  
./helion/operations/blockstorage/vsa/remove_vsa_node.dita: host_name: helion-cp1-vsa0001-mgmt
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          to restart the monasca-agent on each of those servers with this command:
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          Monasca API hosts, you can then delete the corresponding alarm to finish this process. To
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;vsa node deleted></codeblock>
./helion/operations/blockstorage/vsa/remove_vsa_node.dita:          <codeblock>monasca alarm-list --metric-dimensions hostname=helion-cp1-vsa0001-mgmt</codeblock></li>
./helion/operations/blockstorage_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Block Storage Overview</title>
./helion/operations/blockstorage_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/blockstorage_overview.dita:      <p><keyword keyref="kw-hos"/> Block Storage volume operations use the OpenStack Cinder service
./helion/operations/blockstorage_overview.dita:      <p><keyword keyref="kw-hos-phrase"/> supports the following storage back ends for block
./helion/operations/blockstorage_overview.dita:      <p><keyword keyref="kw-hos"/> supports setting up multiple block storage backends and multiple
./helion/operations/centralized_logging.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Centralized Logging Service</title>
./helion/operations/central_log_configure_CL.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Centralized Logging</title>
./helion/operations/central_log_configure_CL.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_CL.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_CL.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li> 
./helion/operations/central_log_configure_CL.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_CL.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_CL.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_CL.dita:ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_CL.dita:          <codeblock>curator_backup_repo_name: "es_{{host.my_dimensions.cloud_name}}"
./helion/operations/central_log_configure_CL.dita:$ cd ~/helion/hos/ansible
./helion/operations/central_log_configure_CL.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/central_log_configure_CL.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li> 
./helion/operations/central_log_configure_CL.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_CL.dita:$ ansible-playbook -i hosts/verb_hosts kronos-server-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_manage.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Running the Centralized Logging Service</title>
./helion/operations/central_log_configure_manage.dita:        centralized logging for a file when you want to make sure you are able to monitor those logs
./helion/operations/central_log_configure_manage.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
./helion/operations/central_log_configure_manage.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts logging-stop.yml</codeblock></li>
./helion/operations/central_log_configure_manage.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
./helion/operations/central_log_configure_manage.dita:          <codeblock>ansible-playbook -i ansible-playbook -i hosts/verb_hosts logging-start.yml
./helion/operations/central_log_configure_manage.dita:      <note type="attention">There are consequences if you enable too many logging files for a service. If there isnâ€™t enough storage to support the increased logging, the retention period of logs in elasticsearch is decreased. Alternatively, if you wanted to increase the retention period of log files or if you didnâ€™t want those logs to show up in Fibana, you would disable centralized logging for a file. </note>
./helion/operations/central_log_configure_manage.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_manage.dita:ansible-playbook -i ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</codeblock>
./helion/operations/central_log_configure_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Settings for Other Services</title>
./helion/operations/central_log_configure_services.dita:      <p>When you configure settings for the Centralized Logging Service, those changes impact all
./helion/operations/central_log_configure_services.dita:              <entry>Every message is logged including those that are only for debugging. Provides
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:            <codeblock>~/helion/hos/ansible/roles/CEI-API/templates/api-logging.conf.j2
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/CEI-COL/templates/collector-logging.conf.j2
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/CEI-NAG/templates/agent-notification-logging.conf.j2
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/CEI-CAG/templates/agent-central-logging.conf.j2
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/CEI-EXP/templates/expirer-logging.conf.j2</codeblock></note></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>~/helion/hos/ansible/roles/_CEP-CMN/defaults/main.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>~/helion/hos/ansible/roles/_CND-CMN/defaults/main.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>~/helion/hos/ansible/roles/HUX-SVC/defaults/main.yml</codeblock></li> 
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts hlm-ux-services-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts hux-svc-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>~/helion/hos/ansible/roles/monasca-persister/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/zookeeper/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/storm/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/monasca-notification/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/monasca-api/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/kafka/defaults/main.yml
./helion/operations/central_log_configure_services.dita:~/helion/hos/ansible/roles/monasca-agent/defaults/main.yml (For this file, you will need to add the variable)</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:        <codeblock>~/helion/hos/ansible/roles/VSA-DEP/defaults/main.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts vsa-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/central_log_configure_services.dita:ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</codeblock></li>
./helion/operations/central_log_configure_services.dita:        <li>If your administrator set a hostname value in the <b>external-name</b> field during the
./helion/operations/central_log_configure_services.dita:          hostname.</li>
./helion/operations/central_log_configure_services.dita:        <li>If your administrator did not set a hostname value, then to determine which IP address
./helion/operations/central_log_configure_services.dita:            <codeblock>grep vip-HZN-WEB /etc/hosts</codeblock><p>The output of that command will
./helion/operations/central_log_configure_services.dita:          <codeblock>~/scratch/ansible/next/hos/ansible/group_vars/*</codeblock></li>
./helion/operations/central_log_GS.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Getting Started with Centralized Logging Service</title>
./helion/operations/central_log_troubleshoot.dita:          localhost:9200/_cat/indices?v</codeph> from the control plane to get a full list of
./helion/operations/central_log_troubleshoot.dita:        <li>host - you can specify a specific host to search for in the logs</li>
./helion/operations/central_log_troubleshoot.dita:        <li>Find the alarm definitions that are applied to the various hosts. See the <xref
./helion/operations/central_log_troubleshoot.dita:        <li>Find the alarm definitions applied to the various hosts. These should match the alarm
./helion/operations/central_log_understanding.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding the Centralized Logging Service</title>
./helion/operations/change_service_passwords.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Changing Service Passwords</title>
./helion/operations/change_service_passwords.dita:  <abstract><shortdesc outputclass="hdphidden"><keyword keyref="kw-hos-phrase"/> provides a process
./helion/operations/change_service_passwords.dita:      want to do for security or other purposes.</shortdesc><keyword keyref="kw-hos-phrase"/>
./helion/operations/change_service_passwords.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/change_service_passwords.dita:      <p>In <keyword keyref="kw-hos-phrase"/>, the configuration processor will produce metadata
./helion/operations/change_service_passwords.dita:      <codeblock>cd helion/hos/ansible
./helion/operations/change_service_passwords.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/operations/change_service_passwords.dita:        those passwords which are of interest to you.</p>
./helion/operations/change_service_passwords.dita:      <codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ cd helion/hos/ansible
./helion/operations/change_service_passwords.dita:stack@padawan-ccp-c0-m1-mgmt:~/helion/hos/ansible$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/change_service_passwords.dita:stack@padawan-ccp-c0-m1-mgmt:~/helion/hos/ansible$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/change_service_passwords.dita:      the changes: <codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-keystone-credentials-change.yml</codeblock>
./helion/operations/change_service_passwords.dita:      the changes: <codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-rabbitmq-credentials-change.yml</codeblock>
./helion/operations/change_service_passwords.dita:        <codeph>hlm-reconfigure.yml</codeph>. To make the changes, execute the following commands:<codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
./helion/operations/change_service_passwords.dita:        <codeph>hlm-cluster-credentials-change.yml</codeph>. To make changes, execute the following commands:<codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-cluster-credentials-change.yml</codeblock>
./helion/operations/change_service_passwords.dita:      the following commands:<codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts monasca-reconfigure-credentials-change.yml</codeblock>
./helion/operations/change_service_passwords.dita:      <codeblock>stack@localhost:~/helion/my_cloud/info$ cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/change_service_passwords.dita:stack@localhost:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-other-credentials-change.yml</codeblock>
./helion/operations/change_service_passwords.dita:      following are immutable in <keyword keyref="kw-hos-phrase"/>:<ul>
./helion/operations/cloudadmin_cli.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Cloud Admin Actions with the Command Line</title>
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source keystone.osrc</codeblock>The user called testuser and
./helion/operations/cloudadmin_cli.dita:      them:<codeblock>stack@localhost:~$ openstack role add --user testuser --project test_project glance_admin</codeblock>Next,
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role assignment list --user testuser</codeblock>Your
./helion/operations/cloudadmin_cli.dita:      following:<codeblock>stack@localhost:~$ openstack role list | grep 46ba80078bc64853b051c964db918816 </codeblock>Here
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ cat testuser.osrc
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source testuser.osrc</codeblock>Upload an image and publicize it
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ glance image-create  --name "upload me" --visibility public --container-format bare --disk-format qcow2 -
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source keystone.osrc</codeblock>User testuser and project
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role add --user testuser --project test_project nova_admin list</codeblock>Get
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role assignment list --user testuser</codeblock>Your
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role list | grep 8cdb02bab38347f3b65753099f3ab73c</codeblock>
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source testuser.osrc</codeblock>List all the VMs in project
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ set | grep OS_PROJECT_NAME
./helion/operations/cloudadmin_cli.dita:        stack@localhost:~$ nova list</codeblock>There
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ nova list --all-tenants</codeblock>Here is the
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ nova delete --all-tenants da4f46e2-4432-411b-82f7-71ab546f91f3</codeblock>Next,
./helion/operations/cloudadmin_cli.dita:      list again:<codeblock>stack@localhost:~$ nova list --all-tenants</codeblock>Here you see the
./helion/operations/cloudadmin_cli.dita:      administrator:<codeblock>stack@localhost:~$ source keystone.osrc</codeblock>Next, assign the
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role add --user testuser --project test_project neutron_admin</codeblock>For
./helion/operations/cloudadmin_cli.dita:      ID:<codeblock>stack@localhost:~$ source service.osrc
./helion/operations/cloudadmin_cli.dita:        stack@localhost:~$ neutron net-show 0e560113-f578-4c16-b978-a02ab55a7d6e</codeblock>You
./helion/operations/cloudadmin_cli.dita:      ID:<codeblock>stack@localhost:~$ source keystone.osrc 
./helion/operations/cloudadmin_cli.dita:        stack@localhost:~$ openstack project list | grep dc0f8b0076d4403b83ad517021803d30</codeblock>
./helion/operations/cloudadmin_cli.dita:      net-list:<codeblock>stack@localhost:~$ source testuser.osrc
./helion/operations/cloudadmin_cli.dita:        stack@localhost:~$ neutron net-list</codeblock>Here
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ neutron net-delete mynet</codeblock>Next, run neutron net-list
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ neutron net-list</codeblock>Note that the network called mynet
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source keystone.osrc</codeblock>User testuser and project
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ openstack role add --user testuser --project test_project cinder_admin</codeblock>Switch
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ source testuser.osrc</codeblock>For demonstration purposes,
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ set | grep OS_PROJECT_NAME
./helion/operations/cloudadmin_cli.dita:        stack@localhost:~$ cinder list</codeblock>
./helion/operations/cloudadmin_cli.dita:      admin only argument. <codeblock>stack@localhost:~$ cinder list --all-tenants </codeblock>Here
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ cinder list --tenant dc0f8b0076d4403b83ad517021803d30</codeblock>The
./helion/operations/cloudadmin_cli.dita:      <codeblock>stack@localhost:~$ cinder delete 2c3b4612-4762-4b3c-84b4-4597d6ec8000</codeblock>
./helion/operations/cloudadmin_cli.dita:        <i>test_project</i></p><codeblock>stack@localhost:~$ Source keystone.osrc
./helion/operations/cloudadmin_cli.dita:          stack@localhost:~$ openstack role add --user testuser --project test_project swiftoperator</codeblock>
./helion/operations/cloudadmin_cli.dita:        With <keyword keyref="kw-hos-phrase-20"/> and above, service policy files can be modified and deployed
./helion/operations/cloudadmin_cli.dita:      <codeblock>~/helion/hos/ansible/roles/GLA-API/templates/policy.json.j2
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/ironic-common/files/policy.json
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/KEYMGR-API/templates/policy.json
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/heat-common/files/policy.json
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/CND-API/templates/policy.json
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/nova-common/files/policy.json
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/CEI-API/templates/policy.json.j2
./helion/operations/cloudadmin_cli.dita:        ~/helion/hos/ansible/roles/neutron-common/templates/policy.json.j2 </codeblock>
./helion/operations/cloudadmin_cli.dita:        reconfiguration playbooks, change directories to ~/scratch/ansible/next/hos/ansible and
./helion/operations/cloudadmin_dashboard.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Cloud Admin Actions with the Dashboard</title>
./helion/operations/cloudadmin_dashboard.dita:      <note type="warning">The dashboard in <keyword keyref="kw-hos-phrase"/> is pre-configured to
./helion/operations/cloudadmin_dashboard.dita:      <p>If your administrator set a hostname value for <codeph>external_name</codeph> in your
./helion/operations/cloudadmin_dashboard.dita:        then Horizon will be accessed over port 443 (https) on that hostname.</p>
./helion/operations/cloudadmin_dashboard.dita:      <p>If your administrator did not set a hostname value then in order to determine which IP
./helion/operations/cloudadmin_dashboard.dita:      <codeblock>grep vip-HZN-WEB /etc/hosts</codeblock>
./helion/operations/compute/add_compute_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Linux for HPE Helion Compute
./helion/operations/compute/add_compute_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/compute/add_compute_node.dita:        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;hostname></codeblock>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</codeblock></li>
./helion/operations/compute/add_compute_node.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock>
./helion/operations/compute/add_compute_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/add_compute_node.dita:ansible-playbook -i hosts/verb_hosts nova-cloud-configure.yml</codeblock></li>
./helion/operations/compute/add_compute_node.dita:   hostname: helion-cp1-comp0001-mgmt
./helion/operations/compute/add_compute_node.dita:        commands you can map the hosts shown there to items in the <codeph>servers.yml</codeph> file
./helion/operations/compute/add_compute_node.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/operations/compute/add_compute_node.dita:| ID | Hypervisor hostname      | State | Status  |
./helion/operations/compute/add_rhel_compute.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a RHEL Compute Node</title>
./helion/operations/compute/add_rhel_compute.dita:      additional RHEL compute hosts for more virtual machine capacity or another purpose and these
./helion/operations/compute/add_rhel_compute.dita:      <p>There are two methods you can use to add RHEL compute hosts to your environment:</p>
./helion/operations/compute/add_rhel_compute.dita:        <li>Adding RHEL pre-installed compute hosts. This method does not require the RHEL iso be on
./helion/operations/compute/add_rhel_compute.dita:          compute hosts. This method requires that you provided a RHEL 7.2 iso during the initial
./helion/operations/compute/add_rhel_compute.dita:      <p>You need to ensure your input model files are properly setup for RHEL compute host
./helion/operations/compute/add_rhel_compute.dita:        <p><b>Adding Pre-Installed RHEL Compute Hosts</b></p>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:          <li>Update your deployment directory: <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit &#60;node name></codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:          <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;hostname></codeblock>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</codeblock></li>
./helion/operations/compute/add_rhel_compute.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/add_rhel_compute.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</codeblock>
./helion/operations/compute/configure_compute.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the Compute Service</title>
./helion/operations/compute/creating_aggregates.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Compute Hosts using Aggregates and Scheduler
./helion/operations/compute/creating_aggregates.dita:    <p>OpenStack Nova has the concepts of availability zones and host aggregates that enable you to
./helion/operations/compute/creating_aggregates.dita:      segregate your Compute hosts. Availability zones are used to specify logical separation within
./helion/operations/compute/creating_aggregates.dita:      your cloud based on the physical isolation or redundancy you have setup. Host aggregates are
./helion/operations/compute/creating_aggregates.dita:      used to group compute hosts together based upon common features, such as operation system. For
./helion/operations/compute/creating_aggregates.dita:    <p>This document is going to show you how to set up both a Nova host aggregate and configure the
./helion/operations/compute/creating_aggregates.dita:      filter scheduler to further segregate your compute hosts.</p>
./helion/operations/compute/creating_aggregates.dita:      <p>These steps will show you how to create a Nova aggregate and how to add a compute host to
./helion/operations/compute/creating_aggregates.dita:          <p>So, for example, if you wish to create a new aggregate for your RHEL compute hosts and
./helion/operations/compute/creating_aggregates.dita:| Id | Name | Availability Zone | Hosts | Metadata                 |
./helion/operations/compute/creating_aggregates.dita:        <li>Next, you need to add compute hosts to this aggregate so you can start by listing your
./helion/operations/compute/creating_aggregates.dita:          current hosts. You will want to limit the output of this command to only the hosts running
./helion/operations/compute/creating_aggregates.dita:          this:<codeblock>nova host-list | grep compute</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:        <li>You can then add host(s) to your aggregate with this
./helion/operations/compute/creating_aggregates.dita:          syntax:<codeblock>nova aggregate-add-host &lt;aggregate-name> &lt;host></codeblock></li>
./helion/operations/compute/creating_aggregates.dita:        compute hosts that we'll describe here.</p>
./helion/operations/compute/creating_aggregates.dita:              <entry>Isolates compute hosts based on image properties and aggregate metadata. You
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/compute/creating_aggregates.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/creating_aggregates.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
./helion/operations/compute/enabling_resize.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enabling the Nova Resize and Migrate Features</title>
./helion/operations/compute/enabling_resize.dita:        between Compute hosts with the user having access to the file systems to perform the
./helion/operations/compute/enabling_resize.dita:        deploy a set of public and private SSH keys to the Compute hosts, allowing the
./helion/operations/compute/enabling_resize.dita:          <codeph>nova</codeph> user SSH access between each of your Compute hosts.</p>
./helion/operations/compute/enabling_resize.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/enabling_resize.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=true</codeblock></li>
./helion/operations/compute/enabling_resize.dita:        of public and private SSH keys that were previously added to the Compute hosts, removing the
./helion/operations/compute/enabling_resize.dita:          <codeph>nova</codeph> users SSH access between each of your Compute hosts.</p>
./helion/operations/compute/enabling_resize.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/compute/enabling_resize.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=false</codeblock></li>
./helion/operations/compute/remove_compute_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Compute Node</title>
./helion/operations/compute/remove_compute_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/compute/remove_compute_node.dita:    <section id="disable_provisioning"><title>Disable Provisioning on the Compute Host</title>
./helion/operations/compute/remove_compute_node.dita:          to disable the provisionong on the Compute host you are wanting to remove: <codeblock>nova service-list</codeblock>
./helion/operations/compute/remove_compute_node.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/operations/compute/remove_compute_node.dita:          it is taken out of the scheduling rotation: <codeblock>nova service-disable --reason "&lt;enter reason here>" &lt;node hostname> nova-compute</codeblock>
./helion/operations/compute/remove_compute_node.dita:| Host                     | Binary       | Status   | Disabled Reason       |
./helion/operations/compute/remove_compute_node.dita:    <section id="remove_az"><title>Remove the Compute Host from its Availability Zone</title>
./helion/operations/compute/remove_compute_node.dita:      <p>If you configured the Compute host to be part of an availability zone, these steps will
./helion/operations/compute/remove_compute_node.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
./helion/operations/compute/remove_compute_node.dita:        <li>You can remove the Compute host from the availability zone it was a part of with this
./helion/operations/compute/remove_compute_node.dita:          command: <codeblock>nova aggregate-remove-host &lt;availability zone> &lt;nova hostname></codeblock>
./helion/operations/compute/remove_compute_node.dita:              <codeph>helion-cp1-comp0002-mgmt</codeph> host was in the <codeph>AZ2</codeph>
./helion/operations/compute/remove_compute_node.dita:          <codeblock>$ nova aggregate-remove-host AZ2 helion-cp1-comp0002-mgmt
./helion/operations/compute/remove_compute_node.dita:Host helion-cp1-comp0002-mgmt has been successfully removed from aggregate 4
./helion/operations/compute/remove_compute_node.dita:| Id | Name | Availability Zone | Hosts | Metadata                |
./helion/operations/compute/remove_compute_node.dita:| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
./helion/operations/compute/remove_compute_node.dita:    <section id="live_migration"><title>Use Live Migration to Move Any Instances on this Host to
./helion/operations/compute/remove_compute_node.dita:        Other Hosts</title>
./helion/operations/compute/remove_compute_node.dita:          <li>You will need to verify if the Compute node is currently hosting any instances on it.
./helion/operations/compute/remove_compute_node.dita:            You can do this with the command below: <codeblock>nova list --host=&lt;nova hostname> --all_tenants=1</codeblock>
./helion/operations/compute/remove_compute_node.dita:            <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
./helion/operations/compute/remove_compute_node.dita:            <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
./helion/operations/compute/remove_compute_node.dita:            <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1</codeblock> to
./helion/operations/compute/remove_compute_node.dita:| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
./helion/operations/compute/remove_compute_node.dita:        Host</title>
./helion/operations/compute/remove_compute_node.dita:      <p>To do this step you have a few options. You can SSH to the Compute host and run the
./helion/operations/compute/remove_compute_node.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/remove_compute_node.dita:ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&#60;node name></codeblock>
./helion/operations/compute/remove_compute_node.dita:    <section id="delete_node"><title>Delete the Compute Host from Nova</title>
./helion/operations/compute/remove_compute_node.dita:      <p>Here is an example highlighting the Compute host we're going to remove:</p>
./helion/operations/compute/remove_compute_node.dita:| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
./helion/operations/compute/remove_compute_node.dita:      <p>Delete the host from Nova using the command below:</p>
./helion/operations/compute/remove_compute_node.dita:      <p>Use the command below to confirm that the Compute host has been completely removed from
./helion/operations/compute/remove_compute_node.dita:    <section id="deletefromneutron"><title>Delete the Compute Host from Neutron</title>
./helion/operations/compute/remove_compute_node.dita:| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
./helion/operations/compute/remove_compute_node.dita:    <section id="remove_node"><title>Remove the Compute Host from the servers.yml File and Run the
./helion/operations/compute/remove_compute_node.dita:            <codeph>max-count</codeph> if you used those to ensure they reflect the proper number of
./helion/operations/compute/remove_compute_node.dita:        <li>Run the configuration processor: <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/remove_compute_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/operations/compute/remove_compute_node.dita:          <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</codeblock></li>
./helion/operations/compute/remove_compute_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/remove_compute_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/compute/remove_compute_node.dita:    <section id="remove_cobbler"><title>Remove the Compute Host from Cobbler</title>
./helion/operations/compute/remove_compute_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/compute/remove_compute_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/compute/remove_compute_node.dita:    <section id="remove_monitoring"><title>Remove the Compute Host from Monitoring</title>
./helion/operations/compute/remove_compute_node.dita:          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
./helion/operations/compute/remove_compute_node.dita:  built_by: HostAlive
./helion/operations/compute/remove_compute_node.dita:  host_name: helion-cp1-comp0001-mgmt
./helion/operations/compute/remove_compute_node.dita:        restart the monasca-agent on each of those servers with this command:</p>
./helion/operations/compute/remove_compute_node.dita:      <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;compute node deleted></codeblock>
./helion/operations/compute/remove_compute_node.dita:      <codeblock>monasca alarm-list --metric-dimensions hostname=helion-cp1-comp0001-mgmt</codeblock>
./helion/operations/configure_firewall.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the <keyword keyref="kw-hos"/>
./helion/operations/configure_firewall.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configure_firewall.dita:        <keyword keyref="kw-hos"/> firewall that is configured in front of the control services. This
./helion/operations/configure_firewall.dita:        that enables the ability for <keyword keyref="kw-hos"/> tenants to create north-south,
./helion/operations/configure_firewall.dita:        <li>Commit those changes to your local git:
./helion/operations/configure_firewall.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/configure_firewall.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/configure_firewall.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configure_firewall.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/configure_firewall.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configure_firewall.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configure_firewall.dita:ansible-playbook -i hosts/verb_hosts osconfig-iptables-deploy.yml</codeblock></li>
./helion/operations/configuring/change_horizon_timeout.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Changing the Dashboard Timeout Value</title>
./helion/operations/configuring/change_horizon_timeout.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/change_horizon_timeout.dita:        recommended default and best practice for those concerned with security.</p>
./helion/operations/configuring/change_horizon_timeout.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/change_horizon_timeout.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/change_horizon_timeout.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/change_horizon_timeout.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/change_horizon_timeout.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/change_horizon_timeout.dita:ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_dashboard.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Dashboard Service Administration
./helion/operations/configuring/configure_dashboard.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/configure_dashboard.dita:    <p>Horizon is the OpenStack service that serves as the basis for the <keyword keyref="kw-hos"/>
./helion/operations/configuring/configure_dashboard.dita:    <p>The dashboards provide a web-based user interface to <keyword keyref="kw-hos"/> services including
./helion/operations/configuring/configure_dashboard.dita:      <title>Dashboard Service and TLS in <keyword keyref="kw-hos-phrase-30"/></title>
./helion/operations/configuring/configure_horizon_v3.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Horizon for Keystone v3</title>
./helion/operations/configuring/configure_horizon_v3.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/configure_horizon_v3.dita:      <li>The Keystone v3 endpoint is in the format: http://&lt;host>:&lt;port>/v3 and should be the
./helion/operations/configuring/configure_horizon_v3.dita:            <codeblock>OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST</codeblock></li>
./helion/operations/configuring/configure_horizon_v3.dita:        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_horizon_v3.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_horizon_v3.dita:        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_horizon_v3.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_horizon_v3.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_horizon_v3.dita:ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the Logging Level for
./helion/operations/configuring/configure_logging_level.dita:                        following file: <codeblock>~/helion/hos/ansible/roles/_CEI-CMN/defaults/main.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        following file: <codeblock>~/helion/hos/ansible/roles/_CEI-CMN/defaults/main.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                          below:</note><codeblock>~/helion/hos/ansible/roles/CEI-API/templates/api-logging.conf.j2
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/CEI-COL/templates/collector-logging.conf.j2
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/CEI-NAG/templates/agent-notification-logging.conf.j2
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/CEI-CAG/templates/agent-central-logging.conf.j2
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/CEI-EXP/templates/expirer-logging.conf.j2</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>~/helion/hos/ansible/roles/_CEP-CMN/defaults/main.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>~/helion/hos/ansible/roles/_CND-CMN/defaults/main.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>~/helion/hos/ansible/roles/monasca-persister/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/zookeeper/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/storm/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/monasca-notification/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/monasca-api/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/kafka/defaults/main.yml
./helion/operations/configuring/configure_logging_level.dita:~/helion/hos/ansible/roles/monasca-agent/defaults/main.yml (For this file, you will need to add the variable)</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-reconfigure.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>~/helion/hos/ansible/roles/ops-common/defaults/main.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts sherpa-reconfigure.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>~/helion/hos/ansible/roles/VSA-DEP/defaults/main.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/helion/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/configure_logging_level.dita:                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/configure_logging_level.dita:ansible-playbook -i hosts/verb_hosts vsa-reconfigure.yml</codeblock>
./helion/operations/configuring/identity/configure_identity.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Default Settings in the Identity Service</title>
./helion/operations/configuring/identity/configure_identity.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/configure_identity.dita:          keyref="kw-hos"/>, and as such it is installed automatically by the lifecycle manager
./helion/operations/configuring/identity/identity_benchmark.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Keystone Identity Performance
./helion/operations/configuring/identity/identity_benchmark.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/identity_benchmark.dita:      <image href="../../../../media/keystone/HOSKeystoneDomain-SpecificBackend.png" id="image_eht_kcm_25"/>
./helion/operations/configuring/identity/identity_benchmark.dita:      <image placement="break" href="../../../../media/keystone/HOS2_LdapUserLookup_bars.png"
./helion/operations/configuring/identity/identity_benchmark.dita:      <image href="../../../../media/keystone/HOS2_LdapUserLookup_lines.png" id="image_ggk_15b_r5"/>
./helion/operations/configuring/identity/identity_benchmark.dita:      <image placement="break" href="../../../../media/keystone/HOS2_CPST_KPU_bars.png"
./helion/operations/configuring/identity/identity_benchmark.dita:        href="../../../../media/keystone/HOS2_CPST_KPU_lines.png" id="image_wk4_wqb_r5"/>
./helion/operations/configuring/identity/identity_ldap.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Integrating LDAP with the Identity
./helion/operations/configuring/identity/identity_ldap.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/identity_ldap.dita:            keyref="kw-hos-phrase-30"/>.</li>
./helion/operations/configuring/identity/identity_ldap.dita:$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          the previous step as a command-line option: <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/home/stack/helion/my_cloud/config/keystone/keystone_configure_ldap_my.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          <codeblock>$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          playbooks:<codeblock>$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          <codeblock>$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/configuring/identity/identity_ldap.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_ldap.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/home/stack/helion/my_cloud/config/keystone/keystone_configure_ldap_certs_all.yml </codeblock></li>
./helion/operations/configuring/identity/identity_ldap.dita:      <keyword keyref="kw-hos-phrase-30"/> domain-specific configuration: <ul>
./helion/operations/configuring/identity/identity_ldap.dita:            all users and listing all groups are not supported operations. Those calls require a
./helion/operations/configuring/identity/identity_ldap.dita:      <keyword keyref="kw-hos-phrase-30"/> API-based domain-specific configuration management <ul>
./helion/operations/configuring/identity/identity_ldap.dita:              <keyword keyref="kw-hos-phrase-30"/>. </li>
./helion/operations/configuring/identity/identity_limitations.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Identity Service Notes and
./helion/operations/configuring/identity/identity_limitations.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/identity_limitations.dita:        <li><keyword keyref="kw-hos"/> MFA support is a custom configuration requiring HPE
./helion/operations/configuring/identity/identity_limitations.dita:        <li>MFA drivers are not included with <keyword keyref="kw-hos"/> and need to be provided by
./helion/operations/configuring/identity/identity_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>The Identity Service</title>
./helion/operations/configuring/identity/identity_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/identity_overview.dita:      <p>The <keyword keyref="kw-hos"/> Identity service, based on the OpenStack Keystone API, is
./helion/operations/configuring/identity/identity_overview.dita:          Keystone, it has not yet been certified by the <keyword keyref="kw-hos"/> engineering team
./helion/operations/configuring/identity/identity_overview.dita:          and is an experimental feature in <keyword keyref="kw-hos"/>.</note></p>
./helion/operations/configuring/identity/identity_overview.dita:        Kilo release but not yet validated for <keyword keyref="kw-hos"/>.--></p>
./helion/operations/configuring/identity/identity_overview.dita:        has not yet been certified or documented for <keyword keyref="kw-hos"/>.</p></section>
./helion/operations/configuring/identity/identity_reconfigure.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Reconfiguring the Identity Service</title>
./helion/operations/configuring/identity/identity_reconfigure.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/identity_reconfigure.dita:        configuration options for the Identity Service. </p><keyword keyref="kw-hos"/> allows you to
./helion/operations/configuring/identity/identity_reconfigure.dita:       <note><p><keyword keyref="kw-hos-phrase"/> has the token expiration setting as below, which is
./helion/operations/configuring/identity/identity_reconfigure.dita:$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/identity/identity_reconfigure.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock>
./helion/operations/configuring/identity/identity_reconfigure.dita:$ cd ~/helion/hos/ansible
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/configuring/identity/identity_reconfigure.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring/identity/identity_reconfigure.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</codeblock></li>
./helion/operations/configuring/identity/identity_reconfigure.dita:      <p><keyword keyref="kw-hos-phrase"/> supports UUID tokens by default. Fernet Tokens are available as an
./helion/operations/configuring/identity/identity_reconfigure.dita:      <note>Tempest doesn't work with Fernet Tokens in <keyword keyref="kw-hos-phrase"/>. If Fernet tokens are
./helion/operations/configuring/identity/identity_reconfigure.dita:        host='192.168.245.3', port=5672, credentials=credentials))
./helion/operations/configuring/identity/keystone_federation.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Keystone to Keystone Federation</title>
./helion/operations/configuring/identity/keystone_federation.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/keystone_federation.dita:          keyref="kw-hos-phrase-30"/> via configuration parameters in specific Ansible files.
./helion/operations/configuring/identity/keystone_federation.dita:	shib_sso_idp_entity_id: &lt;protocol>://&lt;idp_host>:&lt;port>/v3/OS-FEDERATION/saml2/idp
./helion/operations/configuring/identity/keystone_federation.dita:                    &lt;protocol>://&lt;idp_host>:&lt;port>/v3/OS-FEDERATION/saml2/idp. </entry>
./helion/operations/configuring/identity/keystone_federation.dita:         "https://idp_host:5000/v3/OS-FEDERATION/saml2/idp"
./helion/operations/configuring/identity/keystone_federation.dita:        <li>Next, go to <codeph>/home/stack/scratch/ansible/next/hos/ansible</codeph> and run the
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</codeblock></li>
./helion/operations/configuring/identity/keystone_federation.dita:            sp_url: https://sp_host:5000
./helion/operations/configuring/identity/keystone_federation.dita:            auth_url: https://sp_host:5000/v3
./helion/operations/configuring/identity/keystone_federation.dita:              the server's hostname.
./helion/operations/configuring/identity/keystone_federation.dita:        <li>Go to <codeph>/home/stack/scratch/ansible/next/hos/ansible</codeph> and run the
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</codeblock></li>
./helion/operations/configuring/identity/keystone_federation.dita:          self.auth_url = "http://idp_host:5000/v3/"
./helion/operations/configuring/identity/keystone_federation.dita:        <li>On the identity provider side: <codeblock>hostname=myidp.com
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>hostname=myidp.com
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>hostname=myidp.com
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>hostname=myidp.com, username=user5, role_name=_member_</codeblock>
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>hostname=myidp.com, username=user1, user_domain_name=Default</codeblock>
./helion/operations/configuring/identity/keystone_federation.dita:          <codeblock>hostname=myidp.com, username=user1, user_domain_name=Default</codeblock>
./helion/operations/configuring/identity/keystone_federation.dita:        <li>On the identity provider side: <codeblock>hostname=myidp.com, username=user1</codeblock>
./helion/operations/configuring/identity/keystone_federation.dita:            <li>On the IdP side, set <codeph>hostname=myidp.com</codeph> and
./helion/operations/configuring/identity/websso.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Web Single Sign On</title>
./helion/operations/configuring/identity/websso.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/configuring/identity/websso.dita:        dashboard in <keyword keyref="kw-hos-phrase-30"/> . If a user has not yet authenticated with
./helion/operations/configuring/identity/websso.dita:          with those vendors for specific instructions for those products. </li><li>Only WebSSO federation via the SAML method is supported in <keyword keyref="kw-hos-phrase-30"/>
./helion/operations/configuring/identity/websso.dita:      <keyword keyref="kw-hos-phrase-30"/> now provides WebSSO support for the Horizon web
./helion/operations/configuring/identity/websso.dita:          https://&lt;adfs_server_hostname>/FederationMetadata/2007-06/FederationMetadata.xml. </li>
./helion/operations/configuring/identity/websso.dita:        <li>Go to /home/stack/scratch/ansible/next/hos/ansible and run the following to enable
./helion/operations/configuring/identity/websso.dita:          <codeblock>ansible-playbook  -i hosts/verb_hosts keystone-reconfigure.yml  -e@/tmp/adfs_config.yml</codeblock>
./helion/operations/configuring/identity/websso.dita:          <codeblock>ansible-playbook  -i hosts/verb_hosts horizon-reconfigure.yml</codeblock>
./helion/operations/configuring_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Your Cloud Services</title>
./helion/operations/configuring_tls.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Transport Layer Security
./helion/operations/configuring_tls.dita:        <keyword keyref="kw-hos-phrase-30"/> and additional configuration options are available to
./helion/operations/configuring_tls.dita:      <keyword keyref="kw-hos-phrase-30"/> and additional configuration options are available to
./helion/operations/configuring_tls.dita:      should be .crt or it will not be processed by <keyword keyref="kw-hos"/>.</section>
./helion/operations/configuring_tls.dita:        hostname-suffix: mgmt
./helion/operations/configuring_tls.dita:        hostname: true
./helion/operations/configuring_tls.dita:      <p><keyword keyref="kw-hos"/> generates its own internal certificates but is designed to allow
./helion/operations/configuring_tls.dita:            keyref="kw-hos"/>. </li>
./helion/operations/configuring_tls.dita:      <codeblock>cd ~/helion/hos/ansible/
./helion/operations/configuring_tls.dita:ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
./helion/operations/configuring_tls.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/configuring_tls.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/configuring_tls.dita:ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml </codeblock>Note:
./helion/operations/configuring_tls.dita:      <codeblock>ansible-playbook -i hosts/verb_hosts _tls-endpoint-reconfigure.yml</codeblock>
./helion/operations/create_extnet.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating an External Network</title>
./helion/operations/create_extnet.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/create_extnet.dita:        provide two of them here. The <keyword keyref="kw-hos"/> installer provides an Ansible
./helion/operations/create_extnet.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/create_extnet.dita:ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&#60;CIDR></codeblock>
./helion/operations/create_hdp_servicenet.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating the HPE Helion Development Platform
./helion/operations/create_hdp_servicenet.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/create_hdp_servicenet.dita:        VLAN-encapsulated provider network which is fully routable to the <keyword keyref="kw-hos"/>
./helion/operations/create_hdp_servicenet.dita:  hostname-suffix: mgmt
./helion/operations/create_hdp_servicenet.dita:  hostname: true
./helion/operations/create_hdp_servicenet.dita:              <codeblock>cd ~/helion/hos/ansible
./helion/operations/create_hdp_servicenet.dita:              processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/create_hdp_servicenet.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/create_hdp_servicenet.dita:              directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/create_hdp_servicenet.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/create_hdp_servicenet.dita:            <li>Run the <codeph>site.yml</codeph> playbook below: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/create_hdp_servicenet.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
./helion/operations/create_hdp_servicenet.dita:                <codeblock>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</codeblock>
./helion/operations/create_hdp_servicenet.dita:              subnet definition using the <codeph>--host-route</codeph> option of the
./helion/operations/create_hdp_servicenet.dita:                <codeph>neutron subnet-create</codeph> command. The <codeph>--host-route</codeph>
./helion/operations/create_hdp_servicenet.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/create_hdp_servicenet.dita:ansible-playbook -i hosts/verb_hosts neutron-svc-net-configure.yml \
./helion/operations/create_hdp_servicenet.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/create_hdp_servicenet.dita:ansible-playbook -i hosts/verb_hosts neutron-svc-net-configure.yml -e PROV_NET_TYPE=vlan -e PROV_PHYS_NET=physnet1 -e PROV_NET_VLAN_ID=306 -e SVC_NET_CIDR=10.1.6.0/24 -e SVC_NET_DEST=192.168.245.0/24 -e SVC_NET_GATEWAY=10.1.6.1 -e SVC_ALLOCATION_START=10.1.6.0 -e SVC_ALLOCATION_END=10.1.6.255</codeblock></li>
./helion/operations/create_hdp_servicenet.dita:              <codeblock>neutron subnet-update --dns-nameserver &lt;nameserver IP or hostname> &lt;subnet ID></codeblock>
./helion/operations/create_hdp_servicenet.dita:              <codeblock>neutron subnet-update --dns-nameserver &lt; first nameserver IP or hostname> --dns-nameserver &lt;second nameserver IP or hostname> &lt;subnet ID></codeblock></li>
./helion/operations/dns/designate_initialconfig.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Designate Initial Configuration</title>
./helion/operations/dns/designate_initialconfig.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/dns/designate_initialconfig.dita:      <p>After the <keyword keyref="kw-hos"/>  installation has been completed, Designate 
./helion/operations/dns/designate_initialconfig.dita:      <p>Depending on the Backend chosen, the method used to identify the nameserver's public IP will differ.</p>
./helion/operations/dns/designate_initialconfig.dita:          <li>On the <keyword keyref="kw-hos"/> lifecycle manager, locate and open 
./helion/operations/dns/designate_initialconfig.dita:        where these may be registered, either within a zone hosted on Designate itself, or 
./helion/operations/dns/designate_initialconfig.dita:        within a zone hosted on a external DNS service.</p>
./helion/operations/dns/designate_initialconfig.dita:          <codeblock>$ openstack zone create --email hostmaster@example.com example.com.
./helion/operations/dns/designate_initialconfig.dita:| email          | hostmaster@example.com               |
./helion/operations/dns/designate_initialconfig.dita:| 2d6cfef1-efd3-44ac-b7d9-7149a7bb655b | example.com.     | SOA  | ns1.example.com. hostmaster.example.com 1457529552 3600 600 86400 3600 |
./helion/operations/dns/designate_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>DNS Service Overview</title>
./helion/operations/dns/designate_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/dns/designate_overview.dita:      <p><keyword keyref="kw-hos"/> DNS Service provides multi-tenant Domain Name Service with REST API management 
./helion/operations/image/configure_glance.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the Image Service</title>
./helion/operations/image/configure_glance.dita:        <p>In <keyword keyref="kw-hos-phrase"/>, by default, the Glance image caching option is not
./helion/operations/image/configure_glance.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/image/configure_glance.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/image/configure_glance.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/operations/image/configure_glance.dita:          <li>The server hosting the Glance service must have network access to the remote location
./helion/operations/image/configure_glance.dita:            that is hosting the image.</li>
./helion/operations/image/configure_glance.dita:                <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:                <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/image/configure_glance.dita:                <codeblock>cd ~/helion/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/image/configure_glance.dita:                <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/image/configure_glance.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/operations/live_migration.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Live Migration of Instances</title>
./helion/operations/live_migration.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/live_migration.dita:        <li>The <codeph>nova host-evacuate-live</codeph> command cannot be used for a Compute host
./helion/operations/live_migration.dita:          work if the host has instances that are comprised of one or the other.
./helion/operations/live_migration.dita:        <li>If you are using both Linux for HPE Helion (KVM) and RHEL compute hosts, you cannot live
./helion/operations/live_migration.dita:          migrate instances between them. Instances on KVM hosts can only be live migrated to other
./helion/operations/live_migration.dita:          KVM hosts and the same for RHEL hosts. </li>
./helion/operations/live_migration.dita:        <li>Block live-migration between RHEL compute hosts is not supposed due to Red Hat not
./helion/operations/live_migration.dita:          migration that are supported between RHEL hosts are ones where block storage volumes are
./helion/operations/live_migration.dita:        stopped instances data from one Compute host to another. It does this using passwordless SSH
./helion/operations/live_migration.dita:        running processes and associated resources to the destination Compute host using the
./helion/operations/live_migration.dita:        wish to live migrate all of the instances off of a single Compute host:</p>
./helion/operations/live_migration.dita:                href="../../media/hos.docs/livemigration1.png"/></p></li>
./helion/operations/live_migration.dita:          <li>In the Live Migrate wizard you will see the Compute host the instance currently
./helion/operations/live_migration.dita:            resides on and then a drop down menu that allows you to choose the Compute host you want
./helion/operations/live_migration.dita:            to migrate the instance to. Select a destination host from that menu. You also have two
./helion/operations/live_migration.dita:              destination host has the available disk space to host the instance.</p>
./helion/operations/live_migration.dita:            <p><image href="../../media/hos.docs/livemigration2.png"/></p></li>
./helion/operations/live_migration.dita:                           &lt;server> [&lt;host>]
./helion/operations/live_migration.dita:   &lt;host>              destination host name.
./helion/operations/live_migration.dita:          <li>Identify the instances on the compute node you wish to migrate: <codeblock>nova list --all-tenants --host &lt;hostname&gt;</codeblock>
./helion/operations/live_migration.dita:            <p>Example showing a host with a single Compute instance on it:</p>
./helion/operations/live_migration.dita:            <codeblock>stack@helion-cp1-c1-m1-mgmt:~$ nova list --host helion-cp1-comp0001-mgmt --all-tenants
./helion/operations/live_migration.dita:          <li>When using live migration you can either specify a target host that the instance will
./helion/operations/live_migration.dita:            for you. If you want to get a list of available hosts you can use this command:
./helion/operations/live_migration.dita:            <codeblock>nova host-list</codeblock></li>
./helion/operations/live_migration.dita:            <codeblock>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/operations/live_migration.dita:            <codeblock>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/operations/live_migration.dita:            <note>The [&lt;target compute host&gt;] option is optional. If you do not specify a
./helion/operations/live_migration.dita:              target host then the nova scheduler will choose a node for you.</note>
./helion/operations/live_migration.dita:        <p>If you want to live migrate all of the instances off a single Compute host you can
./helion/operations/live_migration.dita:          utilize the <codeph>nova host-evacuate-live</codeph> command.</p>
./helion/operations/live_migration.dita:            mix of instances on the Compute host in that some have at least one ephemeral disk and
./helion/operations/live_migration.dita:            need to live migrate the instances off the host individually using the instructions in
./helion/operations/live_migration.dita:          <li>Using admin credentials, determine the hostname of the Compute host you wish to live
./helion/operations/live_migration.dita:            migrate. <p>You can determing the hostname by getting a list of instances using this
./helion/operations/live_migration.dita:            <codeblock>nova list --all-tenants --fields=name,status,task_state,host,flavor</codeblock></li>
./helion/operations/live_migration.dita:          <li>You will want to ensure that all of the instances on the host are in either
./helion/operations/live_migration.dita:            <codeblock>nova list --all-tenants --host &lt;hostname></codeblock></li>
./helion/operations/live_migration.dita:          <li>Issue the host-evacuate-live command, which will begin the live migration process.
./helion/operations/live_migration.dita:              <p>If all of the instances on the host are using at least one local (ephemeral) disk,
./helion/operations/live_migration.dita:            <codeblock>nova host-evacuate-live --block-migrate &lt;hostname></codeblock>
./helion/operations/live_migration.dita:            <codeblock>nova host-evacuate-live &lt;hostname></codeblock>
./helion/operations/live_migration.dita:            <note>You can either let the nova-scheduler choose a suitable target host or you can
./helion/operations/live_migration.dita:              specify one using the <codeph>--target-host &lt;hostname> </codeph> switch. See
./helion/operations/live_migration.dita:                <codeph>nova help host-evacuate-live</codeph> for details.</note></li>
./helion/operations/live_migration.dita:            <codeblock>nova list --all-tenants --fields=name,status,task_state,host,flavor</codeblock></li>
./helion/operations/managing/managing_quotas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Project Quotas</title>
./helion/operations/managing_notificationmethods.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Notification Methods for Alarms</title>
./helion/operations/managing_notificationmethods.dita:              <codeph>~/helion/hos/ansible/roles/monasca-notification/defaults/main.yml</codeph>
./helion/operations/managing_notificationmethods.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/managing_notificationmethods.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/managing_notificationmethods.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/managing_notificationmethods.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/managing_notificationmethods.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/managing_notificationmethods.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/managing_notificationmethods.dita:ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</codeblock></li>
./helion/operations/managing_notificationmethods.dita:              href="../../media/hos.docs/opsconsole_createnotificationmethod1.png"/></p></li>
./helion/operations/managing_notificationmethods.dita:              href="../../media/hos.docs/opsconsole_createnotificationmethod2.png"/></p></li>
./helion/operations/managing_notificationmethods.dita:            <li>Address/Key - Enter the value corresponding to the type you chose.</li>
./helion/operations/managing_notificationmethods.dita:          <p><image href="../../media/hos.docs/opsconsole_createnotificationmethod3.png"/></p></li>
./helion/operations/managing_notificationmethods.dita:        <li>Once those values are entered, select the <b>Create Notification</b> button.</li>
./helion/operations/managing_notificationmethods.dita:              href="../../media/hos.docs/opsconsole_createnotificationmethod4.png"/></p></li>
./helion/operations/managing_notificationmethods.dita:              href="../../media/hos.docs/opsconsole_createnotificationmethod5.png"/></p></li>
./helion/operations/managing_notificationmethods.dita:              href="../../media/hos.docs/opsconsole_createnotificationmethod6.png"/></p></li>
./helion/operations/managing_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Your Cloud Services</title>
./helion/operations/monitoring/configure_monitoring.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the Monitoring Service</title>
./helion/operations/monitoring/configure_monitoring.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/monitoring/configure_monitoring.dita:      <p>In <keyword keyref="kw-hos-phrase"/> you have the option to specify a SMTP server for email
./helion/operations/monitoring/configure_monitoring.dita:          keyref="kw-hos"/>, but must be specified in the configuration file described below. The
./helion/operations/monitoring/configure_monitoring.dita:                <entry>The server entry must be uncommented and set to a valid hostname or IP
./helion/operations/monitoring/configure_monitoring.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/monitoring/configure_monitoring.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/monitoring/configure_monitoring.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/monitoring/configure_monitoring.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/monitoring/configure_monitoring.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/monitoring/configure_monitoring.dita:        <li>Run the Monasca reconfigure playbook to deploy the changes: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/monitoring/configure_monitoring.dita:ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</codeblock>
./helion/operations/monitoring/configure_monitoring.dita:        <!--(please see eula )-->. Vertica is the recommended database for <keyword keyref="kw-hos"
./helion/operations/monitoring/configure_monitoring.dita:              <codeph>influxdb</codeph> instead of <codeph>vertica</codeph>: <codeblock>~/helion/hos/ansible/roles/monasca-variables/defaults/main.yml</codeblock>
./helion/operations/monitoring_service.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Getting Started with Monitoring</title> 
./helion/operations/monitoring_service.dita:      <keyword keyref="kw-hos"/> Monitoring service to monitor the health of your cloud and, if needed, to troubleshoot issues. This service leverages Monasca, a multi-tenant,
./helion/operations/monitoring_service.dita:      <keyword keyref="kw-hos"/> Monitoring service leverages Monasca, which is a multi-tenant,
./helion/operations/monitoring_service.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/monitoring_service.dita:      <p>The monitoring service is automatically installed as part of the <keyword keyref="kw-hos"/>
./helion/operations/monitoring_service.dita:    <section id="differences"><title>Differences Between Upstream and <keyword keyref="kw-hos"/>
./helion/operations/monitoring_service.dita:      <p>In the <keyword keyref="kw-hos-phrase"/> release, the OpenStack monitoring service,
./helion/operations/monitoring_service.dita:      <note>Icinga was supported in previous <keyword keyref="kw-hos"/> versions but it has been
./helion/operations/monitoring_service.dita:        deprecated in <keyword keyref="kw-hos-phrase"/>.</note>
./helion/operations/monitoring_service.dita:          <li>VM Metrics: CPU utilization, disk I/O, network I/O, and memory usage of hosted virtual
./helion/operations/monitoring_service.dita:      <p>For a full list of packaged plugins that are included <keyword keyref="kw-hos"/>, see <xref
./helion/operations/monitoring_service.dita:      <p><keyword keyref="kw-hos"/> comes with some predefined monitoring alarms for the services
./helion/operations/networking/add_network_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Neutron Network Node</title>
./helion/operations/networking/add_network_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/networking/add_network_node.dita:        networking nodes then you need to ensure that those roles are defined. You can look in the
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/networking/add_network_node.dita:        <li>Then you can image the node: <codeblock>cd ~/helion/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;hostname></codeblock>
./helion/operations/networking/add_network_node.dita:          <note>If you don't know the <codeph>&lt;hostname></codeph> already, you can get it by
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &#60;hostname></codeblock></li>
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock></li>
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit &#60;hostname></codeblock></li>
./helion/operations/networking/add_network_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</codeblock></li>
./helion/operations/networking/add_network_node.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/networking/add_network_node.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</codeblock>
./helion/operations/objectstorage/add_new_storage_policy.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a New Swift Storage Policy</title>
./helion/operations/objectstorage/add_new_storage_policy.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/add_new_storage_policy.dita:          processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_new_storage_policy.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/add_new_storage_policy.dita:          directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_new_storage_policy.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/add_new_storage_policy.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_new_storage_policy.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock>If
./helion/operations/objectstorage/add_new_storage_policy.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_new_storage_policy.dita:ansible-playbook -i hosts/verb_hosts swift-status.yml
./helion/operations/objectstorage/add_new_storage_policy.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
./helion/operations/objectstorage/add_swift_object_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Swift Object Node</title>
./helion/operations/objectstorage/add_swift_object_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/add_swift_object_node.dita:              HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph>. For instructions, see
./helion/operations/objectstorage/add_swift_object_node.dita:          processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/add_swift_object_node.dita:          directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/add_swift_object_node.dita:            argument):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-idd></codeblock><p>In
./helion/operations/objectstorage/add_swift_object_node.dita:            3):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj4</codeblock></p><note>You
./helion/operations/objectstorage/add_swift_object_node.dita:        <li>Configure the operating system: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock>
./helion/operations/objectstorage/add_swift_object_node.dita:          <p>The hostname of the newly added server can be found in the list generated from the
./helion/operations/objectstorage/add_swift_object_node.dita:          <codeblock>grep hostname ~/helion/my_cloud/info/server_info.yml</codeblock>
./helion/operations/objectstorage/add_swift_object_node.dita:          <p>For example, for <b>swobj4</b>, the hostname is <b>helion-cp1-swobj0004-mgmt</b>.
./helion/operations/objectstorage/add_swift_object_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit helion-cp1-swobj0004-mgmt</codeblock></p></li>
./helion/operations/objectstorage/add_swift_object_node.dita:          node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</codeblock>If
./helion/operations/objectstorage/add_swift_object_node.dita:        <li>Run the following playbook to ensure that all other server's host file are updated with
./helion/operations/objectstorage/add_swift_object_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</codeblock></li>
./helion/operations/objectstorage/add_swift_object_node.dita:          adding:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_object_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml </codeblock></li>
./helion/operations/objectstorage/add_swift_object_node.dita:            example:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml</codeblock></p></li>
./helion/operations/objectstorage/add_swift_pac_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Swift Proxy, Account, Container (PAC)
./helion/operations/objectstorage/add_swift_pac_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/add_swift_pac_node.dita:                HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph>. For instructions, see
./helion/operations/objectstorage/add_swift_pac_node.dita:            processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/add_swift_pac_node.dita:            directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/add_swift_pac_node.dita:              argument):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id></codeblock><p>In
./helion/operations/objectstorage/add_swift_pac_node.dita:              3):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</codeblock></p><note>You
./helion/operations/objectstorage/add_swift_pac_node.dita:              <codeph>data/control_plane.yml</codeph> files to get the host prefix (for example,
./helion/operations/objectstorage/add_swift_pac_node.dita:            helion) and the control plane name (for example, cp1). This gives you the hostname of
./helion/operations/objectstorage/add_swift_pac_node.dita:              system:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</codeblock><p>For
./helion/operations/objectstorage/add_swift_pac_node.dita:              example, for <b>swpac6</b>, the hostname is
./helion/operations/objectstorage/add_swift_pac_node.dita:              <b>helion-cp1-c2-m3-mgmt</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit helion-cp1-c2-m3-mgmt</codeblock></p></li>
./helion/operations/objectstorage/add_swift_pac_node.dita:            node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock>If
./helion/operations/objectstorage/add_swift_pac_node.dita:          <li>Run the following playbook to ensure that all other server's host file are updated
./helion/operations/objectstorage/add_swift_pac_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</codeblock></li>
./helion/operations/objectstorage/add_swift_pac_node.dita:            adding:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/add_swift_pac_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml </codeblock></li>
./helion/operations/objectstorage/changing_swift_zone.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Changing Swift Zone Layout</title>
./helion/operations/objectstorage/changing_swift_zone.dita:          originally hosted on the removed servers.</li>
./helion/operations/objectstorage/changing_swift_zone.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/changing_swift_zone.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/changing_swift_zone.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/changing_swift_zone.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/changing_swift_zone.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/changing_swift_zone.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/changing_swift_zone.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Applying Input Model Changes to Existing
./helion/operations/objectstorage/input_model_change_existing_rings.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          output:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml </codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          reconfiguration:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:            report:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</codeblock><p>The
./helion/operations/objectstorage/input_model_change_existing_rings.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml | tee /tmp/rebalance.log</codeblock><note>The
./helion/operations/objectstorage/input_model_change_existing_rings.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/input_model_change_existing_rings.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/objectstorage/input_model_change_existing_rings.dita:                <p>In <keyword keyref="kw-hos-phrase"/>, when you remove servers from the input
./helion/operations/objectstorage/lm_scan_metering.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Gathering Swift Monitoring Metrics</title>
./helion/operations/objectstorage/lm_scan_metering.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/lm_scan_metering.dita:      "hostname": "padawan-ccp-c1-m2-mgmt", 
./helion/operations/objectstorage/lm_scan_metering.dita:              <li><b>hostname</b>: This is the name of the node the metric relates to. This is not
./helion/operations/objectstorage/objectstorage_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Object Storage Overview</title>
./helion/operations/objectstorage/objectstorage_overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/objectstorage_overview.dita:      <p>The <keyword keyref="kw-hos"/> Object Storage service leverages <tm tmtype="reg"
./helion/operations/objectstorage/objectstorage_overview.dita:      <p><keyword keyref="kw-hos"/> Object Storage provides a highly-available, resilient, and
./helion/operations/objectstorage/objectstorage_overview.dita:        single point of failure. In addition, this version of <keyword keyref="kw-hos"/> introduces
./helion/operations/objectstorage/objectstorage_overview.dita:        exists in the example cloud models distributed in <keyword keyref="kw-hos"/>:</p><p>
./helion/operations/objectstorage/operating_swift.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Object Storage Overview</title>
./helion/operations/objectstorage/operating_swift.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/rebalanced_explained.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Rebalancing Swift Rings</title>
./helion/operations/objectstorage/rebalanced_explained.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/rebalanced_explained.dita:        <li>Not all devices are of the same size. <keyword keyref="kw-hos-phrase"/> automatically
./helion/operations/objectstorage/removing_swift_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Swift Node</title>
./helion/operations/objectstorage/removing_swift_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:              playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <b>&lt;hostname></b></codeblock><p>The
./helion/operations/objectstorage/removing_swift_node.dita:              <b>helion-cp1-swobj0004</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <b>helion-cp1-swobj0004</b></codeblock></p><note>When
./helion/operations/objectstorage/removing_swift_node.dita:              using the <codeph>--limit</codeph> argument, you must specify the full hostname (for
./helion/operations/objectstorage/removing_swift_node.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:          <li>Run the configuration processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/operations/objectstorage/removing_swift_node.dita:            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:            directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:            before proceeding further: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</codeblock>
./helion/operations/objectstorage/removing_swift_node.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/objectstorage/removing_swift_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml</codeblock>
./helion/operations/objectstorage/removing_swift_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/removing_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock>
./helion/operations/objectstorage/removing_swift_node.dita:          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
./helion/operations/objectstorage/removing_swift_node.dita:        restart the monasca-agent on each of those servers with this command:</p>
./helion/operations/objectstorage/removing_swift_node.dita:      <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;swift node deleted></codeblock>
./helion/operations/objectstorage/ring_management.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Swift Rings</title>
./helion/operations/objectstorage/ring_management.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/ring_management.dita:      provided during setup of the <keyword keyref="kw-hos"/> Input Model. (For more information,
./helion/operations/objectstorage/safe_rebalance_deploy_ring.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Determining When to Rebalance and Deploy a New
./helion/operations/objectstorage/safe_rebalance_deploy_ring.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/safe_rebalance_deploy_ring.dita:--> Starting reconnaissance on 3 hosts
./helion/operations/objectstorage/swift_cli.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using the Swift Command-line Client
./helion/operations/objectstorage/swift_cli.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/swift_container_sync.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring your Swift System to Allow Container Sync</title>
./helion/operations/objectstorage/swift_container_sync.dita:          keyref="kw-hos-version-30"/> system. If you are synchronizing with another Swift system
./helion/operations/objectstorage/swift_container_sync.dita:          <keyword keyref="kw-hos-version-30"/> terminology.</p>
./helion/operations/objectstorage/swift_container_sync.dita:            this command: <codeblock>curl -k &lt;destination IP or hostname>:8080/healthcheck</codeblock>
./helion/operations/objectstorage/swift_container_sync.dita:# cluster_name1 = https://host1/v1/
./helion/operations/objectstorage/swift_container_sync.dita:# cluster_name2 = https://host2/v1/</codeblock></li>
./helion/operations/objectstorage/swift_container_sync.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_container_sync.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/swift_container_sync.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_container_sync.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/swift_container_sync.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/objectstorage/swift_container_sync.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/objectstorage/swift_dispersion_report.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Running the Swift Dispersion Report</title>
./helion/operations/objectstorage/swift_dispersion_report.dita:      <p>Once a Swift system has been fully deployed in <keyword keyref="kw-hos-phrase"/>, you can
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeph>~/helion/hos/ansible/roles/swift-dispersion/templates/dispersion.conf.j2</codeph>.
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_dispersion_report.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_dispersion_report.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/objectstorage/swift_dispersion_report.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/objectstorage/swift_dispersion_report.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/swift_dispersion_report.dita:ansible-playbook -i hosts/verb_hosts swift-dispersion-populate.yml</codeblock></li>
./helion/operations/objectstorage/swift_dispersion_report.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/swift_dispersion_report.dita:ansible-playbook -i hosts/verb_hosts swift-dispersion-report.yml</codeblock>
./helion/operations/objectstorage/swift_min_part_hours.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Changing min-part-hours in Swift</title>
./helion/operations/objectstorage/swift_min_part_hours.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/swift_min_part_hours.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_min_part_hours.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_min_part_hours.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/swift_min_part_hours.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_min_part_hours.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/swift_min_part_hours.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/objectstorage/swift_min_part_hours.dita:ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
./helion/operations/objectstorage/swift_ring_mgmt.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Managing Rings Using Swift Playbooks</title>
./helion/operations/objectstorage/swift_ring_mgmt.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/swift_ring_mgmt.dita:          <codeph>~/scratch/ansible/next/hos/ansible</codeph> directory.</p>
./helion/operations/objectstorage/swift_ring_mgmt.dita:                  <codeph>site.yml</codeph> playbooks, so if you run either of those playbooks, the
./helion/operations/objectstorage/swift_ring_mgmt.dita:                  changed and copies those rings to all Swift nodes.</p>
./helion/operations/objectstorage/swift_ring_mgmt.dita:                <codeblock>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml --extra-vars "limit-ring=object-1"</codeblock>
./helion/operations/objectstorage/swift_ring_mgmt.dita:                variable:<codeblock>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</codeblock></entry>
./helion/operations/objectstorage/swift_weight_attribute.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using the Weight-Step Attributes to Prepare for
./helion/operations/objectstorage/swift_weight_attribute.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/swift_weight_attribute.dita:        keyref="kw-hos-phrase"/> does this by limiting the weights of the new drives to a smaller
./helion/operations/objectstorage/swift_weight_attribute.dita:        keyref="kw-hos-phrase"/> will increase the weight and rebuild rings to trigger another round
./helion/operations/objectstorage/swift_weight_attribute.dita:    <p>In <keyword keyref="kw-hos"/>, the weight-step attribute is set in the ring specification of
./helion/operations/objectstorage/swift_weight_attribute.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_weight_attribute.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/objectstorage/swift_weight_attribute.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/objectstorage/swift_weight_attribute.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/objectstorage/validating_swift_recon.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Gathering Swift Data</title>
./helion/operations/objectstorage/validating_swift_recon.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage/validating_swift_recon.dita:--> Starting reconnaissance on 3 hosts
./helion/operations/objectstorage/validating_swift_recon.dita:--> Starting reconnaissance on 3 hosts
./helion/operations/objectstorage/validating_swift_recon.dita:--> Starting reconnaissance on 3 hosts
./helion/operations/objectstorage/validating_swift_recon.dita:3/3 hosts matched, 0 error[s] while checking hosts.
./helion/operations/objectstorage/validating_swift_recon.dita:3/3 hosts matched, 0 error[s] while checking hosts.
./helion/operations/objectstorage_index.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Object Storage Operations</title>
./helion/operations/objectstorage_index.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/objectstorage_index.dita:    <p>This section contains operations tasks for your <keyword keyref="kw-hos-phrase"/> Object Storage service.</p>
./helion/operations/operations_best_practices.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Getting Started with Operations</title>
./helion/operations/operations_best_practices.dita:      operating a <keyword keyref="kw-hos-phrase"/> cloud.</shortdesc></abstract>
./helion/operations/operations_best_practices.dita:          <keyword keyref="kw-hos-phrase"/> cloud.</p>
./helion/operations/operations_best_practices.dita:        resource to hosted virtual machines. This information can be viewed in the Operations
./helion/operations/operations_best_practices.dita:        hosted resource utilization by using quotas for projects. You can track this usage over time
./helion/operations/operations_best_practices.dita:        <li>Health of the hosting compute node and virtualization layer</li>
./helion/operations/operations_best_practices.dita:        <li>Determine the hosting node of the virtual machine that is having issues.</li>
./helion/operations/operations_best_practices.dita:        <li>On the compute hosts page, view the status and resource utilization of the compute node
./helion/operations/operations_index.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Operations Guide</title>
./helion/operations/operations_index.dita:      <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/operations_index.dita:          keyref="kw-hos-phrase"/> cloud.  The audience is the admin-level operator of the
./helion/operations/opsconsole/alarm_definitions.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Alarm Creation</title>
./helion/operations/opsconsole/alarm_definitions.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/alarm_definitions.dita:                        <li><uicontrol>Dimension(s)</uicontrol>: identifies which aspect (Hostname,
./helion/operations/opsconsole/alarm_definitions.dita:        <!-- removed     <section>       <title>Alarm Expression Editor</title>       <p>The Alarm Expression Editor is particularly useful if you already know the string syntax of         the expression(s) you want to use, or can paste it from another expression and adjust values         in the syntax as needed.</p>       <p>If you prefer to use an Expression Editor instead, perform the following steps:</p>       <ol>         <li>Click the <uicontrol>Convert to Alarm Expression Editor</uicontrol> button.<p>The right             half of the form turns into a free-form text field. </p></li>         <li>Type or paste syntax for the expression you want to use. Adjust values as           necessary.</li>         <li>Once in the Editor mode, and you need to create a new expression, click the             <uicontrol>Alarm Expression Editor</uicontrol> button to open the Alarm Expressions           form. The form on this screen allows you to set up one or multiple expressions at once.             <note>For more information about working with multiple expressions, refer to the next             section.</note></li>         <li>Use the form to apply rules to your alarm as described in the Create Alarm Definitions           section.</li>         <li>When done applying rules to create an expression for your alarm, click <uicontrol>Create             Alarm</uicontrol>.</li>       </ol>       <title>Work with Multiple Expressions</title>       <p>You may create, edit, or delete multiple expressions using the Alarm Expression Editor         form.</p>       <p>To create multiple expressions:</p>       <ol>         <li>From the Create Alarm Definition window, click the <uicontrol>Convert to Alarm             Expression Editor</uicontrol> button if not already converted. </li>         <li>Click the <uicontrol>Alarm Expression Editor</uicontrol> button to open a separate           window.</li>         <li>Use the form to apply rules to your alarm as described in the Create Alarm Definitions           section.</li>         <li>Click <uicontrol>Add New Expression</uicontrol> to add another expression to your alarm.             <p>A new blank form is inserted for creating a new expression.</p></li>         <li>Repeat for each new expression you want to add.</li>         <li>If there are multiple expressions, the following options are available: <ul>             <li>To edit an expression, locate the one you want to edit, change its values and then               click the <uicontrol>Update Expression</uicontrol> button.</li>             <li>To remove any expression you no longer need, locate the expression you want to               delete, and click the <uicontrol>Delete Expression</uicontrol> button.</li>             <li>To create expressions to be included as part of a group, click the <uicontrol>Add                 New Expression Group</uicontrol> button, then refer to the previous section to               create multiple expressions for each group.</li>             <li>To add more expression groups, click the <uicontrol>Add New Expression                 Group</uicontrol> button to insert a new Expression Group form.</li>             <li>To remove any expression group you no longer need, locate the expression group you               want to delete, and click the <uicontrol>Delete Expression Group</uicontrol>               button.</li>             <li>Specify the type of trigger you want for each expression group by selecting                 <uicontrol>Any Expression in Group</uicontrol> to let the alarm trigger if the               criteria applied to any of the expressions in the group is met; or select                 <uicontrol>All Expressions in the Group</uicontrol> to only trigger the alarm if all               the critera of all the expressions in the group are met.</li>           </ul></li>         <li>Multiple group expressions must have triggering conditions, which is presented in a           table format at the bottom of the form in the Expression Group Combination Triggering           Conditions area: <ol>             <li>You may set <uicontrol>Any</uicontrol>, <uicontrol>All</uicontrol> or                 <uicontrol>None</uicontrol> of the groups to trigger by selecting the conditions and               their associated expession groups.</li>             <li>When done specifying the triggers and their conditions, click the <uicontrol>Set                 Conditions</uicontrol> button.</li>             <li>To see the syntax of the newly created expressions, click the <uicontrol>View                 Expressions Syntax</uicontrol> button. <p>Viewing the syntax opens a window showing                 the entire expression(s) created, like in the following               example:</p><codeblock>avg(cpu.system_perc{hostname=hostname.domain.com}) > 90 or avg(disk_read_ops{hostname=hostname.domain.com, device=vda}, 120) > 1000</codeblock></li>           </ol></li>         <li>When done with the Expressions Editor, click <uicontrol>Complete Editing</uicontrol> to           close the Alarm Expression Editor form.</li>         <li>When done creating the alarm, click <uicontrol>Create Alarm</uicontrol>. </li>       </ol>     </section>     -->
./helion/operations/opsconsole/alarm_explorer.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Alarm Explorer</title>
./helion/operations/opsconsole/alarm_explorer.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/alarm_explorer.dita:                <li><uicontrol>Sort by Dimension</uicontrol> displays the alarms by device, hostname
./helion/operations/opsconsole/alarm_resolutions.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Alarm Resolutions</title>
./helion/operations/opsconsole/alarm_resolutions.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/block_storage_alarm.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/cap_storage.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/code_enghost.dita:<concept id="code_enghost">
./helion/operations/opsconsole/code_enghost.dita:    <title>Operations Console: Code Engine Hosts</title>
./helion/operations/opsconsole/code_enghost.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/compute_hosts.dita:<concept id="compute_hosts">
./helion/operations/opsconsole/compute_hosts.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Compute Hosts</title>
./helion/operations/opsconsole/compute_hosts.dita:    <shortdesc>This page allows you to view your Compute Host resources.</shortdesc>
./helion/operations/opsconsole/compute_hosts.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/compute_instances.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Compute Instances</title>
./helion/operations/opsconsole/compute_instances.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/compute_instances.dita:                <li><uicontrol>Sort by Host</uicontrol> displays the compute instances by their
./helion/operations/opsconsole/compute_instances.dita:                    host.</li>
./helion/operations/opsconsole/compute_summary.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Compute - Alarm Summary</title>
./helion/operations/opsconsole/compute_summary.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/compute_summary.dita:                <li><uicontrol>Sort by Dimension</uicontrol> displays the alarms by device, hostname
./helion/operations/opsconsole/dashboard.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Dashboard</title>
./helion/operations/opsconsole/dashboard.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/hdp_summary.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>HDP - Alarm Summary</title>
./helion/operations/opsconsole/hdp_summary.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/hdp_summary.dita:                <li><uicontrol>Sort by Dimension</uicontrol> displays the alarms by device, hostname
./helion/operations/opsconsole/invent_storage.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/logging.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Logging</title>
./helion/operations/opsconsole/logging.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/metric_monitoring.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Time Series Graphs</title>
./helion/operations/opsconsole/metric_monitoring.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/metric_monitoring.dita:                                <li>Host</li>
./helion/operations/opsconsole/my_dashboard_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/networking_summary.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Networking - Alarm Summary</title>
./helion/operations/opsconsole/networking_summary.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/networking_summary.dita:                <li><uicontrol>Sort by Dimension</uicontrol> displays the alarms by device, hostname
./helion/operations/opsconsole/notification_methods.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Notification Methods</title>
./helion/operations/opsconsole/notification_methods.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/notification_methods.dita:                    based on the type you chose.</li>
./helion/operations/opsconsole/perf_storage.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/storage_summary.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Object Storage - Alarm Summary</title>
./helion/operations/opsconsole/storage_summary.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/opsconsole/storage_summary.dita:                <li><uicontrol>Sort by Dimension</uicontrol> displays the alarms by device, hostname
./helion/operations/opsconsole_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using the Operations Console</title>
./helion/operations/opsconsole_overview.dita:      <p>You can use the Operations Console (Ops Console) for <keyword keyref="kw-hos-phrase"/> to
./helion/operations/opsconsole_overview.dita:          keyref="kw-hos"/> administrators can manage data in the following ways:</p>
./helion/operations/opsconsole_overview.dita:        <li>Manage compute nodes and easily use a form to create a new host</li>
./helion/operations/opsconsole_overview.dita:          metrics, services, and hosts that match the triggers unique to an environment</li>
./helion/operations/opsconsole_overview.dita:        <li><xref type="section" href="#opsconsole/find_nameIP">Optionally use a Hostname OR virtual
./helion/operations/opsconsole_overview.dita:      <title>Optionally use a Hostname OR virtual IP address to access Ops Console</title>
./helion/operations/opsconsole_overview.dita:      <p>To find your hostname OR IP address:</p>
./helion/operations/opsconsole_overview.dita:        <li>If your administrator set a hostname value in the external-name field, you will use that
./helion/operations/opsconsole_overview.dita:          hostname when logging in to OpsConsole. or example, in a browser you would type:
./helion/operations/opsconsole_overview.dita:        <li>If your administrator did not set a hostname value, then to determine the IP address to
./helion/operations/opsconsole_overview.dita:          use, from your lifecycle manager, run: <codeblock>Grep vip -HZN-WEB /etc/hosts</codeblock>
./helion/operations/orchestration/configure_orchestration.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring the Orchestration Service</title>
./helion/operations/reboot_cloud_down.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Bringing Down Your Cloud: Services Down
./helion/operations/reboot_cloud_down.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/reboot_cloud_down.dita:        <li>Gracefully shut down your <keyword keyref="kw-hos-phrase"/> cloud by running the
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml</codeblock></li>
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts bm-power-status.yml</codeblock></li>
./helion/operations/reboot_cloud_down.dita:          up:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts bm-power-up.yml </codeblock></li>
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock></li>
./helion/operations/reboot_cloud_down.dita:        <li>Gracefully bring up your <keyword keyref="kw-hos-phrase"/> cloud by running the
./helion/operations/reboot_cloud_down.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml</codeblock></li>
./helion/operations/reboot_cloud_down.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml</codeblock></li>
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml</codeblock>
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</codeblock>
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</codeblock>
./helion/operations/reboot_cloud_down.dita:              <codeph>~/scratch/ansible/next/hos/ansible</codeph> directory in the format of
./helion/operations/reboot_cloud_down.dita:        <li>Continue checking the status of your <keyword keyref="kw-hos-phrase"/> cloud services
./helion/operations/reboot_cloud_down.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_down.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml</codeblock></li>
./helion/operations/reboot_cloud_rolling.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Bringing Down Your Cloud: Rolling Reboot
./helion/operations/reboot_cloud_rolling.dita:        <p>Execute the following command on each controller node to determine which node is hosting
./helion/operations/reboot_cloud_rolling.dita:          migrate to in the <codeph>migrate_host</codeph> argument:</p>
./helion/operations/reboot_cloud_rolling.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --extra-vars "migrate_host=padawan-ccp-c1-m2-mgmt"</codeblock>
./helion/operations/reboot_cloud_rolling.dita:          node, that is, the host with <codeph>consoleauth_host_index=0</codeph>. To move it to
./helion/operations/reboot_cloud_rolling.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</codeblock></p>
./helion/operations/reboot_cloud_rolling.dita:        <codeblock>for i in $(grep -w cluster-prefix ~/helion/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</codeblock>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;controller node></codeblock></li>
./helion/operations/reboot_cloud_rolling.dita:            <p>Note that the current node being rebooted could be hosting the lifecycle manager.</p>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;controller node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;controller node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            credentials.<codeblock>nova list --host &lt;hostname&gt; --all-tenants</codeblock></li>
./helion/operations/reboot_cloud_rolling.dita:              HOS 3.0 that will be based on the Mitaka release, the only live-migration cases that
./helion/operations/reboot_cloud_rolling.dita:            command:</p><codeblock>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/operations/reboot_cloud_rolling.dita:            option:<codeblock>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/operations/reboot_cloud_rolling.dita:            Note: The [&lt;target compute host&gt;] option is optional. If you do not specify a
./helion/operations/reboot_cloud_rolling.dita:            target host then the nova scheduler will choose a node for you.<p>OR</p><p>Stop the
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;compute node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            '@&lt;filename>' to process all hosts listed in the file.
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;compute node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:        Swift host <ol>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;swift node></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;swift node> </codeblock>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;vsa_node name></codeblock></li>
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname></codeblock>
./helion/operations/reboot_cloud_rolling.dita:            observing that all the OSDs under the current host are reported as 'up')
./helion/operations/reboot_cloud_rolling.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname></codeblock>
./helion/operations/reboot_cloud_rolling.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/reboot_cloud_rolling.dita:        <codeblock>stack@helion-control-plane-cluster1-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ls *status*
./helion/operations/recovering_controller_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering the Control Plane</title>
./helion/operations/recovering_controller_nodes.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/recovering_controller_nodes.dita:            jobs<codeblock>freezer-scheduler -c &lt;hostname&gt; job-list</codeblock>Get the id
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock>This
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock>
./helion/operations/recovering_controller_nodes.dita:            percona-bootstrap.yml:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:            cluster:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:ssh_host = {{ freezer_ssh_host }}
./helion/operations/recovering_controller_nodes.dita:hostname = {{ hostname of the first MySQL node }}</codeblock>
./helion/operations/recovering_controller_nodes.dita:            instances:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts percona-start.yml</codeblock>Mysql
./helion/operations/recovering_controller_nodes.dita:            percona-status.yml:<codeblock>ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:            command:<codeblock>ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock>
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-list</codeblock>Get the id
./helion/operations/recovering_controller_nodes.dita:            job:<codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock>This
./helion/operations/recovering_controller_nodes.dita:            so:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts swift-stop.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:            swift-reconfigure.yml:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-list</codeblock>Get
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-start -j &lt;job-id&gt;</codeblock>This
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit helion-ccp-c1-m2-mgmt</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit helion-ccp-c1-m1-mgmt,helion-ccp-c1-m2-mgmt,helion-ccp-c1-m3-mgmt -e '{ "freezer_backup_jobs_upload": false }'</codeblock>You
./helion/operations/recovering_controller_nodes.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:          <li>On the lifecycle manager, install the freezer-agent, as follows:<codeblock>cd ~/helion/hos/ansible/
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock>
./helion/operations/recovering_controller_nodes.dita:            <p>You must then retrieve the /etc/hosts file from any compute or controller node and
./helion/operations/recovering_controller_nodes.dita:            so:</p><codeblock>127.0.0.1       localhost
./helion/operations/recovering_controller_nodes.dita:::1             localhost ip6-localhost ip6-loopback
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;hostname> job-list</codeblock>Get the id of the job
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;hostname> job-stop -j &lt;job-id&gt;</codeblock>If
./helion/operations/recovering_controller_nodes.dita:            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock>This
./helion/operations/recovering_controller_nodes.dita:            state:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:cd ~/helion/hos/ansible/
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock>Perform
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:cd ~/helion/hos/ansible/
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock>Run
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_backup_jobs_upload": false }'</codeblock>You
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
./helion/operations/recovering_controller_nodes.dita:ssh_host = &lt;freezer_ssh_host&gt;
./helion/operations/recovering_controller_nodes.dita:hostname = &lt;hostname of the old first Swift-Proxy (SWF-PRX[0])&gt;
./helion/operations/recovering_controller_nodes.dita:              <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock><p>If
./helion/operations/recovering_controller_nodes.dita:              run<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts guard-deployment.yml
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</codeblock>
./helion/operations/recovering_controller_nodes.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recovering_controller_nodes.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml</codeblock></li>
./helion/operations/recovering_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Nodes</title>
./helion/operations/recover_compute_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Compute Nodes</title>
./helion/operations/recover_compute_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/recover_compute_node.dita:          <li>You will use the hostname of the compute node you are repairing in these steps. To
./helion/operations/recover_compute_node.dita:            retrieve this you can use this command: <codeblock>nova host-list | grep compute</codeblock>
./helion/operations/recover_compute_node.dita:            <codeblock>$ nova host-list | grep compute
./helion/operations/recover_compute_node.dita:| Host                     | Binary       | Status   | Disabled Reason |
./helion/operations/recover_compute_node.dita:            <codeblock>nova list --host &lt;hostname> --all-tenants</codeblock>
./helion/operations/recover_compute_node.dita:            <codeblock>$ nova list --host helion-cp1-comp0002-mgmt --all-tenants
./helion/operations/recover_compute_node.dita:            <codeblock>nova list --host &lt;ihostname> --all-tenants</codeblock></li>
./helion/operations/recover_compute_node.dita:            <codeblock>$ nova list --host helion-cp1-comp0002-mgmt --all-tenants
./helion/operations/recover_compute_node.dita:| Host                     | Binary       | Status  |
./helion/operations/recover_compute_node.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/recover_compute_node.dita:ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=cpn-0004</codeblock></li>
./helion/operations/recover_compute_node.dita:            wildcard arguments and also '@&lt;filename>' to process all hosts listed in a file:
./helion/operations/recover_compute_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recover_compute_node.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit hlm004-ccp-comp0004-mgmt</codeblock></li>
./helion/operations/recover_compute_node.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/recover_compute_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0004</codeblock></li>
./helion/operations/recover_compute_node.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/recover_compute_node.dita:ansible-playbook -i hosts/verb_hosts site.yml --limit hlm004-ccp-comp0004-mgmt</codeblock></li>
./helion/operations/removing_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing Nodes</title>
./helion/operations/replace_controller.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Controller Node</title>
./helion/operations/replace_controller.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/replace_controller.dita:    <p>For <keyword keyref="kw-hos-phrase"/> versions, you must have three controller nodes.
./helion/operations/replace_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/operations/replace_controller.dita:          <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/operations/replace_controller.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock><note>After
./helion/operations/replace_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</codeblock></li>
./helion/operations/replace_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/replace_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/replace_controller.dita:        <li>Remove the SSH key of the old controller node from the known hosts file. You will
./helion/operations/replace_controller.dita:          specify the <codeph>ip-addr</codeph> value: <codeblock>ssh-keygen -f "/home/stack/.ssh/known_hosts" -R &lt;ip_addr></codeblock>
./helion/operations/replace_controller.dita:          <codeblock>stack@helion-cp1-c1-m1-mgmt:~/helion/hos/ansible$ ssh-keygen -f "/home/stack/.ssh/known_hosts" -R 10.13.111.135
./helion/operations/replace_controller.dita:# Host 10.13.111.135 found: line 6 type ECDSA
./helion/operations/replace_controller.dita:/home/stack/.ssh/known_hosts updated.
./helion/operations/replace_controller.dita:Original contents retained as /home/stack/.ssh/known_hosts.old</codeblock></li>
./helion/operations/replace_controller.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/replace_controller.dita:          the node in Cobbler in the command: <codeblock>cd ~/helion/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name></codeblock>
./helion/operations/replace_controller.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replace_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</codeblock></li>
./helion/operations/replace_controller.dita:          <codeblock>ssh-keygen -f "/home/dbadmin/.ssh/known_hosts" -R {{ip_of_node_to_replace}}</codeblock></li>
./helion/operations/replace_controller.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
./helion/operations/replace_controller.dita:            switch for that node, otherwise you need to specify the hostname of your Swift ringer
./helion/operations/replace_controller.dita:            builder server and the hostname of the node being replaced.</p>
./helion/operations/replace_controller.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname>,&lt;swift-ring-builder-hostname></codeblock>
./helion/operations/replacing_drives_swift_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing Drives in a Swift Node</title>
./helion/operations/replacing_drives_swift_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/replacing_drives_swift_node.dita:          profile:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/replacing_drives_swift_node.dita:            playbook:<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server name></codeblock><p>In
./helion/operations/replacing_drives_swift_node.dita:            reimaged:<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj2</codeblock></p></li>
./helion/operations/replacing_drives_swift_node.dita:          files to get the host prefix (for example, helion) and the control plane name (for
./helion/operations/replacing_drives_swift_node.dita:          example, cp1). This gives you the hostname of the node. Configure the operating
./helion/operations/replacing_drives_swift_node.dita:            system:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock><p>In
./helion/operations/replacing_drives_swift_node.dita:            the following example, for <b>swobj2</b>, the hostname is
./helion/operations/replacing_drives_swift_node.dita:            <b>helion-cp1-swobj0002</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit helion-cp1-swobj0002*</codeblock></p></li>
./helion/operations/replacing_drives_swift_node.dita:            argument.<codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --ask-vault-pass --limit &lt;hostname></codeblock><p>For
./helion/operations/replacing_drives_swift_node.dita:            example:<codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --ask-vault-pass --limit helion-cp1-swobj0002*</codeblock></p></li>
./helion/operations/replacing_drives_swift_node.dita:              commands:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit &lt;hostname></codeblock><p>In
./helion/operations/replacing_drives_swift_node.dita:              <b>swobj2</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_drives_swift_node.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit helion-cp1-swobj0002-mgmt </codeblock></p></li>
./helion/operations/replacing_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing Nodes</title>
./helion/operations/replacing_swift_node.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Swift Node</title>
./helion/operations/replacing_swift_node.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/replacing_swift_node.dita:          processor:<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/replacing_swift_node.dita:          directory:<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/replacing_swift_node.dita:          node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
./helion/operations/replacing_swift_node.dita:            playbook:<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server id></codeblock><p>In
./helion/operations/replacing_swift_node.dita:            reimaged.<codeblock>cd ~/helion/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj5</codeblock></p></li>
./helion/operations/replacing_swift_node.dita:            ~/helion/my_cloud/definition/data/control_plane.yml</codeph> files to get the host
./helion/operations/replacing_swift_node.dita:          the hostname of the node. Configure the operating
./helion/operations/replacing_swift_node.dita:            system:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit &lt;hostname></codeblock><p>In
./helion/operations/replacing_swift_node.dita:            the following example, for <b>swobj5</b>, the hostname is
./helion/operations/replacing_swift_node.dita:            <b>helion-cp1-swobj0005</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/replacing_swift_node.dita:ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit helion-cp1-swobj0005*</codeblock></p></li>
./helion/operations/replacing_swift_node.dita:            that node, otherwise you need to specify the hostname of your Swift ring builder server
./helion/operations/replacing_swift_node.dita:            and the hostname of the node being replaced.</p>
./helion/operations/replacing_swift_node.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --ask-vault-pass --limit &lt;hostname>,&lt;swift-ring-builder-hostname></codeblock>
./helion/operations/restarting_nodes.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restarting Nodes</title>
./helion/operations/restart_ceph.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Ceph Hosts After a Reboot</title>
./helion/operations/restart_ceph.dita:  <abstract><shortdesc outputclass="hdphidden">Steps for recovering Ceph hosts after a
./helion/operations/restart_ceph.dita:      reboot.</shortdesc>If you have one or more Ceph hosts that have been rebooted, these steps
./helion/operations/restart_ceph.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/restart_ceph.dita:    <section id="steps"><title>Recovering Ceph Hosts After a Reboot</title>
./helion/operations/restart_ceph.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname&gt;</codeblock></li>
./helion/operations/restart_ceph.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname&gt;</codeblock></li>
./helion/operations/restart_ceph_rados.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restarting a Ceph RADOS Gateway Node</title>
./helion/operations/restart_ceph_rados.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/restart_ceph_rados.dita:            node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_ceph_rados.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;radosgw-node></codeblock></li>
./helion/operations/restart_ceph_rados.dita:            node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_ceph_rados.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;radosgw-node></codeblock></li>
./helion/operations/restart_ceph_rados.dita:              that is, after the <codeph>verb_hosts hlm-status.yml</codeph> playbook has completed
./helion/operations/restart_compute.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Compute Hosts After a Reboot</title>
./helion/operations/restart_compute.dita:  <abstract><shortdesc outputclass="hdphidden">Steps for recovering Compute hosts after a
./helion/operations/restart_compute.dita:      reboot.</shortdesc>If you have one or more Compute hosts that have been rebooted, these steps
./helion/operations/restart_compute.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/restart_controller.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restarting Controller Nodes</title>
./helion/operations/restart_controller.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/restart_controller.dita:      <p>These steps may also be used if the Host Status (ping) alarm is triggered for one or more
./helion/operations/restart_controller.dita:      <image href="../../media/hos.docs/opsconsole_hostalarm.png"/>
./helion/operations/restart_controller.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml</codeblock>
./helion/operations/restart_controller.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml --ask-vault-pass</codeblock>
./helion/operations/restart_controller.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_controller.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=&lt;hostname&gt;</codeblock>
./helion/operations/restart_controller.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=helion-cp-c1-m1-mgmt,helion-cp-c1-m2-mgmt,helion-cp-c1-m3-mgmt</codeblock>
./helion/operations/restart_controller.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</codeblock></li>
./helion/operations/restart_controller.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</codeblock></li>
./helion/operations/restart_controller.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/restart_controller.dita:ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
./helion/operations/retrieve_adminpassword.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Retrieving the Admin Password</title>
./helion/operations/retrieve_adminpassword.dita:      tools and API.</shortdesc><p>In a default <keyword keyref="kw-hos-phrase"/> installation there is a
./helion/operations/start_stop_services.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Starting and Stopping Individual
./helion/operations/start_stop_services.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/start_stop_services.dita:        <li>Stop the Nova services: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/start_stop_services.dita:ansible-playbook -i hosts/verb_hosts nova-stop.yml</codeblock>
./helion/operations/start_stop_services.dita:          <p>If you specify the <codeph> --limit &lt;hostname></codeph> option at the end of the
./helion/operations/start_stop_services.dita:            ansible-playbook command then you will only target the specific hosts in your
./helion/operations/start_stop_services.dita:            environment that you specify and thus only the Nova services that live on that host will
./helion/operations/start_stop_services.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/operations/start_stop_services.dita:        <li>Start the nova-compute service: <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/start_stop_services.dita:ansible-playbook -i hosts/verb_hosts nova-start.yml</codeblock>
./helion/operations/start_stop_services.dita:          <p>If you specify the <codeph> --limit &lt;hostname></codeph> option at the end of the
./helion/operations/start_stop_services.dita:            ansible-playbook command then you will only target the specific hosts in your
./helion/operations/start_stop_services.dita:            environment that you specify and thus only the Nova services that live on that host will
./helion/operations/start_stop_services.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/operations/start_stop_services.dita:        <li>You can list out your Nova services showing the host they exist on and what their
./helion/operations/start_stop_services.dita:| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
./helion/operations/start_stop_services.dita:        <li>SSH to the host that the service you want to start or stop exists on.</li>
./helion/operations/start_stop_services.dita:              <codeph>helion-cp1-comp0001-mgmt</codeph> host and use this command:
./helion/operations/start_stop_services.dita:              <codeph>helion-cp1-comp0001-mgmt</codeph> host and use this command:
./helion/operations/start_stop_services.dita:                  directory:<codeblock>~/scratch/ansible/next/hos/ansible</codeblock></li>
./helion/operations/start_stop_services.dita:                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml</codeblock></li>
./helion/operations/start_stop_services.dita:                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
./helion/operations/start_stop_services.dita:                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-start.yml</codeblock></li>
./helion/operations/start_stop_services.dita:                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
./helion/operations/start_stop_services.dita:                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-stop.yml</codeblock></li>
./helion/operations/start_stop_services.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/start_stop_services.dita:ansible-playbook -i hosts/verb_hosts logging-stop.yml</codeblock>
./helion/operations/start_stop_services.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/start_stop_services.dita:ansible-playbook -i hosts/verb_hosts logging-start.yml</codeblock>
./helion/operations/swift_node_maintenance.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Swift Node Maintenance</title>
./helion/operations/systemmon_log_usage.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>System Monitoring, Logging, and Usage
./helion/operations/systemmon_log_usage.dita:    sections to help you monitor your <keyword keyref="kw-hos-phrase"/> cloud.</abstract>
./helion/operations/system_maintenance.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>System Maintenance</title>
./helion/operations/system_maintenance.dita:      keyref="kw-hos-phrase"/> cloud.</abstract>
./helion/operations/thirdparty_driver_support.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Third-party Driver Support</title>
./helion/operations/thirdparty_driver_support.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/thirdparty_driver_support.dita:      <p><keyword keyref="kw-hos-phrase"/> comes with a selection of built-in plugins and drivers
./helion/operations/thirdparty_driver_support.dita:        additions to a <keyword keyref="kw-hos"/> deployment.</p>
./helion/operations/thirdparty_driver_support.dita:          <keyword keyref="kw-hos"/> environment. The second will be from the perspective of a
./helion/operations/thirdparty_driver_support.dita:        developer who wants to package their driver for use on <keyword keyref="kw-hos"/>.</p>
./helion/operations/thirdparty_driver_support.dita:                    third-party drivers forward with new <keyword keyref="kw-hos"/>
./helion/operations/thirdparty_driver_support.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/operations/thirdparty_driver_support.dita:ansible-playbook -i hosts/localhost venv-edit.yml \
./helion/operations/thirdparty_driver_support.dita:ansible-playbook -i hosts/localhost venv-edit.yml \
./helion/operations/thirdparty_driver_support.dita:          keyref="kw-hos"/> versions</title>
./helion/operations/thirdparty_driver_support.dita:        target venv (and therefore the host that it'll be deployed on), native extensions can be
./helion/operations/thirdparty_driver_support.dita:        operating system; Ceph's `rados.py` is an example of this. For those third-party drivers,
./helion/operations/thirdparty_driver_support.dita:              <codeblock>cd ~/helion/hos/ansible
./helion/operations/thirdparty_driver_support.dita:ansible-playbook -i hosts/localhost venv-edit.yml -e source=/opt/hlm_packager/neutron-20151026T074248Z.tgz -e suffix=0001 -e wheelhouse=~/wheelhouse -e wheel='networking_l2gw'</codeblock></li>
./helion/operations/thirdparty_driver_support.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/thirdparty_driver_support.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
./helion/operations/troubleshooting/contacting_support.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Support Resources</title>
./helion/operations/troubleshooting/contacting_support.dita:      <p>The public knowledge base for <keyword keyref="kw-hos-phrase"/> can be reached at <xref
./helion/operations/troubleshooting/objectstorage/deploy_fails_with_msdos_disks.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Deployment Fails With - MSDOS Disks Labels
./helion/operations/troubleshooting/objectstorage/deploy_fails_with_msdos_disks.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/examine_details_planned_ring_changes_prior_deploy.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Examining Planned Ring Changes</title>
./helion/operations/troubleshooting/objectstorage/examine_details_planned_ring_changes_prior_deploy.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/examine_details_planned_ring_changes_prior_deploy.dita:          playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/objectstorage/examine_details_planned_ring_changes_prior_deploy.dita:ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/filesystem_label.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Verifying a Swift File System Label</title>
./helion/operations/troubleshooting/objectstorage/filesystem_label.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/filesystem_usage_nowipe.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Swift File System Usage
./helion/operations/troubleshooting/objectstorage/filesystem_usage_nowipe.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/objectstorage/filesystem_usage_nowipe.dita:ansible-playbook -i hosts/verb_hosts swift-start.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/identify_ring_builder.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Identifying the Swift Ring Building
./helion/operations/troubleshooting/objectstorage/identify_ring_builder.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/identify_ring_builder.dita:          command:<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/objectstorage/identify_ring_builder.dita:ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-ACC[0]</codeblock></li>
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Increasing the Swift Node Timeout Value</title>
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/objectstorage/increase_timeout.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/interpreting_swift_validate_input_model.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Interpreting Swift Input Model Validation
./helion/operations/troubleshooting/objectstorage/interpreting_swift_validate_input_model.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/interpreting_swift_validate_input_model.dita:            host: padawan-ccp-c1-m3-mgmt</b><simpletable id="simpletable_j3n_lfj_2t">
./helion/operations/troubleshooting/objectstorage/label_on_partition.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Verifying a Swift Partition Label</title>
./helion/operations/troubleshooting/objectstorage/label_on_partition.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/recovering_builder_file.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Swift Builder Files</title>
./helion/operations/troubleshooting/objectstorage/recovering_builder_file.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/objectstorage/recovering_builder_file.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/objectstorage/recovering_builder_file.dita:ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/objectstorage/restart_deploy_from_scratch.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Restarting the Object Storage Deployment</title>
./helion/operations/troubleshooting/objectstorage/restart_deploy_from_scratch.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/recover_rabbit.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding and Recovering RabbitMQ after
./helion/operations/troubleshooting/recover_rabbit.dita:        <keyword keyref="kw-hos-phrase"/> cloud environment. It is important for cloud operators to
./helion/operations/troubleshooting/recover_rabbit.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/recover_rabbit.dita:        brokers communication between multiple services in your <keyword keyref="kw-hos-phrase"/>
./helion/operations/troubleshooting/recover_rabbit.dita:          last node down determined in the first step):<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;hostname></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:          <note>The <codeph>&lt;hostname></codeph> value will be the "shortname" for your node, as
./helion/operations/troubleshooting/recover_rabbit.dita:            found in the <codeph>/etc/hosts</codeph> file.</note></li>
./helion/operations/troubleshooting/recover_rabbit.dita:        RabbitMQ host or not. The primary host is going to be the first host member in the
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:          <codeph>rabbit_primary_hostname</codeph> parameter to specify the hostname for one of the
./helion/operations/troubleshooting/recover_rabbit.dita:        other controller nodes in your environment hosting RabbitMQ, which will service as the
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_you_are_bringing_up></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_you_are_bringing_up></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:        <li>Run the <codeph>rabbitmq-stop.yml</codeph> playbook, specifying the hostname of the node
./helion/operations/troubleshooting/recover_rabbit.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing></codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:        <li>Run the <codeph>hlm-stop.yml</codeph> playbook, again specifying the hostname of the
./helion/operations/troubleshooting/recover_rabbit.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;hostname_of_node_you_are_removing></codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:          <codeblock>sudo rabbitmqctl forget_cluster_node rabbit@&lt;hostname_of_node_you_are_removing></codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:      <p>If the node you are removing/replacing is your primary host then when you are adding it to
./helion/operations/troubleshooting/recover_rabbit.dita:        your cluster then you will want to ensure that you specify a new primary host when doing so,
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_you_are_adding></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:      <p>If the node you are removing/replacing is not your primary host then you can add it as
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_you_are_adding></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:      <p>If the rebooted node was your primary RabbitMQ host, you will specify a different primary
./helion/operations/troubleshooting/recover_rabbit.dita:        hostname using one of the other nodes in your cluster:</p>
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_that_rebooted></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:      <p>If the rebooted node was not the primary RabbitMQ host then you can just start it back up
./helion/operations/troubleshooting/recover_rabbit.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_that_rebooted></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
./helion/operations/troubleshooting/recover_rabbit.dita:        keyref="kw-hos-phrase"/> is likely to recover automatically without any further action
./helion/operations/troubleshooting/recover_rabbit.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-status.yml</codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:        those services are present in your cloud. These services are effected by the fan-out queues
./helion/operations/troubleshooting/recover_rabbit.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts logging-server-configure.yml</codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/recover_rabbit.dita:    <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_that_needs_recovered></codeblock>
./helion/operations/troubleshooting/recover_rabbit.dita:    <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_rabbit.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered></codeblock>
./helion/operations/troubleshooting/recover_vertica.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Vertica after File System
./helion/operations/troubleshooting/recover_vertica.dita:    <p>In <keyword keyref="kw-hos-phrase"/>, the monitoring service uses Vertica as its database. If
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>~/helion/hos/ansible/roles/monasca-variables/defaults/main.yml</codeblock>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags monasca-schema-vertica</codeblock></li>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/verb_hosts monasca-vertica-dbclean.yml -e "force='yes'"</codeblock></li>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags vertica,monasca-schema-vertica</codeblock></li>
./helion/operations/troubleshooting/recover_vertica.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags monasca-api,persister
./helion/operations/troubleshooting/recover_vertica.dita:ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api,persister</codeblock></li>
./helion/operations/troubleshooting/troubleshooting_issues.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Issues</title>
./helion/operations/troubleshooting/troubleshooting_issues.dita:      <keyword keyref="kw-hos-phrase"/> cloud.</abstract>
./helion/operations/troubleshooting/troubleshooting_issues.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/troubleshooting_sosreport.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Retrieving the SOS Report</title>
./helion/operations/troubleshooting/troubleshooting_sosreport.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/troubleshooting_sosreport.dita:ansible-playbook -i hosts/verb_hosts sosreport-run.yml</codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Ceph Service</title>
./helion/operations/troubleshooting/ts_ceph.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_ceph.dita:FATAL: all hosts have already failed -- aborting</codeblock>
./helion/operations/troubleshooting/ts_ceph.dita:          <codeblock>TASK: [CEP-OSD | configure | Configure the osds of {{ inventory_hostname }}] *** 
./helion/operations/troubleshooting/ts_ceph.dita:          deployment.<codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts site.yml </codeblock></p>
./helion/operations/troubleshooting/ts_ceph.dita:skipping: [localhost]
./helion/operations/troubleshooting/ts_ceph.dita:fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
./helion/operations/troubleshooting/ts_ceph.dita:fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
./helion/operations/troubleshooting/ts_ceph.dita:FATAL: all hosts have already failed -- aborting  
./helion/operations/troubleshooting/ts_ceph.dita:            <codeph>site.yml</codeph> playbook and include the hostnames of all the <b>Ceph monitor
./helion/operations/troubleshooting/ts_ceph.dita:            <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml --limit
./helion/operations/troubleshooting/ts_ceph.dita:                command:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit new-compute,monitor-1,monitor-2,monitor-3</codeblock></p></li>
./helion/operations/troubleshooting/ts_ceph.dita:            <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml -limit
./helion/operations/troubleshooting/ts_ceph.dita:                @new_nodes.txt</codeph><p>You can add the monitor hosts to the text file (named
./helion/operations/troubleshooting/ts_ceph.dita:                  <codeph>new_nodes.txt</codeph>), which specifies the new compute host (along with
./helion/operations/troubleshooting/ts_ceph.dita:                monitor hosts) as follows:<ul id="ul_ak5_fgq_5t">
./helion/operations/troubleshooting/ts_ceph.dita:changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'cinder-volume'}, 'name': 'volumes', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 100, 'permission': 'rwx'}}))
./helion/operations/troubleshooting/ts_ceph.dita:changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'nova'}, 'name': 'vms', 'attrs': {'creation_policy': 'eager', 'type': 'replicated', 'pg': 100, 'permission': 'rwx'}}))
./helion/operations/troubleshooting/ts_ceph.dita:skipping: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'type': 'erasure', 'creation_policy': 'lazy', 'replica_size': 2, 'pg': 128, 'permission': 'rwx'}}))
./helion/operations/troubleshooting/ts_ceph.dita:failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'glance'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'creation_policy': 'eager', 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set images pgp_num 128", "delta": "0:00:00.456878", "end": "2015-10-19 12:08:37.379630", "item": [{"user": {"name": "glance", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128}, "name": "images", "usage": {"purpose": "glance-datastore"}}], "rc": 16, "start": "2015-10-19 12:08:36.922752", "warnings": []}
./helion/operations/troubleshooting/ts_ceph.dita:failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder-backup'}}, {'usage': {'purpose': 'cinder-backup'}, 'name': 'backups', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set backups pgp_num 128", "delta": "0:00:00.242184", "end": "2015-10-19 12:08:37.824764", "item": [{"user": {"name": "cinder-backup", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128, "replica_size": 3, "type": "replicated"}, "name": "backups", "usage": {"purpose": "cinder-backup"}}], "rc": 16, "start": "2015-10-19 12:08:37.582580", "warnings": []}
./helion/operations/troubleshooting/ts_ceph.dita:FATAL: all hosts have already failed -- aborting</codeblock>
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>~/helion/hos/ansible/roles/_CEP-CMN/files/logrotate.conf</codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>cd ~/helion/hos/ansible/
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>cd ~/helion/hos/ansible/
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:-2      3 host padawan-ceph-ccp-ceph0003-mgmt
./helion/operations/troubleshooting/ts_ceph.dita:-4      3 host padawan-ceph-ccp-ceph0001-mgmt
./helion/operations/troubleshooting/ts_ceph.dita:-3      3 host padawan-ceph-ccp-ceph0002-mgmt
./helion/operations/troubleshooting/ts_ceph.dita:          <li>Remove the OSD host from crush map if all osd's on the particular host is down:
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>ceph --cluster &lt;cluster-name> osd crush remove &lt;osd-hostname></codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:          <li>If the OSD node is up, but one/more OSDs on that node are down, remove those OSD's
./helion/operations/troubleshooting/ts_ceph.dita:            <codeblock>ceph --cluster &lt;cluster-name> osd crush add osd.&lt;osd-num> 1.0 host=&lt;osd-hostname></codeblock></li>
./helion/operations/troubleshooting/ts_ceph.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;radosgw-node> </codeblock>
./helion/operations/troubleshooting/ts_ceph.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;radosgw-node></codeblock></p>
./helion/operations/troubleshooting/ts_ceph.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;radosgw-node></codeblock></p>
./helion/operations/troubleshooting/ts_ceph.dita:      </p><codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/operations/troubleshooting/ts_ceph.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;radosgw-node> </codeblock></section>
./helion/operations/troubleshooting/ts_compute.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Compute Service</title>
./helion/operations/troubleshooting/ts_compute.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_compute.dita:            node, that is, the host with <codeph>consoleauth_host_index=0</codeph>. If
./helion/operations/troubleshooting/ts_compute.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</codeblock>
./helion/operations/troubleshooting/ts_image.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Glance Service</title>
./helion/operations/troubleshooting/ts_image.dita:      <p>In <ph conkeyref="HOS-conrefs/product-title"/> when creating a new image in the Horizon UI
./helion/operations/troubleshooting/ts_image.dita:          <codeblock>~/helion/hos/ansible/roles/GLA-API/defaults/main.yml</codeblock></li>
./helion/operations/troubleshooting/ts_image.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_image.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_image.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/troubleshooting/ts_image.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_image.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/ts_image.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
./helion/operations/troubleshooting/ts_image.dita:ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/ts_logging.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Centralized Logging Service</title>
./helion/operations/troubleshooting/ts_logging.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_logging.dita:            log files that were added <b>before</b> Beaver started watching those files. Similarly,
./helion/operations/troubleshooting/ts_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_logging.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/operations/troubleshooting/ts_logging.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_logging.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/ts_logging.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/ts_logging.dita:ansible-playbook -i hosts/verb_hosts logging-reconfigure.yml</codeblock></li>
./helion/operations/troubleshooting/ts_objectstorage.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Object Storage Troubleshooting</title>
./helion/operations/troubleshooting/ts_objectstorage.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_orchestration.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Heat Orchestration</title>
./helion/operations/troubleshooting/ts_orchestration.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_orchestration.dita:            playbooks:<codeblock>cd ~/helion/hos/ansible
./helion/operations/troubleshooting/ts_orchestration.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/operations/troubleshooting/ts_orchestration.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/operations/troubleshooting/ts_orchestration.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/troubleshooting/ts_orchestration.dita:ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml  </codeblock></li>
./helion/operations/troubleshooting/ts_rados.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting the Ceph RADOS Service</title>
./helion/operations/troubleshooting/ts_rados.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_telemetry.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Metering Service (Ceilometer):
./helion/operations/troubleshooting/ts_telemetry.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/troubleshooting/ts_telemetry.dita:      <p>Use the RabbitMQ CLI to re-start the instances and then the host.</p>
./helion/operations/troubleshooting/ts_telemetry.dita:          <p>Restart the RabbitMQ host</p>
./helion/operations/understanding_identity.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding Domains, Projects, Users, Groups,
./helion/operations/understanding_identity.dita:      <keyword keyref="kw-hos-phrase"/> identity service uses OpenStack Keystone and the concepts of
./helion/operations/understanding_identity.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/understanding_identity.dita:            <keyword keyref="kw-hos"/> keystone installation after the Keystone service has been
./helion/operations/understanding_identity.dita:        <li>The "default" domain is created automatically during the <keyword keyref="kw-hos"/>
./helion/operations/understanding_identity.dita:          the <keyword keyref="kw-hos"/> keystone installation process.</li>
./helion/operations/understanding_identity.dita:      <codeblock>~/helion/hos/ansible/roles/GLA-API/templates/policy.json.j2
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/ironic-common/files/policy.json
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/KEYMGR-API/templates/policy.json
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/heat-common/files/policy.json
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/CND-API/templates/policy.json
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/nova-common/files/policy.json
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/CEI-API/templates/policy.json.j2
./helion/operations/understanding_identity.dita:~/helion/hos/ansible/roles/neutron-common/templates/policy.json.j2 </codeblock>
./helion/operations/understanding_identity.dita:        reconfiguration playbooks, change directories to ~/scratch/ansible/next/hos/ansible and
./helion/operations/upload_image.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Uploading an Image for Use</title>
./helion/operations/upload_image.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/operations/upload_image.dita:          keyref="kw-hos"/> lifecycle manager provides an Ansible playbook that will download a
./helion/operations/upload_image.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/operations/upload_image.dita:ansible-playbook -i hosts/verb_hosts glance-cloud-configure.yml -e proxy=&#60;PROXY></codeblock>
./helion/overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Overview</title>
./helion/overview.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/overview.dita:    <p><keyword keyref="kw-hos-phrase"/> is the latest OpenStack-based infrastructure-as-a-service
./helion/overview.dita:      <p>With the release of <keyword keyref="kw-hos-phrase"/> comes an easy installation and
./helion/overview.dita:        install and configure them individually. Instead, in <keyword keyref="kw-hos-phrase"/>,
./helion/overview.dita:      <p>These files allow customization, and <keyword keyref="kw-hos-phrase"/> ships with complete
./helion/overview.dita:          keyref="kw-hos-phrase"/>, the configuration processor, a set of Python scripts that
./helion/overview.dita:      <p>As you will read in the <keyword keyref="kw-hos-phrase"/>
./helion/overview.dita:        combines it with the service definitions provided by <keyword keyref="kw-hos"/> and any
./helion/overview.dita:        understanding of how <keyword keyref="kw-hos-phrase"/> allows you to deploy and manage your
./helion/overview.dita:      <p>The example configurations that ship with <keyword keyref="kw-hos-phrase"/> are described
./helion/overview.dita:            keyref="kw-hos-phrase"/>: Example Configurations</xref></p>
./helion/overview.dita:          keyref="kw-hos-phrase"/> documentation to refer to the features described in this
./helion/overview.dita:        may perform management tasks such as those below, along with many others.</p>
./helion/overview.dita:      <p>In <keyword keyref="kw-hos-phrase"/>, the YAML files that define your cloud configuration
./helion/pdf30.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Documentation PDF</title>
./helion/pdf30.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/planning/high_availability.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>High Availability (HA) Concepts</title>
./helion/planning/high_availability.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/planning/high_availability.dita:      <p>By design, <keyword keyref="kw-hos"/> strives to create a system architecture resilient to
./helion/planning/high_availability.dita:          <li>If the nova-compute KVM hypervisors/servers hosting a project compute instance
./helion/planning/high_availability.dita:        <p><keyword keyref="kw-hos"/> Compute hypervisors do not support transparent high
./helion/planning/high_availability.dita:      <p>The <keyword keyref="kw-hos"/> installer deploys highly available configurations of
./helion/planning/high_availability.dita:      <p>The nova-api service, which is listening for requests on the IP of its host machine, then
./helion/planning/high_availability.dita:        <p>It is important for the HA setup to tolerate network failures, specifically those that
./helion/planning/high_availability.dita:      <p><keyword keyref="kw-hos-phrase-20"/> implemented controller level HA which required a
./helion/planning/high_availability.dita:      <p>In <keyword keyref="kw-hos-phrase-30"/> we are implementing L3 HA mechanism that was
./helion/planning/high_availability.dita:          <keyword keyref="kw-hos-phrase-30"/> uses keepalived package of the pacemaker resource
./helion/planning/high_availability.dita:      <p><keyword keyref="kw-hos"/> offers APIs, CLIs and Horizon UIs for the administrator to
./helion/planning/high_availability.dita:      <note>By default, <keyword keyref="kw-hos"/> is deployed in a single availability zone upon
./helion/planning/high_availability.dita:      <p>Nova host aggregates and Nova availability zones can be used to segregate Nova compute
./helion/planning/high_availability.dita:                  HA Proxy type load balancer in your application VMs until <keyword keyref="kw-hos"
./helion/planning/high_availability.dita:            <keyword keyref="kw-hos"/> platform services such as Designate, the DNS service. </li>
./helion/planning/high_availability.dita:        <p>The lifecycle manager in <keyword keyref="kw-hos"/> is not highly-available. The
./helion/planning/planning_preinstall.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Planning and Pre-installation</title>
./helion/planning/planning_preinstall.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/releasenotes30.dita:    <title><keyword keyref="kw-hos-tm"/>
./helion/releasenotes30.dita:        <keyword keyref="kw-hos-version-30"/>: Release Notes</title>
./helion/releasenotes30.dita:                    keyref="kw-hos-phrase"/>, including known issues and workarounds for this
./helion/releasenotes30RC1.dita:    <title><keyword keyref="kw-hos-tm"/>
./helion/releasenotes30RC1.dita:        <keyword keyref="kw-hos-version-30"/>: Release Candidate 1 Release Notes</title>
./helion/releasenotes30RC1.dita:                    keyref="kw-hos-phrase"/> Release Candidate 1. 
./helion/releasenotes30RC1.dita:            <p>Preliminary release notes for <keyword keyref="kw-hos-phrase"/> are available <xref keyref="releasenotes30">here</xref>. </p>
./helion/releasenotes30RC1.dita:             <p>It is not possible to upgrade from <keyword keyref="kw-hos"/> 2.* to  <keyword
./helion/releasenotes30RC1.dita:                    keyref="kw-hos-phrase"/> Release Candidate 1. Also, it will not be possible to upgrade from Release Candidate 1
./helion/releasenotes30RC2.dita:    <title><keyword keyref="kw-hos-tm"/>
./helion/releasenotes30RC2.dita:        <keyword keyref="kw-hos-version-30"/>: Release Candidate 2 Release Notes</title>
./helion/releasenotes30RC2.dita:                keyref="kw-hos-phrase"/> Release Candidate 2. </p>
./helion/releasenotes30RC2.dita:            <p>Preliminary release notes for <keyword keyref="kw-hos-phrase"/> are available <xref keyref="releasenotes30">here</xref>. </p>
./helion/releasenotes30RC2.dita:            following: <codeblock>hos-3.0.0-20160414T004740Z.tar</codeblock> For example:
./helion/releasenotes30RC2.dita:            <codeblock>tar xvf /media/cdrom/hos-3.0.0-20160414T004740Z.tar</codeblock> Continue with
./helion/releasenotes30RC2.dita:             <p>It is not possible to upgrade from <keyword keyref="kw-hos"/> 2.* to <keyword
./helion/releasenotes30RC2.dita:                    keyref="kw-hos-phrase"/> Release Candidate 2. Also, it will not be possible to
./helion/security/admin_role_segregation.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Service Admin Role Segregation in the Identity
./helion/security/admin_role_segregation.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/admin_role_segregation.dita:        privileges required to perform only those administrative tasks and no others. This prevents
./helion/security/admin_role_segregation.dita:    <section><title/>The following are the new roles defined in <keyword keyref="kw-hos-phrase"/>.
./helion/security/admin_role_segregation.dita:          <dd>Assign this role to users whose job function it is to perform nova (compute) related
./helion/security/admin_role_segregation.dita:          <dd>Assign this role to users whose job function it is to perform neutron (networking)
./helion/security/admin_role_segregation.dita:          <dd> Assign this role to users whose job function it is to perform cinder (storage)
./helion/security/admin_role_segregation.dita:          <dd> Assign this role to users whose job function it is to perform glance (image service)
./helion/security/arcsight.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Threat Monitoring and Integration with HPE ArcSight</title>
./helion/security/arcsight.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/arcsight.dita:    <p><keyword keyref="kw-hos"/>, through its centralized logging infrastructure (Logstash), can be
./helion/security/arcsight.dita:      monitoring of the entire <keyword keyref="kw-hos"/> infrastructure.The HPE ArcSight connector in
./helion/security/arcsight.dita:      configuration inside an <keyword keyref="kw-hos"/> environment, further converting it to a Common
./helion/security/arcsight.dita:    <p>Integration with HPE ArcSight gives you the ability to monitor, analyze, and correlate <keyword keyref="kw-hos"/> 
./helion/security/arcsight.dita:      continuous compliance across an <keyword keyref="kw-hos"/> deployment including: <ul
./helion/security/arcsight.dita:      infrastructure by monitoring their entire <keyword keyref="kw-hos"/> cloud in addition to your
./helion/security/arcsight.dita:        format="html" scope="external">Monitoring <keyword keyref="kw-hos"/> with HPE ArcSight</xref>.</p>
./helion/security/barbican.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Key Management with the Barbican Service</title>
./helion/security/barbican.dita:          when upgrading</xref> from previous versions of <keyword keyref="kw-hos"/> (separate
./helion/security/barbican.dita:            keyref="kw-hos-phrase"/>, Cinder uses Barbican as its key manager when Barbican is
./helion/security/barbican.dita:          enabled. KeyMgr encrypts data in the virtualization host before writing data to the remote
./helion/security/barbican.dita:          disk. There are three options available in <keyword keyref="kw-hos"/>
./helion/security/barbican.dita:          keyref="kw-hos-phrase"/> from 2.x versions, you need follow these steps after upgrading
./helion/security/barbican.dita:      <p>New installations of <keyword keyref="kw-hos-phrase"/>:</p><ol>
./helion/security/barbican.dita:            keyref="kw-hos-phrase"/>, nothing else needs to be done in those files.</li>
./helion/security/barbican.dita:            keyref="kw-hos-phrase"/>, you will need to redeploy the service. Please refer to <xref
./helion/security/barbican.dita:          <codeblock>python  ~/helion/hos/ansible/roles/KEYMGR-API/templates/generate_kek</codeblock>The
./helion/security/barbican.dita:          <codeblock>~/helion/hos/ansible/roles/barbican-common/vars/barbican_deploy_config.yml</codeblock></li>
./helion/security/barbican.dita:          <codeblock>cd ~/helion/hos/ansible/
./helion/security/barbican.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/security/barbican.dita:      <note>Changing the master key can result in read errors for existing secrets as those secrets
./helion/security/barbican.dita:        master key is used, Barbican will not be able to read those secrets. Also it will not be
./helion/security/barbican.dita:        your client certs to nodes hosting the Barbican API server. </p><p>KMIP deployment
./helion/security/barbican.dita:          <keyword keyref="kw-hos-phrase"/> that includes support for auditing. It uses OpenStack
./helion/security/barbican.dita:          <codeblock>cd ~/helion/hos/ansible/
./helion/security/barbican.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/barbican.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
./helion/security/barbican.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
./helion/security/barbican.dita:        component=barbican-api,monitored_host_type=vip,api_endpoint=internal - -merge_metrics</codeblock>-->
./helion/security/barbican.dita:      <codeblock>cd ~/helion/hos/ansible/ 
./helion/security/barbican.dita:ansible-playbook -i hosts/localhost config-processor-run.yml 
./helion/security/barbican.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/security/barbican.dita:      playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/security/barbican.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
./helion/security/barbican_admin.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Key Management Service Administration</title>
./helion/security/barbican_admin.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/barbican_admin.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts barbican-status.yml</codeblock>In any
./helion/security/barbican_admin.dita:        host limit option. Note that Barbican administration tasks should be performed by an admin
./helion/security/barbican_admin.dita:          playbook:<codeblock>cd ~/helion/hos/ansible/
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/security/barbican_admin.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
./helion/security/barbican_admin.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/security/barbican_admin.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
./helion/security/barbican_admin.dita:      <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:$ ansible-playbook -i hosts/verb_hosts barbican-stop.yml</codeblock>
./helion/security/barbican_admin.dita:      <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:$ ansible-playbook -i hosts/verb_hosts barbican-start.yml</codeblock>
./helion/security/barbican_admin.dita:      that the value (shown in bold) is optional; it is used to set a user-chosen password. If left
./helion/security/barbican_admin.dita:      helion/hos/ansible/:<codeblock>$ cd ~/helion/hos/ansible/
./helion/security/barbican_admin.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
./helion/security/barbican_admin.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/security/barbican_admin.dita:$ cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:$ ansible-playbook -i hosts/verb_hosts barbican-reconfigure-credentials-change.yml</codeblock>Finally,
./helion/security/barbican_admin.dita:      <codeblock>ansible-playbook -i hosts/verb_hosts barbican-status.yml</codeblock></section>
./helion/security/barbican_admin.dita:        <li>Edit file <codeph>~/helion/hos/ansible/roles/KEYMGR-API/templates/api-logging.conf.j2
./helion/security/barbican_admin.dita:      playbook:<codeblock>cd ~/helion/hos/ansible/
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/security/barbican_admin.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/security/barbican_admin.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
./helion/security/configure_auditing.dita:      <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring Auditing</title>
./helion/security/configure_auditing.dita:$ cd ~/helion/hos/ansible
./helion/security/configure_auditing.dita:$ ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/configure_auditing.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
./helion/security/configure_auditing.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/security/configure_auditing.dita:$ source /opt/stack/venv/ansible-hos-3.0.0/bin/activate</codeblock>
./helion/security/configure_auditing.dita:          # The osconfig playbook uses the stub file /etc/hos/osconfig-ran to decide if disks were
./helion/security/configure_auditing.dita:          <codeblock>$ ansible -i hosts/verb_hosts KEY-API -a 'sudo rm -f /etc/hos/osconfig-ran'
./helion/security/configure_auditing.dita:$ ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API</codeblock>
./helion/security/configure_auditing.dita:          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
./helion/security/configure_auditing.dita:$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml
./helion/security/configure_auditing.dita:$ ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
./helion/security/configure_auditing.dita:                  <codeblock>$ ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
./helion/security/configure_auditing.dita:      <!--The above steps are based on the wiki: Enable or Disable HOS Audit-->
./helion/security/configure_auditing.dita:  <p>No. Centralized Logging is not enabled for audit logs in HOS 3.0. Audit logs are more sensitive than regular logs and they need to be stored in separate indices with access controls enabled.</p>
./helion/security/dar.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Data at Rest Encryption</title>
./helion/security/dar.dita:  <body><!--not tested-->    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/dar.dita:    <section> The data at rest features in <keyword keyref="kw-hos-phrase-30"/> include the Barbican
./helion/security/dar.dita:      Cinder volumes to be encrypted. <p>The Barbican service in <keyword keyref="kw-hos-phrase"/>
./helion/security/dar.dita:        <li>Native database: This is the default configuration in <keyword keyref="kw-hos-phrase"/>. </li>
./helion/security/dar.dita:          <codeblock>~/helion/hos/ansible/roles/KEYMGR-API/files/samples/barbican_kmip_plugin_config_sample.yml</codeblock></li>
./helion/security/dar.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/security/dar.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml -e@/tmp/kmip_plugin_certs.yml</codeblock></li><li>Provide HSM connection credentials for the Barbican service. In this step, provide  the KMIP
./helion/security/dar.dita:          here:<codeblock>~/helion/hos/ansible/roles/barbican-common/vars/barbican_deploy_config.yml</codeblock></li><li>Set
./helion/security/dar.dita:        </li><li>Next, add KMIP client connection credentials and KMIP server hostname and port to
./helion/security/dar.dita:barbican_kmip_port: 1234 barbican_kmip_host: 111.222.333.444</codeblock>
./helion/security/dar.dita:      <codeblock>cd ~/helion/hos/ansible/ 
./helion/security/dar.dita:ansible-playbook -i hosts/localhost ready-deployment.yml 
./helion/security/dar.dita:cd ~/scratch/ansible/next/hos/ansible 
./helion/security/dar.dita:ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock></li></ol>
./helion/security/dar.dita:    <section>The data-at-rest encryption model in <keyword keyref="kw-hos-phrase"/> provides support
./helion/security/dar.dita:      <codeblock>stack@localhost:~$ source  ~/service.osrc
./helion/security/dar.dita:stack@localhost:~$ cinder type-Â­create LUKS
./helion/security/dar.dita:stack@localhost:~$ cinder encryption-type-create \
./helion/security/dar.dita:      instance:<codeblock>stack@localhost:~$ cinder create --display-name testVolumeEncrypted --volume-type LUKS --availability-zone nova 1</codeblock>The
./helion/security/dar.dita:      <codeblock>stack@localhost:~$ cinder show 2ebf610b-98bf-4914-aee1-9b866d7b1897
./helion/security/dar.dita:    |         os-vol-host-attr:host         |  ha-volume-manager@lvm-1#LVM_iSCSI   |
./helion/security/encrypted_storage.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Encryption of Passwords and Sensitive Data</title>
./helion/security/encrypted_storage.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/encrypted_storage.dita:    <p> In <keyword keyref="kw-hos-phrase"/>, sensitive connection data is encrypted. The passwords that are
./helion/security/encrypted_storage.dita:                <p>The environment variable HOS_USER_PASSWORD_ENCRYPT_KEY must contain the key used
./helion/security/encrypted_storage.dita:                  to encrypt those passwords.</p>
./helion/security/encrypted_storage.dita:        library) then the environment variable HOS_USER_PASSWORD_ENCRYPT_KEY must contain the key
./helion/security/encrypted_storage.dita:        used to encrypt those passwords.</p><p>In the case where the HOS_USER_PASSWORD_ENCRYPT_KEY
./helion/security/encrypted_storage.dita:        will be performed on your passwords when using the hosencrypt.py script.</p>
./helion/security/encrypted_storage.dita:        data using the <codeph>hosencrypt.py</codeph> script. You may want to change the encryption
./helion/security/encrypted_storage.dita:        <li><b>iLO passwords, VSA password if encrypted with <codeph>hosencrypt.py</codeph></b> -
./helion/security/header_poisoning.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Preventing Host Header Poisoning</title>
./helion/security/header_poisoning.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/header_poisoning.dita:    <section> Depending on the environment and context of your <keyword keyref="kw-hos"/> deployment, it
./helion/security/header_poisoning.dita:      may be advisable to configure Horizon to protect against Host header poisoning (see ref. #1
./helion/security/header_poisoning.dita:      below) by using Django's ALLOWED_HOSTS setting (see ref. #2 below). To configure Horizon to
./helion/security/header_poisoning.dita:      use the ALLOWED_HOSTS setting, take the following steps: <ol id="ol_ctx_f2r_b5">
./helion/security/header_poisoning.dita:          allowed hostname(s). This needs to be done first, before configuring Horizon itself.
./helion/security/header_poisoning.dita:          Otherwise, if Horizon is first configured to restrict the values of the "Host" header on
./helion/security/header_poisoning.dita:            <li> On your lifecycle manager node, open <b>~/helion/hos/services/horizon.yml</b>
./helion/security/header_poisoning.dita:              <codeblock>- "option httpchk GET / HTTP/1.1\\r\\nHOST:\\ my.example.com" 
./helion/security/header_poisoning.dita:              In this example, my.example.com is the hostname associated with the Horizon VIP on the
./helion/security/header_poisoning.dita:              external API network. However, you aren't restricted to just one allowed host. In
./helion/security/header_poisoning.dita:              addition, allowed hosts can contain wildcards (though not in the horizon.yml file;
./helion/security/header_poisoning.dita:              there you must have an actual resolvable hostname or a routeable IP address). However,
./helion/security/header_poisoning.dita:              for this change to the haproxy healthcheck, it is suggested that the hostname
./helion/security/header_poisoning.dita:            <li>Change the line that sets the "ALLOWED_HOSTS" setting. This can be a list of
./helion/security/header_poisoning.dita:              hostnames and (V)IPs that eventually get routed to Horizon. Wildcards are supported.
./helion/security/header_poisoning.dita:              <codeblock>ALLOWED_HOSTS = ['my.example.com', '*.example.net', '192.168.245.6']</codeblock>
./helion/security/header_poisoning.dita:              In the above example, any HTTP request received with a hostname not matching any in
./helion/security/header_poisoning.dita:          <codeblock>cd ~/helion/hos/ansible 
./helion/security/header_poisoning.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
./helion/security/header_poisoning.dita:          since changing the ALLOWED_HOSTS setting in Horizon first will cause the default health
./helion/security/header_poisoning.dita:          check to fail, if it doesn't specify a "Host" header in the HTTP request sent to check the
./helion/security/header_poisoning.dita:          health of Horizon's Apache virtual host.
./helion/security/header_poisoning.dita:          <codeblock>cd ~/helion/hos/ansible 
./helion/security/header_poisoning.dita:ansible-playbook -i hosts/localhost ready-deployment.yml 
./helion/security/header_poisoning.dita:cd ~/scratch/ansible/next/hos/ansible
./helion/security/header_poisoning.dita:ansible-playbook -i hosts/verb_hosts horizon-deploy.yml 
./helion/security/header_poisoning.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-deploy.yml</codeblock>
./helion/security/header_poisoning.dita:      https://www.djangoproject.com/weblog/2013/feb/19/security/#s-issue-host-header-poisoning </section>
./helion/security/header_poisoning.dita:    <section>2 https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts </section>
./helion/security/middleware_auditing.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Introducing Auditing Support</title>
./helion/security/middleware_auditing.dita:  <body><!--not tested-->    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/middleware_auditing.dita:      key aspects of auditing support in <keyword keyref="kw-hos-phrase"/>: <ul>
./helion/security/middleware_auditing.dita:        <li>Auditing is a new non-core feature in <keyword keyref="kw-hos"/>.</li>
./helion/security/security_features.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>New Security Features Overview</title>
./helion/security/security_features.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/security_features.dita:    <section id="overview"><title>Security features in <keyword keyref="kw-hos-phrase"/></title>
./helion/security/security_features.dita:          keyref="kw-hos-phrase"/> provides capabilities that enable you to protect your data at
./helion/security/security_features.dita:        HIPAA/HITECH, Sarbanes/Oxley, and state privacy laws. </p><p>In <keyword keyref="kw-hos-phrase"/>, a number of security enhancements are
./helion/security/security_features.dita:      strong security measures as SELinux, malicious attacks on VMs and the underlying host OS are
./helion/security/security_features.dita:      endpoints</title> <p id="tls_internal">With <keyword keyref="kw-hos-phrase"/>, data transmission between internal
./helion/security/security_features.dita:          keyref="kw-hos-phrase"/>.
./helion/security/security_features.dita:      <keyword keyref="kw-hos-phrase"/> is now PCI (Payment Card Industry) ready, enabling retail
./helion/security/security_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Security Overview</title>
./helion/security/security_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/security_overview.dita:        <p>The <keyword keyref="kw-hos-phrase"/> Security topics include:</p>
./helion/security/security_overview.dita:                <li><xref href="header_poisoning.dita">Preventing Host Header Poisoning</xref></li>
./helion/security/tls_config.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>TLS Configuration</title>
./helion/security/tls_config.dita:    <section><title/>In <keyword keyref="kw-hos-phrase-30"/>, you can provide your own certificate
./helion/security/tls_config.dita:        service has a different domain name in <keyword keyref="kw-hos-phrase-30"/>. So it is
./helion/security/tls_config.dita:      <p>All of the <keyword keyref="kw-hos-phrase"/> example cloud models ship with TLS enabled on
./helion/security/tls_config.dita:          keyref="kw-hos-phrase-30"/>. Let's also assume that for the internal VIP you will use the
./helion/security/tls_config.dita:        <li>You should use a distinct name from those already existing in config/tls/cacerts. This
./helion/security/tls_config.dita:              <codeblock>ansible -i hosts/verb_hosts FND-STN -a 'sudo keytool -delete -alias debian:&lt;filename to remove> \
./helion/security/tls_config.dita:        <keyword keyref="kw-hos-phrase-30"/> expects a CA certificate to have a .crt extension or it
./helion/security/tls_config.dita:      <p><keyword keyref="kw-hos"/> generates its own internal certificates but is designed to allow
./helion/security/tls_config.dita:          for the CA certificate to be processed by <keyword keyref="kw-hos"/>. Detailed steps are
./helion/security/tls_config.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/security/tls_config.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>Verify
./helion/security/tls_config.dita:        internal CA that is self-signed and whose CA certificates are deployed on your
./helion/security/tls_config.dita:commonName = Common Name (hostname, IP, or your name)
./helion/security/tls_config.dita:      <codeblock>cd ~/helion/hos/ansible
./helion/security/tls_config.dita:        <codeph>ready-deployment.yml</codeph>:<codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/tls_config.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>If
./helion/security/tls_config.dita:        <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock></note>Then
./helion/security/tls_config.dita:      change directories:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock>Then stop and
./helion/security/tls_config.dita:      <!--to be fixed in next version. https://jira.hpcloud.net/browse/HLM-3779  and 3780--><codeblock>ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml
./helion/security/tls_config.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml</codeblock>Run
./helion/security/tls_config.dita:      hlm-reconfigure.yml:<codeblock>ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
./helion/security/tls_config.dita:      repository:<codeblock>cd ~/helion/hos/ansible/
./helion/security/tls_config.dita:      <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml
./helion/security/tls_config.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>Change
./helion/security/tls_config.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/security/tls_config.dita:ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml </codeblock>Note:
./helion/security/tls_config.dita:      hlm-reconfigure:<codeblock>ansible-playbook -i hosts/verb_hosts _tls-endpoint-reconfigure.yml</codeblock>
./helion/security/tls_overview.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Transport Layer Security (TLS)  Overview</title>
./helion/security/tls_overview.dita:        user communications to and between the <keyword keyref="kw-hos"/> services from internal and
./helion/security/tls_overview.dita:      <p>In <keyword keyref="kw-hos-phrase-30"/>, the following are enabled for TLS</p>
./helion/security/tls_overview.dita:            keyref="kw-hos-phrase-30"/>. The external name in the input model files (in
./helion/security/tls_overview.dita:    <section><title><keyword keyref="kw-hos-phrase-30"/> clean install vs. upgrade</title> <p>Clean install: all
./helion/security/ts_keymgmt.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Key Management</title>
./helion/security/ts_keymgmt.dita:  <body><!--not tested-->    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/using_apparmor.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Refining Access Control with AppArmor</title>
./helion/security/using_apparmor.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/security/using_apparmor.dita:    <section id="apparmor_helion"><title>AppArmor in <keyword keyref="kw-hos-phrase"/></title>
./helion/security/using_apparmor.dita:      <p> AppArmor in <keyword keyref="kw-hos-phrase"/> is installed and enabled on the KVM compute nodes by
./helion/security/using_apparmor.dita:        <li>Hypervisor breakout followed by compromise of hosting qemu/kvm process. </li>
./helion/upgrade/add_rados_gateway.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Adding the Rados Gateway to Existing Cloud</title>
./helion/upgrade/add_rados_gateway.dita:            <p id="ceph_radosgw_summary">The Entry-scale KVM with Ceph model in <keyword keyref="kw-hos-phrase"/> has been extended to include 
./helion/upgrade/enable_barbican_for_upgrade.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Enabling Barbican Support When Upgrading</title>
./helion/upgrade/enable_barbican_for_upgrade.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/enable_barbican_for_upgrade.dita:      is a new service added in <keyword keyref="kw-hos"/> and is not available in 2.x
./helion/upgrade/enable_barbican_for_upgrade.dita:        <keyword keyref="kw-hos"/> with your existing input model configuration files, you will have
./helion/upgrade/enable_barbican_for_upgrade.dita:      <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/upgrade/enable_barbican_for_upgrade.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock></section>
./helion/upgrade/enable_barbican_for_upgrade.dita:    <section id="tls"><title>TLS configuration </title>In <keyword keyref="kw-hos-phrase"/> internal
./helion/upgrade/enable_barbican_for_upgrade.dita:      the Barbican API node/host (which makes it the only service with back-end TLS support in
./helion/upgrade/enable_barbican_for_upgrade.dita:      <keyword keyref="kw-hos-phrase"/>). Therefore, you will need to make changes in existing
./helion/upgrade/enable_barbican_for_upgrade.dita:      <codeblock>$ ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
./helion/upgrade/enable_barbican_for_upgrade.dita:$ ansible-playbook -i hosts/localhost ready-deployment.yml
./helion/upgrade/enable_barbican_for_upgrade.dita:$ cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/enable_barbican_for_upgrade.dita:$ ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml
./helion/upgrade/enable_barbican_for_upgrade.dita:$ ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
./helion/upgrade/general_upgrade.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>General Upgrade Steps</title>
./helion/upgrade/general_upgrade.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/update_disk_models.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Updating Disk Models</title>
./helion/upgrade/update_disk_models.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/update_disk_models.dita:                <p>In a <keyword keyref="kw-hos-phrase"/> cloud, the controller and compute nodes have volume groups setup based
./helion/upgrade/update_disk_models.dita:                    configurations. In <keyword keyref="kw-hos-phrase"/>, new services have been added and some existing services 
./helion/upgrade/update_disk_models.dita:                    for the various volumes have changed. The disk templates in the examples folder for <keyword keyref="kw-hos-phrase"/> 
./helion/upgrade/update_disk_models.dita:            <p>This means that after upgrade to <keyword keyref="kw-hos-phrase"/>, your existing disks and volume allocations that originated in 
./helion/upgrade/update_disk_models.dita:                <keyword keyref="kw-hos-phrase-21"/> will now be incompatible with the recommended values for a 
./helion/upgrade/update_disk_models.dita:                cloud now running <keyword keyref="kw-hos-phrase"/> services.
./helion/upgrade/update_disk_models.dita:            <p>If you are planning on upgrading from <keyword keyref="kw-hos-phrase-21"/> to <keyword keyref="kw-hos-phrase"/>, this document
./helion/upgrade/update_disk_models.dita:                will need after upgrade to <keyword keyref="kw-hos-phrase"/>.
./helion/upgrade/upgrade21to30.dita:  <title>Upgrade from <keyword keyref="kw-hos-phrase-21"/> to <keyword keyref="kw-hos-phrase-30"/></title>
./helion/upgrade/upgrade21to30.dita:    <!--<p conkeyref="HOS-conrefs/applies-to"/>-->
./helion/upgrade/upgrade21to30.dita:      <title>Performing the Upgrade from HPE Helion OpenStack <keyword keyref="kw-hos-phrase-21"/> to <keyword keyref="kw-hos-phrase-30"/></title>
./helion/upgrade/upgrade21to30.dita:      <note type="warning">The process for upgrading HPE Helion OpenStack to version <keyword keyref="kw-hos-phrase-30"/> requires a
./helion/upgrade/upgrade21to30.dita:        instructions for the <xref href="/#devplatform/2.0/upgrade_HOS_21.html" format="html"
./helion/upgrade/upgrade21to30.dita:      <p>When upgrading from <keyword keyref="kw-hos-phrase-21"/> to <keyword
./helion/upgrade/upgrade21to30.dita:          keyref="kw-hos-phrase-30"/> you will use the same configuration files in your
./helion/upgrade/upgrade21to30.dita:        them to the git repository so that those changes are tracked and applied during the upgrade
./helion/upgrade/upgrade21to30.dita:        will by default run on the first node in the Cinder Volume host group. 
./helion/upgrade/upgrade21to30.dita:      <p>You may also install <keyword keyref="kw-hos-phrase-30"/> as a full install. Those
./helion/upgrade/upgrade21to30.dita:          <codeblock>cd scratch/ansible/next/hos/ansible/</codeblock></li>
./helion/upgrade/upgrade21to30.dita:          <codeblock>ansible-playbook -i hosts/verb_hosts designate-stop.yml
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts designate-start.yml</codeblock></li>
./helion/upgrade/upgrade21to30.dita:      <p>These steps assume you have an installed and working <keyword keyref="kw-hos-phrase-21"/>
./helion/upgrade/upgrade21to30.dita:        <!--, and that you have followed the <xref href="/#devplatform/2.0/upgrade_HOS_21.html"
./helion/upgrade/upgrade21to30.dita:        <li>Run the upgrade to apply updates to all nodes in the cloud (including the node hosting
./helion/upgrade/upgrade21to30.dita:          introduced in <keyword keyref="kw-hos-phrase-30"/>. Instructions on rebooting your cloud
./helion/upgrade/upgrade21to30.dita:        <li>Sign in and download the <keyword keyref="kw-hos-phrase-30"/> product and signature
./helion/upgrade/upgrade21to30.dita:          during the <keyword keyref="kw-hos-phrase-21"/> deployment, and mount the install media at
./helion/upgrade/upgrade21to30.dita:          <codeblock>tar faxv /media/cdrom/hos/hos-3.0.0-????????????.tar</codeblock></li>
./helion/upgrade/upgrade21to30.dita:          <codeblock>~/hos-3.0.0/hos-init.bash</codeblock></li>
./helion/upgrade/upgrade21to30.dita:          <codeblock>cd ~/helion/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/helion/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock><p><!--Removed step to comment out eon-upgrade.yml, per DOCS-2938--></p></li>
./helion/upgrade/upgrade21to30.dita:        <li>Get status of all your services, if desired <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml</codeblock>
./helion/upgrade/upgrade21to30.dita:        <li>Run the upgrade playbook (required for all models/deployments): <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-upgrade.yml</codeblock>
./helion/upgrade/upgrade21to30.dita:      <p>Expand the sections below for those instructions.</p>
./helion/upgrade/upgrade21to30.dita:        <p>Execute the following command on each controller node to determine which node is hosting
./helion/upgrade/upgrade21to30.dita:          migrate to in the <codeph>migrate_host</codeph> argument:</p>
./helion/upgrade/upgrade21to30.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --extra-vars "migrate_host=padawan-ccp-c1-m2-mgmt"</codeblock>
./helion/upgrade/upgrade21to30.dita:          node, that is, the host with <codeph>consoleauth_host_index=0</codeph>. To move it to
./helion/upgrade/upgrade21to30.dita:          <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</codeblock></p>
./helion/upgrade/upgrade21to30.dita:        retrieve a list of nodes in your cloud running control plane services. <codeblock>for i in $(grep -w cluster-prefix ~/helion/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</codeblock>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;controller node></codeblock></li>
./helion/upgrade/upgrade21to30.dita:            <p>Note that the current node being rebooted could be hosting the lifecycle manager.</p>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;controller node></codeblock>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit &lt;controller node></codeblock>
./helion/upgrade/upgrade21to30.dita:              href="http://docs.hpcloud.com/#devplatform/2.0/upgrade_HOS_21.html" format="html"
./helion/upgrade/upgrade21to30.dita:              href="http://docs.hpcloud.com/#devplatform/2.0/upgrade_HOS_21.html" format="html"
./helion/upgrade/upgrade21to30.dita:            credentials.<codeblock>nova list --host &lt;hostname&gt; --all-tenants</codeblock></li>
./helion/upgrade/upgrade21to30.dita:              HOS 3.0 that will be based on the Mitaka release, the only live-migration cases that
./helion/upgrade/upgrade21to30.dita:            command:</p><codeblock>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/upgrade/upgrade21to30.dita:            option:<codeblock>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
./helion/upgrade/upgrade21to30.dita:            Note: The [&lt;target compute host&gt;] option is optional. If you do not specify a
./helion/upgrade/upgrade21to30.dita:            target host then the nova scheduler will choose a node for you.<p>OR</p><p>Stop the
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;compute node></codeblock>
./helion/upgrade/upgrade21to30.dita:            nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock>
./helion/upgrade/upgrade21to30.dita:            '@&lt;filename>' to process all hosts listed in the file.
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;compute node></codeblock>
./helion/upgrade/upgrade21to30.dita:        Swift host <ol>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;swift node></codeblock>
./helion/upgrade/upgrade21to30.dita:            <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;swift node> </codeblock>
./helion/upgrade/upgrade21to30.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;vsa_node name></codeblock></li>
./helion/upgrade/upgrade21to30.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname></codeblock>
./helion/upgrade/upgrade21to30.dita:            observing that all the OSDs under the current host are reported as 'up')
./helion/upgrade/upgrade21to30.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname></codeblock>
./helion/upgrade/upgrade21to30.dita:        <codeblock>HP Helion OpenStack hos-2.1.0 (build 01-291)</codeblock>
./helion/upgrade/upgrade21to30.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade21to30.dita:        <codeblock>stack@helion-control-plane-cluster1-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ls *status*
./helion/upgrade/upgrade21to30.dita:    <p outputclass="hostarget"/>
./helion/upgrade/upgrade_ceph-rados.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>VPN Service Upgrade</title>
./helion/upgrade/upgrade_ceph-rados.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_ceph-rados.dita:      <title>Upgrade from <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_ceph-rados.dita:        <keyword keyref="kw-hos-version-20-21"/></title>
./helion/upgrade/upgrade_ceph-rados.dita:      <p>It is not possible to upgrade your <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_ceph-rados.dita:        <keyword keyref="kw-hos-version-20-21"/> VPNaaS objects to <keyword
./helion/upgrade/upgrade_ceph-rados.dita:          keyref="kw-hos-phrase-30"/> automatically. This is because the <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_ceph-rados.dita:        <keyword keyref="kw-hos-version-20-21"/> VPNaaS uses OpenSwan and the <keyword
./helion/upgrade/upgrade_ceph-rados.dita:          keyref="kw-hos-phrase-30"/> VPNaaS uses StrongSwan and currently there isn't a clean
./helion/upgrade/upgrade_ceph-rados.dita:          <li>Delete all <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_ceph-rados.dita:            <keyword keyref="kw-hos-version-20-21"/> VPNaaS IPSec site connection and VPN service
./helion/upgrade/upgrade_ceph-rados.dita:          <li>Perform the <keyword keyref="kw-hos-phrase-30"/> upgrade.</li>
./helion/upgrade/upgrade_ceph-rados.dita:          <li>Recreate the <keyword keyref="kw-hos-phrase-30"/> VPNaaS VPN service and IPSec site
./helion/upgrade/upgrade_ceph-rados.dita:      <p>Recreate the <keyword keyref="kw-hos-phrase-30"/> VPNaaS VPN service and IPSec site
./helion/upgrade/upgrade_ceph_model.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Upgrade Ceph Model from Single Network to Three Network</title>
./helion/upgrade/upgrade_ceph_model.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_ceph_model.dita:            <p id="ceph_upgrade_summary">New <keyword keyref="kw-hos-phrase"/> clouds use the three network Ceph model as documented in the 
./helion/upgrade/upgrade_delete_alarms.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Delete Alarms that are not Supported</title>
./helion/upgrade/upgrade_delete_alarms.dita:        <!--<p conkeyref="HOS-conrefs/applies-to"/>-->
./helion/upgrade/upgrade_delete_alarms.dita:    <p>A number of alarms that were supported in <keyword keyref="kw-hos-phrase-21"/> 
./helion/upgrade/upgrade_delete_alarms.dita:        have been removed in <keyword keyref="kw-hos-phrase-30"/>. After a successful upgrade, these
./helion/upgrade/upgrade_deprecated.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Update input Model to Remove Deprecated start-address and end-address</title>
./helion/upgrade/upgrade_deprecated.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_designate.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>DNS Service (Designate) Upgrade</title>
./helion/upgrade/upgrade_designate.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_designate.dita:      <p><keyword keyref="kw-hos"/>DNS Service can be upgraded to 3.0 by following the steps
./helion/upgrade/upgrade_designate.dita:    <section id="HOSUpgrade">
./helion/upgrade/upgrade_designate.dita:      <title>Step 1: Upgrade <keyword keyref="kw-hos"></keyword></title>
./helion/upgrade/upgrade_designate.dita:        <codeblock>ansible-playbook -i hosts/verb_hosts designate-stop.yml --limit=DES-API</codeblock>
./helion/upgrade/upgrade_designate.dita:      <note>This assumes you are connected to the <keyword keyref="kw-hos"/> lifecycle manager and have 
./helion/upgrade/upgrade_designate.dita:        The end location is required to be in <codeph>/home/stack/ </codeph>on the <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_designate.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade_designate.dita:ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
./helion/upgrade/upgrade_designate.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible
./helion/upgrade/upgrade_designate.dita:ansible-playbook -i hosts/verb_hosts designate-stop.yml</codeblock>
./helion/upgrade/upgrade_designate.dita:        <codeblock>cd ~/scratch/ansible/next/hos/ansible 
./helion/upgrade/upgrade_designate.dita:ansible-playbook -i hosts/verb_hosts designate-migrate-from-2.0.yml</codeblock>
./helion/upgrade/upgrade_designate.dita:      <keyword keyref="kw-hos"/> lifecycle manager node.
./helion/upgrade/upgrade_designate.dita:        <codeblock>$ ansible-playbook -i hosts/verb_hosts designate-migrate-from-2.0.yml
./helion/upgrade/upgrade_designate.dita:localhost                  : ok=0    changed=0    unreachable=0    failed=0
./helion/upgrade/upgrade_git_merge.dita:        <!--<p conkeyref="HOS-conrefs/applies-to"/>-->
./helion/upgrade/upgrade_git_merge.dita:            <p>When you perform an upgrade, <keyword keyref="kw-hos"/> 
./helion/upgrade/upgrade_git_merge.dita:            certain changes to your environment,  <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_git_merge.dita:            <p>For a general overview of how <keyword keyref="kw-hos"/> uses Git, see the 
./helion/upgrade/upgrade_git_merge.dita:                the end-user, while the <codeph>hos</codeph> branch contains the "upstream" 
./helion/upgrade/upgrade_git_merge.dita:Auto-merging hos/ansible/roles/nova-compute-esx/defaults/main.yml
./helion/upgrade/upgrade_git_merge.dita:Auto-merging hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:<b>CONFLICT (content): Merge conflict in hos/ansible/roles/nova-common/templates/nova.conf.j2</b>
./helion/upgrade/upgrade_git_merge.dita:Auto-merging hos/ansible/roles/nova-cli/tasks/availability_zones.yml
./helion/upgrade/upgrade_git_merge.dita:Auto-merging hos/ansible/roles/nova-api/templates/api-paste.ini.j2
./helion/upgrade/upgrade_git_merge.dita:        <b>both modified:   hos/ansible/roles/nova-common/templates/nova.conf.j2</b>
./helion/upgrade/upgrade_git_merge.dita:            <p>Edit the file <codeph>hos/ansible/roles/nova-common/templates/nova.conf.j2</codeph>
./helion/upgrade/upgrade_git_merge.dita:>>>>>>> hos</b>
./helion/upgrade/upgrade_git_merge.dita:            <p>This indicates that <keyword keyref="kw-hos-phrase-30"/> is trying to set the value of
./helion/upgrade/upgrade_git_merge.dita:            <p>Typically, the previous upstream version will be the last-but-one commit on the <codeph>hos</codeph> branch.
./helion/upgrade/upgrade_git_merge.dita:                   <li>The previous "upstream" version on the <codeph>hos</codeph> branch.</li>
./helion/upgrade/upgrade_git_merge.dita:                   <li>The new "upstream" version on the <codeph>hos</codeph> branch. </li>
./helion/upgrade/upgrade_git_merge.dita:git merge-base hos site
./helion/upgrade/upgrade_git_merge.dita:            the <codeph>hos</codeph> branch.</p>
./helion/upgrade/upgrade_git_merge.dita:git log hos --
./helion/upgrade/upgrade_git_merge.dita:                and so we can use the simplified name <codeph>hos^1</codeph> to identify that commit.</p>
./helion/upgrade/upgrade_git_merge.dita:git diff <b>hos^1</b> HEAD -- hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:diff --git a/hos/ansible/roles/nova-common/templates/nova.conf.j2 b/hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:--- a/hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:+++ b/hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:                from <keyword keyref="kw-hos-phrase"/>
./helion/upgrade/upgrade_git_merge.dita:                on the <codeph>hos</codeph> branch you can use 
./helion/upgrade/upgrade_git_merge.dita:                <codeph>git diff HEAD hos -- &lt;&lt;path/to/file>></codeph>:</p>
./helion/upgrade/upgrade_git_merge.dita:git diff HEAD hos -- hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:                <keyword keyref="kw-hos-phrase"/> wants to set.
./helion/upgrade/upgrade_git_merge.dita:                from <keyword keyref="kw-hos-phrase"/> with the previous upstream version from
./helion/upgrade/upgrade_git_merge.dita:git diff $(git merge-base hos HEAD) hos -- hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:                <keyword keyref="kw-hos-phrase"/> wants to set and the previous upstream value
./helion/upgrade/upgrade_git_merge.dita:git show <b>:1</b>:hos/ansible/roles/nova-common/templates/nova.conf.j2 
./helion/upgrade/upgrade_git_merge.dita:git show <b>:2</b>:hos/ansible/roles/nova-common/templates/nova.conf.j2 
./helion/upgrade/upgrade_git_merge.dita:git show <b>:3</b>:hos/ansible/roles/nova-common/templates/nova.conf.j2 
./helion/upgrade/upgrade_git_merge.dita:            <p>Edit the file <codeph>hos/ansible/roles/nova-common/templates/nova.conf.j2</codeph> and if you want to maintain 
./helion/upgrade/upgrade_git_merge.dita:>>>>>>> hos</b>
./helion/upgrade/upgrade_git_merge.dita:git show <b>:3</b>:hos/ansible/roles/nova-common/templates/nova.conf.j2 > \
./helion/upgrade/upgrade_git_merge.dita:hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:            <p>Now  edit the file <codeph>hos/ansible/roles/nova-common/templates/nova.conf.j2</codeph>
./helion/upgrade/upgrade_git_merge.dita:git diff hos -- hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:git add hos/ansible/roles/nova-common/templates/nova.conf.j2
./helion/upgrade/upgrade_git_merge.dita:git rm -rf hos
./helion/upgrade/upgrade_nova_resize_migrate.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Nova Resize and Nova Migrate Features may be Disabled after Upgrade</title>
./helion/upgrade/upgrade_nova_resize_migrate.dita:        <!--<p conkeyref="HOS-conrefs/applies-to"/>-->
./helion/upgrade/upgrade_nova_resize_migrate.dita:    <p>If you had enabled the Nova Resize or the Nova Migrate feature in your <keyword keyref="kw-hos-phrase-21"/> cloud,
./helion/upgrade/upgrade_octavia.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Upgrading to the LBaaS Octavia Driver.</title>
./helion/upgrade/upgrade_octavia.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_octavia.dita:      <title>Upgrade from <keyword keyref="kw-hos"/> <keyword keyref="kw-hos-version-20-21"/></title>
./helion/upgrade/upgrade_octavia.dita:      <p>In <keyword keyref="kw-hos-version-20-21"/> the default LBaaS driver was based on a
./helion/upgrade/upgrade_octavia.dita:        was commonly called <i>'the namespace driver'</i>. In <keyword keyref="kw-hos-phrase-30"/> a
./helion/upgrade/upgrade_octavia.dita:        <li>Run the <keyword keyref="kw-hos"/> update. The update will install the latest version of the 
./helion/upgrade/upgrade_octavia.dita:          processor.<codeblock>cd ~/helion/hos/ansible
./helion/upgrade/upgrade_octavia.dita:ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
./helion/upgrade/upgrade_overview.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Introduction to the Upgrade Procedure</title>
./helion/upgrade/upgrade_overview.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_overview.dita:            <title>Reasons to Upgrade to <keyword keyref="kw-hos-phrase-30"/></title>       
./helion/upgrade/upgrade_overview.dita:                <li><xref keyref="newfeatures30">New Features</xref> in <keyword keyref="kw-hos-phrase-30"/> 
./helion/upgrade/upgrade_overview.dita:            Ironic is deployed as a separate cloud using a specific <keyword keyref="kw-hos-phrase"/> installation
./helion/upgrade/upgrade_perform_upgrade.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Performing the Upgrade</title>
./helion/upgrade/upgrade_perform_upgrade.dita:      <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_postupgrade_tasks.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Post-Upgrade Tasks</title>
./helion/upgrade/upgrade_preupgrade_tasks.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Pre-Upgrade Tasks</title>
./helion/upgrade/upgrade_preupgrade_tasks.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_recover_rabbit.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering RabbitMQ after Failure during Upgrade</title>
./helion/upgrade/upgrade_recover_rabbit.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_recover_rabbit.dita:TASK: [rabbitmq | _reset-host | Stop the RabbitMQ application to enable reset] *** 
./helion/upgrade/upgrade_recover_rabbit.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery</codeblock>  
./helion/upgrade/upgrade_recover_rabbit.dita:            <codeblock>ansible-playbook -i hosts/verb_hosts hlm-upgrade</codeblock>            
./helion/upgrade/upgrade_toc.dita:    <title><ph conkeyref="HOS-conrefs/product-title"/>Upgrade</title>
./helion/upgrade/upgrade_toc.dita:        <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_vpnaas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>VPN Service Upgrade</title>
./helion/upgrade/upgrade_vpnaas.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/upgrade/upgrade_vpnaas.dita:      <title>Upgrade from <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_vpnaas.dita:        <keyword keyref="kw-hos-version-20-21"/></title>
./helion/upgrade/upgrade_vpnaas.dita:      <p>It is not possible to upgrade your <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_vpnaas.dita:        <keyword keyref="kw-hos-version-20-21"/> VPNaaS objects to <keyword
./helion/upgrade/upgrade_vpnaas.dita:          keyref="kw-hos-phrase-30"/> automatically. This is because the <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_vpnaas.dita:        <keyword keyref="kw-hos-version-20-21"/> VPNaaS uses OpenSwan and the <keyword
./helion/upgrade/upgrade_vpnaas.dita:          keyref="kw-hos-phrase-30"/> VPNaaS uses StrongSwan and currently there isn't a clean
./helion/upgrade/upgrade_vpnaas.dita:          <li>Delete all <keyword keyref="kw-hos"/>
./helion/upgrade/upgrade_vpnaas.dita:            <keyword keyref="kw-hos-version-20-21"/> VPNaaS IPSec site connection and VPN service
./helion/upgrade/upgrade_vpnaas.dita:          <li>Perform the <keyword keyref="kw-hos-phrase-30"/> upgrade.</li>
./helion/upgrade/upgrade_vpnaas.dita:          <li>Recreate the <keyword keyref="kw-hos-phrase-30"/> VPNaaS VPN service and IPSec site
./helion/upgrade/upgrade_vpnaas.dita:      <p>Recreate the <keyword keyref="kw-hos-phrase-30"/> VPNaaS VPN service and IPSec site
./helion/userguide/create_image.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating and Uploading a Glance Image</title>
./helion/userguide/create_image.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/userguide/create_image.dita:              href="../../media/hos.docs/horizon_uploadimage1.png"/></p></li>
./helion/userguide/create_image.dita:              <p><image href="../../media/hos.docs/horizon_uploadimage2.png"/></p></li>
./helion/userguide/create_keypair.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating a Key Pair</title>
./helion/userguide/create_keypair.dita:              href="../../media/hos.docs/create_keypair1.png"/></p></li>
./helion/userguide/create_keypair.dita:              <p><image href="../../media/hos.docs/create_keypair2.png"/></p></li>
./helion/userguide/create_keypair.dita:          button: <p><image href="../../media/hos.docs/create_keypair3.png"/></p></li>
./helion/userguide/create_network.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Creating a Private Network</title>
./helion/userguide/create_network.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/userguide/create_network.dita:        <li>Your Administrator has provided you with the IP address or hostname for the Horizon
./helion/userguide/create_network.dita:                Servers</b>, and <b>Host Routes</b> fields blank.</li>
./helion/userguide/lbaas.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using Load Balancing as a Service
./helion/userguide/lbaas.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/userguide/lbaas.dita:    <p><b><keyword keyref="kw-hos-phrase"/> LBaaS Configuration</b></p>
./helion/userguide/lbaas.dita:      a Horizon web interface in a future <keyword keyref="kw-hos"/> release. This document describes the
./helion/userguide/lbaas.dita:    <p>You can create TLS enabled Load Balancers in <keyword keyref="kw-hos-phrase"/> by following
./helion/userguide/lbaas.dita:    <p><keyword keyref="kw-hos-phrase"/> can support either LBaaS v1 or LBaaS v2 to allow for wide ranging
./helion/userguide/lbaas.dita:      <p><keyword keyref="kw-hos-phrase"/> LBaaS Configuration</p>
./helion/userguide/lbaas_heat.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>Using Load Balancing as a Service with
./helion/userguide/lbaas_heat.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/userguide/lbaas_heat.dita:      <p>In <keyword keyref="kw-hos-phrase-30"/>, the Orchestration Service provides support for
./helion/userguide/lbaas_heat.dita:      <p>For more information on configuring and using the <keyword keyref="kw-hos"/> Load Balancing
./helion/userguide/userguide_index.dita:  <title><ph conkeyref="HOS-conrefs/product-title"/>User Guide Overview</title>
./helion/userguide/userguide_index.dita:    <p conkeyref="HOS-conrefs/applies-to"/>
./helion/userguide/userguide_index.dita:    <p>This section contains user tasks for your <keyword keyref="kw-hos-phrase"/> cloud.</p>
