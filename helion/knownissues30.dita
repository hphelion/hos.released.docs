<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "concept.dtd" >
<topic xml:lang="en-us" id="knownissues30">
    <title>Known Issues in this Release</title>
    <body>
        <!--not tested-->

        <section>
            <title>RabbitMQ stalls the cluster in large scale</title>
            <!-- DOCS-3083 -->
            <p>The RabbitMQ management plugin will cause performance issues with large scale
                installations. The work around is to disable the plugin with the following command: <codeblock>sudo rabbitmq-plugins disable rabbitmq_management</codeblock>
                <note type="important">This should be done on all controllers running
                    RabbitMQ.</note>
            </p>
        </section>

        <section id="DOCS-3057" conkeyref="upgrade_recover_rabbit/recover_rabbit"/>

        <section id="DOCS-3012">
            <title>Upgrade from 2.1 to 3.0 fails due to Kibana error</title>
            
            <p>The failure results in output similar to:</p>
            
<codeblock>
TASK: [logging-server | status | Checking systemd status for Kibana] **********
failed: [helion-cp1-c1-m1-mgmt] => {"changed": false, "cmd": ["systemctl", "status", "kibana"], "delta": "0:00:00.023937", "end": "2016-04-14 10:09:20.297810", "rc": 3, "start": "2016-04-14 10:09:20.273873", "warnings": []}
stdout:  kibana.service - Kibana Service
   Loaded: loaded (/etc/systemd/system/kibana.service; enabled)
   Active: failed (Result: exit-code) since Thu 2016-04-14 09:04:45 UTC; 1h 4min ago
Main PID: 28978 (code=exited, status=143)
....
FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
_hlm-upgrade-base | Apply package updates ----------------------------- 204.28s
neutron-server | start | Restart the neutron-server -------------------- 91.09s
kafka | kafka | start | stop kafka service ----------------------------- 91.03s
rabbitmq | configure-users | Create RabbitMQ users --------------------- 70.62s
monasca-thresh | monasca-thresh | start | restart monasca thresh service -- 64.92s
CMC-DEP | install | Install CMC ---------------------------------------- 58.84s
horizon_post_configure | post-configure | Compress static files -------- 51.42s
horizon_post_configure | post-configure | Compress static files -------- 49.00s
horizon_post_configure | post-configure | Compress static files -------- 48.22s
vertica | vertica | upgrade vertica version | start vertica service ---- 46.73s
-------------------------------------------------------------------------------
Total: --------------------------------------------------------------- 4362.51s
           to retry, use: --limit @/home/stack/hlm-upgrade.retry

helion-cp1-c1-m1-mgmt      : ok=2538 changed=860  unreachable=0    failed=1
helion-cp1-c1-m2-mgmt      : ok=2392 changed=828  unreachable=0    failed=1
helion-cp1-c1-m3-mgmt      : ok=2392 changed=828  unreachable=0    failed=1
</codeblock>            
            
            <p>To workaround this failure, manually start Kibaba on all the controller nodes
                and then start the upgrade again.</p>
            
<codeblock>
sudo systemctl start kibana
</codeblock> 
 
        </section>

        <section>
            <title>NIC-Mapping</title>
            <!-- DOCS-2799 -->
            <p>This is only relevant if you are using the NIC mapping feature in your input model, you 
                are running hLinux and have multi-port cards.</p>
            <p>During internal testing it has been identified that the udev rules for multi-port NIC cards 
                generated by earlier versions of <keyword keyref="kw-hos"/> could prevent hLinux 4.4 
                from booting.</p>
            <p>For reference these rules are defined in this file:
                <codeph>/etc/udev/rules.d/90-helion-nic-mapping.rules</codeph>.</p>
            <p>During the <keyword keyref="kw-hos"/> upgrade this file will be re-generated with 
                updated contents. The new udev file fixes the issue that could prevent hLinux 4.4 from 
                booting while performing the required mapping of devices to device-names.</p>
        </section>


        <section>
            <title>Compute (Nova)</title>
            <p><!-- DOCS-2653 --><b>Change in policy.json File</b></p>
            <p>The <codeph>policy.json</codeph> file for the Nova service has been changed, although
                the location of the file remains the same:</p>
            <p>Old name/location: <codeph>~/helion/my_cloud/config/nova/policy.json</codeph></p>
            <p>New name/location: <codeph>~/helion/my_cloud/config/nova/policy.json.j2</codeph></p>
            <p>If you have made changes to this file in your environment, you should recreate these
                changes in the new file.</p>
            <p><b>Live Migration between KVM and RHEL Compute Hosts Isn't Supported</b></p>
            <p>If you are using both Linux for HPE Helion (KVM) and RHEL compute hosts, you cannot
                live migrate instances between them. Instances on KVM hosts can only be live
                migrated to other KVM hosts and the same for RHEL hosts. For more details about live
                migration, see <xref href="operations/live_migration.dita"/>.</p>
        </section>

        <section>
            <title>Compute (ESX)</title>
            <!-- DOCS-2925 -->
            <p><b>Unable to Create Instance Snapshot when Instance is Active</b></p>
            <p>There is a known issue with VMWare vCenter where if you have a compute instance in
                    <codeph>Active</codeph> state you will receive the error below when attempting
                to take a snapshot of it:</p>
            <codeblock>An error occurred while saving the snapshot: Failed to quiesce the virtual machine</codeblock>
            <p>The workaround for this issue is to stop the instance prior to taking the snapshot.
                For details on the workaround, see <xref
                    href="operations/troubleshooting/ts_compute.dita#troubleshootingNova/esx"/>.</p>
         
            <!-- DOCS-2999 -->
            <p><b>Parallel activations and/or deactivations are not allowed for ESX Compute</b></p>
            <p>You should not attempt parallel activations/deactivations for ESX computes using Eon
                command-line interface (CLI) . Eon CLI used for activate and deactivate are <codeph>eon
                    resource-activate</codeph> and <codeph>eon resource-deactivate</codeph>,
                respectively.</p>
                <p>You should check if cluster resource has changed to the desired states before 
                    attempting activation or deactivation of ESX cluster.  The desired states are <i>imported</i>, 
                    <i>activated</i> and <i>provisioned</i>. The ESX cluster compute states can be 
                    obtained using <codeph>eon resource-list</codeph> CLI.</p>
        </section>

        <section>
            <title>Block Storage (Cinder)</title>
            <p><!-- DOCS-2878 --> Although <keyword keyref="kw-hos-version-30"/> supports encrypted
                Block Storage volumes, the following limitations apply: <ul>
                    <li>Creating encrypted volumes from an image is not supported.</li>
                    <li>Creating encrypted volumes when using a Ceph backend is not supported. This
                        bug can be seen <xref href="https://bugs.launchpad.net/nova/+bug/1463525"
                            scope="external" format="html">here</xref>.</li>
                    <!-- DOCS-2979 -->
                    <li>Attaches and detaches of encrypted volumes are not supported with
                        non-multipath Fiber Channel backends.</li>
                </ul>
            </p>
            <!-- DOCS-2980 -->
            <p>Under the following conditions the <i>allow_availability_zone_fallback</i> flag must 
                be set to True in cinder.conf in order to boot Nova instances:
                <ul>
                    <li>The instance boot source is an image</li>
                    <li>A new volume is being created to persist the instance data</li>
                    <li>The availability zone specified is different to the Cinder availability zone</li>
                </ul>
            </p>
            <!-- DOCS-3001 -->
            <p> If a volume is created from a cached image or from a volume snapshot, and a size 
                   is specified that is different than the cached image or snapshot size, the volume 
                   will not be created at the requested size. Cinder will report that the volume size is 
                   the size requested; however the volume on the back-end will only be the size of 
                   the cached image or snapshot.  This is a known limitation in Cinder and applies 
                   to most back-ends, including 3Par and StoreVirtual.
            </p>
            <!-- DOCS-2998 -->
            <p>The Attach volume operation will fail frequently after scaling to 2200 instances with 
                  volume attached, if it is triggered with concurrency 25.  In order to avoid this, 
                  the <codeph>rpc_timeout</codeph> value must be increased to 180.   Increasing
                  the value merely postpones the error and is only meant to be a temporary solution. 
            </p>
            <!-- DOCS-3033 -->
            <p>Consistency groups are not supported with VSA backends.</p>
            <!-- DOCS-3029 -->
            <p>Cinder volumes on backends connected using native FCoE are not supported.</p>
            
            <!-- DOCS-2826 --> 
            <p>In some configurations where iSCSI targets are configured / enabled and entry 
                similar to the following may appear frequently in the syslog: 
                <codeblock>systemd-sysv-generator[&lt;pid&gt;]: Ignoring creation of an alias umountiscsi.service for itself</codeblock>
                This doesn't indicate a problem and is a result of systemd being overly 
                verbose for informational messages.</p>
        </section>
        
        <section>
            <title>Identity (Keystone)</title>
            <!-- DOCS-2513 --> 
            <p>There will be performance degradation to a user's account in your
                cloud environment if the number of roles assigned to that user goes above ~1000. You
                are advised to use the user account functionality in an efficient way so that this
                does not become an issue. This was reported in upstream OpenStack <xref
                    href="https://bugs.launchpad.net/keystone/+bug/1499555" scope="external"
                    format="html">here</xref>.</p>
            
            <p><b>Rados Gateway error in fetching token revocation list from Keystone</b></p>
            <!-- DOCS-2863 -->
            <p>Fetching the list of revoked tokens from OpenStack Keystone is only supported for 
                PKI tokens. <keyword keyref="kw-hos-phrase-30"/> does not support using PKI 
                tokens (disables it by default) due to security concerns.  The below error encountered 
                (periodically) in Rados Gateway logs can be safely ignored and does not require 
                any corrective action.  
                <codeblock>ERROR: keystone revocation processing returned error r=-22</codeblock></p>
            
        </section>

        <section>
            <title>Bare Metal (Ironic)</title>
            <p>Multipath in not supported for the boot device. In case multiple paths exist use
                zoning so that only one path is available. </p>
            <p> If there are multiple NICs that are connected to the Management network, and the
                first NIC does not have access, then deployment will fail. Ensure that the first
                NIC’s connection is UP. </p>
        </section>

        <section>
            <title>Network (Neutron)</title>
            <!-- DOCS-2857 -->
            <p>VM's may not get an IP address from the DHCP server when creating VMs in large scale.
                This may occur when booting a large number of VMs at the same time when all of the
                networking may not be up in time before the DHCP retries timeout. If this occurs,
                reboot the VM.</p>
        </section>
        
        <section>
            <title>RBAC (Neutron)</title>
            <!-- DOCS-2836 -->
            <p>Port creation on a shared network fails if <codeph>--fixed-ip</codeph> is specified
                in the <codeph>neutron port-create</codeph> command. This is reported in <xref
                    href="https://bugs.launchpad.net/neutron/+bug/1543756" format="html"
                    scope="external">https://bugs.launchpad.net/neutron/+bug/1543756</xref>.</p>
        </section>
        
        <section>
            <title>VPN Service(Neutron)</title>
            <!--DOCS-2840-->
            <p>In <keyword keyref="kw-hos"/> 2.X and <keyword keyref="kw-hos-phrase-30"/> the
                Metering agent cannot be used in conjunction with VPNaaS.</p>
        </section>

        <section>
            <title>Load Balancer Octavia Driver (Neutron)</title>
            <!--DOCS-2879-->
            <p>
                <ul>
                    <li>The Load Balancer Octavia driver requires KVM based compute nodes but also 
                        supports mixed environments. The Octavia driver currently provides limited load 
                        balancer failover functionality that may have a unpredictable failover time. Other 
                        load balancer solutions should be considered If predictable and fast (less than 
                        10 seconds) load balancer failover timing is required.</li>
            <!-- DOCS-2981 -->
                    <li>Using a Floating IP with Octavia will not work in DVR enabled networks. The 
                        recommendation is to create a router with DVR switched off when using a Floating IP.</li>
                </ul>
            </p>
        </section>
        
        <section>
            <title>Monitoring (Monasca)</title>
            <!-- DOCS-3010 -->
            <p>Whenever a system that is part of the Monasca cluster is rebooted, there is a
                possibility that the Monasca Threshold Engine will fail to recover. The symptom will
                be Alarms in the Ops Console that stay red or grey even though they should be green.
                The Monasca Threshold Engine is what determines the states of the alarms based on
                the measurements being received from the Monasca agents. </p>
            <p>The workaround is to restart the Monasca Threshold Engine.  Use the following
                commands to stop and start the Monasca Threshold Engine:
                <codeblock>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock>
            </p>
        </section>
        
        <section>
            <title>SOSReport error during data collection</title>
            <!-- HLX-1836 -->
            <p>In rare instances when issuing command 
                <codeph>ansible-playbook -i hosts/verb_hosts sosreport-run.yml</codeph> from the 
                lifecycle manager, one of the nodes may list an error about not being able to collect data. 
                If this issue occurs, ssh to the node that is experiencing the problem. Once on the node, 
                attempt to run <codeph>sudo sosreport -v</codeph>. If sosreport fails to complete 
                gathering, record what is shown on the screen. A reboot of the affected node will clear 
                the error condition. The exact messages last shown on screen on local run was:
                <codeblock>Running 64/87: openvswitch... 
[plugin:openvswitch] unpacked command tuple: ('ovs-dpctl -s show', 'None', 'None', 300, 'None')
*[plugin:openvswitch] collecting output of 'ovs-dpctl -s show' *</codeblock>
            </p>
        </section>
        
        <section id="DOCS-2555">
            <title>Change to Monasca Monitoring Roles in Keystone</title>
            <p>In Helion OpenStack version 2.x, the keystone role for submitting cross-tenant metrics was called  'monasca-agent'. In HOS 3.0, the keystone role for submitting cross-tenant metrics is now called 'monitoring-delegate'. Any user who submits cross-tenant metrics without the 'monitoring-delegate' role will now receive a '403 Forbidden' http response.</p>
        </section>
        
        <section id="DOCS-3036">
            <!-- https://jira.hpcloud.net/browse/DOCS-3036 -->
            <title>Cannot use wildcards in multipath blacklist on RHEL Boot from SAN nodes</title>
            
            <p>You will need to change your input model if your system includes RHEL nodes that are configured to boot from a multipath device (boot from SAN). Otherwise those nodes will not be able to access their boot devices when you reboot them.
                Most of the example models supplied with HOS 3.0 include a blacklist with wildcards like this:</p>
            
            <p><b>~/helion/my_cloud/config/multipath/multipath_settings.yml</b></p>
<codeblock>
multipath_blacklist:
  - device:
      vendor: ".*"
      product: ".*"
</codeblock>            
            <p>These do not work properly on RHEL 7.2, so you will need to change it to a specific list of devices.
                In particular, you need to ensure that the device you are booting from is not represented 
                in the blacklist stanza.
                For example, you could use the following to blacklist all volumes on the local HP SmartArray:</p>
            
            <p><b>Change to a specific list of device types:</b></p>
<codeblock>
multipath_blacklist:
- device:
vendor: "HP"
product: "LOGICAL VOLUME"
</codeblock>
            
            <p>The exact list that you need will vary depending on the hardware that is in use. 
                However, this example is appropriate for most HP servers that use SmartArray technology 
                for their local disks, assuming you don't want multipath configured on the local devices
            which is typically the case.
            </p>
            
            <note type="important">If you have a mix of RHEL and HPE Linux nodes in your compute pool, 
                you need to be aware that any configuration you use for <codeph>multipath_blacklist</codeph>
                is global and will also apply to the HPE Linux nodes.
            </note>
            
        </section>
        
        <section>
            <title>Cannot remove disks from disk model post deployment.</title>
            <!-- DOCS-2973 -->
            <p>You must ensure your device model aligns with your node configuration at all times.
                If you remove a device from a server, it may lead to renaming of devices on your system
                either immediately, or following a reboot or rescan, and you may invalidate your device model.
                Follow the documented procedures for replacement of broken disk drives so that the
                device model remains accurate.</p>
        </section>
        
        <section>
            <title>Agent monitoring feature of networking-vsphere needs to be disabled during installation or upgrade.</title>
            <!-- DOCS-2961 -->
            <p>You should disable Agent monitoring feature of networking-vsphere if you have modified the 
                <codeph>ml2_conf.ini.j2</codeph> file in the input model folder.  If the Agent monitoring is not
                disabled, this feature will generate false positives and place hosts into maintenance mode.</p>
            <p>To disable the Agent monitoring feature, set the configuration parameter
                    <codeph>enable_ovsvapp_monitor</codeph> to <codeph>False</codeph> in the [OVSVAPP] 
                section of the <codeph>ml2_conf.ini.j2</codeph> file. Once the parameter has been set, it is
                safe to proceed with installation or upgrade.</p>
            <p>Example:
                <codeblock>...
 [OVSVAPP]
 enable_ovsvapp_monitor = False
                </codeblock>
            </p>
        </section>
        
        <section>
            <title>Vertica upgrade failure</title>
            <!-- DOCS-3081 -->
            <p>On rare occasion Vertica hangs on start up during upgrade.  This is due to temporary network
                drops.  To resolve the issue, exit out of the hanging upgrade and then run 
                <codeph>monasca-start.yml --tags vertica</codeph>. You can then continue by rerunning the
                upgrade.
            </p>
        </section>
        
        <section>
            <title>Rapidly Creating VMs</title>
            
            <p>If creating large numbers of VMs in a synchronous manner
            (greater than 3 VMs per minute), VM creation failure of up to 10% may be experienced. To
            work around this behavior, lower the VM creation rate until consistent success is
            achieved.</p> <!--docs-1050-->
        </section>
        
        
        <section>
            <title>Fact cache entry isn't valid</title>
            <p><!--https://jira.hpcloud.net/browse/HLM-3357--> If at the very start of an Ansible
                run you see a message like the following:
                <codeblock>
Fact cache entry for host COMPUTE-0002 isn't valid, deleting and failing
Fact cache entry for host COMPUTE-0001 isn't valid, deleting and failing
ERROR: The JSON cache files COMPUTE-0002, COMPUTE-0001 were corrupt, or did not otherwise contain valid JSON data. They have been removed, so you can re-run your command now.
</codeblock>
                Then please just re-run your Ansible command. </p>
        </section>
        
        <section>
            <title>Remote Catalog (Sherpa)</title>
            <!-- DOCS-2806 -->
            <p>The Sherpa Service is no longer provided with <keyword keyref="kw-hos-phrase-30"/>.</p>
        </section>
        
        <section>
            <title>Best Practices for Backup and Restore (Freezer)</title>
            <!-- DOCS-3027 -->
            <p>You should not add or modify the files in the following standard control-plane
                directories. These directories are automatically backed up and modifying their
                contents will cause control-plane restore jobs to fail.</p>
            <p>
                <ol>
                    <li>Lifecycle Manager
                        <codeblock>/home/stack
/etc/ssh/
/etc/shadow
/etc/passwd
/etc/group
/var/lib/cobbler/
/srv/www/cobbler/</codeblock></li>
                    <li>Controllers
                        <codeblock>/var/audit
/var/lib/mysql/</codeblock></li>
                    <li>Swift Nodes
                        <codeblock>/etc/swiftlm/builder_dir/</codeblock></li>
                </ol>
            </p>
        </section>
        
        <section>
            <title>VSA server creation sometimes fails</title>
            <!-- STOR-1424 -->
            <p>Create VSA appliance playbook sometimes fails to complete on one or more VSA 
                servers. If this happens and the installation fails then re-run the playbook. For example, 
                if the installation failed when running the site.yml playbook, re-run the site.yml playbook.
            </p>
        </section>
        
        <section>
            <title>Security groups in an ESX Cloud</title>
            <!-- VNETEE-769 & HLX-1711 per Alok Kumar Maurya -->
            <p>You cannot have more than 400 virtual machines per security group
            in an ESX cloud.</p>
        </section>
    </body>
</topic>
