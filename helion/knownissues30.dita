<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "concept.dtd" >
<topic xml:lang="en-us" id="knownissues30">
    <title>Known Issues in this Release</title>
    <body>
        <!--not tested-->


        <section id="DOCS-3012">
            <title>Upgrade from 2.1 to 3.0 fails due to Kibana error</title>
            
            <p>The failure results in output similar to:</p>
            
<codeblock>
TASK: [logging-server | status | Checking systemd status for Kibana] **********
failed: [helion-cp1-c1-m1-mgmt] => {"changed": false, "cmd": ["systemctl", "status", "kibana"], "delta": "0:00:00.023937", "end": "2016-04-14 10:09:20.297810", "rc": 3, "start": "2016-04-14 10:09:20.273873", "warnings": []}
stdout:  kibana.service - Kibana Service
   Loaded: loaded (/etc/systemd/system/kibana.service; enabled)
   Active: failed (Result: exit-code) since Thu 2016-04-14 09:04:45 UTC; 1h 4min ago
Main PID: 28978 (code=exited, status=143)
....
FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
_hlm-upgrade-base | Apply package updates ----------------------------- 204.28s
neutron-server | start | Restart the neutron-server -------------------- 91.09s
kafka | kafka | start | stop kafka service ----------------------------- 91.03s
rabbitmq | configure-users | Create RabbitMQ users --------------------- 70.62s
monasca-thresh | monasca-thresh | start | restart monasca thresh service -- 64.92s
CMC-DEP | install | Install CMC ---------------------------------------- 58.84s
horizon_post_configure | post-configure | Compress static files -------- 51.42s
horizon_post_configure | post-configure | Compress static files -------- 49.00s
horizon_post_configure | post-configure | Compress static files -------- 48.22s
vertica | vertica | upgrade vertica version | start vertica service ---- 46.73s
-------------------------------------------------------------------------------
Total: --------------------------------------------------------------- 4362.51s
           to retry, use: --limit @/home/stack/hlm-upgrade.retry

helion-cp1-c1-m1-mgmt      : ok=2538 changed=860  unreachable=0    failed=1
helion-cp1-c1-m2-mgmt      : ok=2392 changed=828  unreachable=0    failed=1
helion-cp1-c1-m3-mgmt      : ok=2392 changed=828  unreachable=0    failed=1
</codeblock>            
            
            <p>To workaround this failure, manually start Kibaba on all the controller nodes
                and then start the upgrade again.</p>
            
<codeblock>
sudo systemctl start kibana
</codeblock> 
 
        </section>


        <section>
            <title>Compute (Nova)</title>
            <p><!-- DOCS-2653 --><b>Change in policy.json File</b></p>
            <p>The <codeph>policy.json</codeph> file for the Nova service has been changed, although
                the location of the file remains the same:</p>
            <p>Old name/location: <codeph>~/helion/my_cloud/config/nova/policy.json</codeph></p>
            <p>New name/location: <codeph>~/helion/my_cloud/config/nova/policy.json.j2</codeph></p>
            <p>If you have made changes to this file in your environment, you should recreate these
                changes in the new file.</p>
            <p><b>Live Migration between KVM and RHEL Compute Hosts Isn't Supported</b></p>
            <p>If you are using both Linux for HPE Helion (KVM) and RHEL compute hosts, you cannot
                live migrate instances between them. Instances on KVM hosts can only be live
                migrated to other KVM hosts and the same for RHEL hosts. For more details about live
                migration, see <xref href="operations/live_migration.dita"/>.</p>
        </section>

        <section><title>Compute (ESX)</title>
            <p><!-- DOCS-2925 --><b>Unable to Create Instance Snapshot when Instance is
                Active</b></p>
            <p>There is a known issue with VMWare vCenter where if you have a compute instance in
                    <codeph>Active</codeph> state you will receive the error below when attempting
                to take a snapshot of it:</p>
            <codeblock>An error occurred while saving the snapshot: Failed to quiesce the virtual machine</codeblock>
            <p>The workaround for this issue is to stop the instance prior to taking the snapshot.
                For details on the workaround, see <xref
                    href="operations/troubleshooting/ts_compute.dita#troubleshootingNova/esx"/>.</p>
        </section>

        <section>
            <title>Block Storage (Cinder)</title>
            <p><!-- DOCS-2878 --> Although <keyword keyref="kw-hos-version-30"/> supports encrypted
                Block Storage volumes, the following limitations apply: <ul>
                    <li>Creating encrypted volumes from an image is not supported.</li>
                    <li>Creating encrypted volumes when using a Ceph backend is not supported. This
                        bug can be seen <xref href="https://bugs.launchpad.net/nova/+bug/1463525"
                            scope="external" format="html">here</xref>.</li>
                    <!-- DOCS-2979 -->
                    <li>Attaches and detaches of encrypted volumes are not supported with
                        non-multipath Fiber Channel backends.</li>
                </ul>
            </p>
            <!-- DOCS-2980 -->
            <p>Under the following conditions the <i>allow_availability_zone_fallback</i> flag must 
                be set to True in cinder.conf in order to boot Nova instances:
                <ul>
                    <li>The instance boot source is an image</li>
                    <li>A new volume is being created to persist the instance data</li>
                    <li>The availability zone specified is different to the Cinder availability zone</li>
                </ul>
            </p>
            <!-- DOCS-3001 -->
            <p> If a volume is created from a cached image or from a volume snapshot, and a size 
                   is specified that is different than the cached image or snapshot size, the volume 
                   will not be created at the requested size. Cinder will report that the volume size is 
                   the size requested; however the volume on the back-end will only be the size of 
                   the cached image or snapshot.  This is a known limitation in Cinder and applies 
                   to most back-ends, including 3Par and StoreVirtual.
            </p>
            <!-- DOCS-2998 -->
            <p>The Attach volume operation will fail frequently after scaling to 2200 instances with 
                  volume attached, if it is triggered with concurrency 25.  In order to avoid this, 
                  the <codeph>rpc_timeout</codeph> value must be increased to 180.   Increasing
                  the value merely postpones the error and is only meant to be a temporary solution. 
            </p>
            <!-- DOCS-3033 -->
            <p>Consistency groups are not supported with VSA backends.</p>
            <!-- DOCS-3029 -->
            <p>Cinder volumes on backends connected using native FCoE are not supported.</p>
            
            <!-- DOCS-2826 --> 
            <p>In some configurations where iSCSI targets are configured / enabled and entry 
                similar to the following may appear frequently in the syslog: 
                <codeblock>systemd-sysv-generator[&lt;pid&gt;]: Ignoring creation of an alias umountiscsi.service for itself</codeblock>
                This doesn't indicate a problem and is a result of systemd being overly 
                verbose for informational messages.</p>
        </section>
        
        <section>
            <title>Identity (Keystone)</title>
            <p><!-- DOCS-2513 --> There will be performance degradation to a user's account in your
                cloud environment if the number of roles assigned to that user goes above ~1000. You
                are advised to use the user account functionality in an efficient way so that this
                does not become an issue. This was reported in upstream OpenStack <xref
                    href="https://bugs.launchpad.net/keystone/+bug/1499555" scope="external"
                    format="html">here</xref>.</p>
        </section>

        <section>
            <title>Bare Metal (Ironic)</title>
            <p>Multipath in not supported for the boot device. In case multiple paths exist use
                zoning so that only one path is available. </p>
            <p> If there are multiple NICs that are connected to the Management network, and the
                first NIC does not have access, then deployment will fail. Ensure that the first
                NICâ€™s connection is UP. </p>
        </section>

        <section>
            <title>Network (Neutron)</title>
            <!-- DOCS-2857 -->
            <p>VM's may not get an IP address from the DHCP server when creating VMs in large scale.
                This may occur when booting a large number of VMs at the same time when all of the
                networking may not be up in time before the DHCP retries timeout. If this occurs,
                reboot the VM.</p>
        </section>
        
        <section>
            <title>NIC-Mapping (Neutron)</title>
            <!-- DOCS-2799 -->
            <p>This is only relevant if you are using the NIC mapping feature in your input model, you 
                are running hLinux and have multi-port cards.</p>
            <p>During internal testing it has been identified that the udev rules for multi-port NIC cards 
                generated by earlier versions of <keyword keyref="kw-hos"/> could prevent hLinux 4.4 
                from booting.</p>
            <p>For reference these rules are defined in this file:
                    <codeph>/etc/udev/rules.d/90-helion-nic-mapping.rules</codeph>.</p>
            <p>During the <keyword keyref="kw-hos"/> upgrade this file will be re-generated with 
                updated contents. The new udev file fixes the issue that could prevent hLinux 4.4 from 
                booting while performing the required mapping of devices to device-names.</p>
        </section>
        
        <section>
            <title>RBAC (Neutron)</title>
            <!-- DOCS-2836 -->
            <p>Port creation on a shared network fails if <codeph>--fixed-ip</codeph> is specified
                in the <codeph>neutron port-create</codeph> command. This is reported in <xref
                    href="https://bugs.launchpad.net/neutron/+bug/1543756" format="html"
                    scope="external">https://bugs.launchpad.net/neutron/+bug/1543756</xref>.</p>
        </section>
        
        <section>
            <title>VPN Service(Neutron)</title>
            <!--DOCS-2840-->
            <p>In <keyword keyref="kw-hos"/> 2.X and <keyword keyref="kw-hos-phrase-30"/> the
                Metering agent cannot be used in conjunction with VPNaaS.</p>
        </section>

        <section>
            <title>Load Balancer Octavia Driver (Neutron)</title>
            <!--DOCS-2879-->
            <p>
                <ul>
                    <li>The Load Balancer Octavia driver requires KVM based compute nodes but also 
                        supports mixed environments. The Octavia driver currently provides limited load 
                        balancer failover functionality that may have a unpredictable failover time. Other 
                        load balancer solutions should be considered If predictable and fast (less than 
                        10 seconds) load balancer failover timing is required.</li>
            <!-- DOCS-2981 -->
                    <li>Using a Floating IP with Octavia will not work in DVR enabled networks. The 
                        recommendation is to create a router with DVR switched off when using a Floating IP.</li>
                </ul>
            </p>
        </section>
        
        <section>
            <title>Monitoring (Monasca)</title>
            <!-- DOCS-3010 -->
            <p>Whenever a system that is part of the Monasca cluster is rebooted, there is a
                possibility that the Monasca Threshold Engine will fail to recover. The symptom will
                be Alarms in the Ops Console that stay red or grey even though they should be green.
                The Monasca Threshold Engine is what determines the states of the alarms based on
                the measurements being received from the Monasca agents. </p>
            <p>The workaround is to restart the Monasca Threshold Engine.  Use the following
                commands to stop and start the Monasca Threshold Engine:
                <codeblock>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock>
            </p>
        </section>
        
        <section id="DOCS-3036">
            <!-- https://jira.hpcloud.net/browse/DOCS-3036 -->
            <title>Cannot use wildcards in multipath blacklist on RHEL Boot from SAN nodes</title>
            
            <p>You will need to change your input model if your system includes RHEL nodes that are configured to boot from a multipath device (boot from SAN). Otherwise those nodes will not be able to access their boot devices when you reboot them.
                Most of the example models supplied with HOS 3.0 include a blacklist with wildcards like this:</p>
            
            <p><b>~/helion/my_cloud/config/multipath/multipath_settings.yml</b></p>
<codeblock>
multipath_blacklist:
  - device:
      vendor: ".*"
      product: ".*"
</codeblock>            
            <p>These do not work properly on RHEL 7.2, so you will need to change it to a specific list of devices.
                In particular, you need to ensure that the device you are booting from is not represented 
                in the blacklist stanza.
                For example, you could use the following to blacklist all volumes on the local HP SmartArray:</p>
            
            <p><b>Change to a specific list of device types:</b></p>
<codeblock>
multipath_blacklist:
- device:
vendor: "HP"
product: "LOGICAL VOLUME"
</codeblock>
            
            <p>The exact list that you need will vary depending on the hardware that is in use. 
                However, this example is appropriate for most HP servers that use SmartArray technology 
                for their local disks, assuming you don't want multipath configured on the local devices
            which is typically the case.
            </p>
            
            <note type="important">If you have a mix of RHEL and HPE Linux nodes in your compute pool, 
                you need to be aware that any configuration you use for <codeph>multipath_blacklist</codeph>
                is global and will also apply to the HPE Linux nodes.
            </note>
            
        </section>
        
        
        <section><title>Rapidly Creating VMs</title>
            
            <p>If creating large numbers of VMs in a synchronous manner
            (greater than 3 VMs per minute), VM creation failure of up to 10% may be experienced. To
            work around this behavior, lower the VM creation rate until consistent success is
            achieved.</p> <!--docs-1050-->
        </section>
        
        
        <section>
            <title>Fact cache entry isn't valid</title>
            <p><!--https://jira.hpcloud.net/browse/HLM-3357--> If at the very start of an Ansible
                run you see a message like the following:
                <codeblock>
Fact cache entry for host COMPUTE-0002 isn't valid, deleting and failing
Fact cache entry for host COMPUTE-0001 isn't valid, deleting and failing
ERROR: The JSON cache files COMPUTE-0002, COMPUTE-0001 were corrupt, or did not otherwise contain valid JSON data. They have been removed, so you can re-run your command now.
</codeblock>
                Then please just re-run your Ansible command. </p>
        </section>
        
    </body>
</topic>
