<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="example_configurations">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Example Configurations</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>The <keyword keyref="kw-hos-phrase"/> system ships with a collection of pre-qualified example
      configurations. These are designed to help you to get up and running quickly with a minimum
      number of configuration changes.</p>
    <p>The <keyword keyref="kw-hos"/> input model allows a wide variety of configuration parameters
      that may, at first glance, appear daunting. The example configurations are designed to
      simplify this process by providing pre-built and pre-qualified examples that need only a
      minimum number of modifications to get started.</p>
    <section id="contents">
      <ul>
        <li><xref href="example_configurations.dita#example_configurations/example_configs">
            <keyword keyref="kw-hos"/> Example Configurations</xref>
          <ul>
            <li><xref keyref="entryscale_kvm_vsa">Entry-Scale KVM with VSA Model</xref></li>
            
            <li><xref keyref="entryscale_kvm_dedicated">Entry-scale KVM with VSA model with Dedicated Cluster for Metering, Monitoring, and
                Logging</xref></li>
            <li><xref keyref="entryscale_esx">Entry-Scale ESX Model</xref></li>
            <li><xref keyref="entryscale_swift">Entry-Scale Swift Model</xref></li>
            <li><xref keyref="entryscale_ceph">Entry-Scale KVM with Ceph Model</xref></li>
            <li><xref keyref="midscale_kvm_vsa">Mid-Scale KVM with VSA Model</xref></li>
          </ul>
        </li>

       <li><xref keyref="alternative_configurations">Alternative Configurations</xref>
          <ul>
            <li><xref keyref="entryscale_ceph_multinetwork">Entry-Scale KVM with Ceph Model with Two Networks</xref>
              <!--<ul>
                <li><xref href="example_configurations.dita#example_configurations/ceph_nicmappings"
                    >Nic_mappings.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_netinterfaces"
                    >Net_interfaces.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_networksgroups"
                    >Network_groups.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph_networks"
                    >Networks.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_servergroups"
                    >Server_groups.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_firewallrules"
                    >Firewall_rules.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph_readme">Edit
                    the README.html and README.md Files</xref></li>
              </ul>-->
            </li>
            <li><xref keyref="entryscale_kvm_ceph_threenetwork">Entry-Scale KVM with Ceph Model with Three Networks</xref>
              <!--<ul>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph3_nicmappings"
                    >Nic_mappings.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph3_servers"
                    >Servers.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph3_netinterfaces"
                    >Net_interfaces.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph3_networkgroups"
                    >Network_groups.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph3_networks"
                    >Networks.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph3_servergroups"
                    >Server_groups.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph3_firewallrules"
                    >Firewall_rules.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph3_readme"
                    >Edit the README.html and README.md Files</xref></li>
              </ul>-->
            </li>
          </ul>
        </li>

        <li><xref
            keyref="modify_entryscale_kvm_vsa"
            >Modifying the Entry-scale KVM with VSA Model for Your Environment</xref>
       <ul>
            <li><xref keyref="localizing_inputmodel">Localizing the Input Model</xref>
              <!--<ul>
                <li><xref href="example_configurations.dita#example_configurations/networks"
                    >Networks.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/nicmappings"
                    >Nic_mappings.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/netinterfaces"
                    >Net_interfaces.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/networkgroups"
                    >Network_groups.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/servers"
                    >Servers.yml</xref></li>
              </ul>-->
            </li>
            <li><xref keyref="customizing_inputmodel">Customizing the Input Model</xref>
              <!--<ul>
                <li><xref href="example_configurations.dita#example_configurations/disks_controller"
                    >Disks_controller.yml</xref>
                  <ul>
                    <li><xref href="example_configurations.dita#example_configurations/filesystems"
                        >File Systems Storage</xref></li>
                    <li><xref href="example_configurations.dita#example_configurations/swiftstorage"
                        >Swift Storage</xref></li>
                  </ul>
                </li>
                <li><xref href="example_configurations.dita#example_configurations/disks_vsa"
                    >Disks_vsa.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/disks_compute"
                    >Disks_compute.yml</xref></li>
              </ul>-->
            </li>
            <li><xref href="example_configurations.dita#example_configurations/standalone">Using a
                Standalone Lifecycle-Manager Node</xref>
              <!--<ul>
                <li><xref
                    href="example_configurations.dita#example_configurations/control_plane_yml"
                    >Control_plane.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/server_roles_yml"
                    >Server_roles.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/net_interfaces_yml"
                    >Net_interfaces.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/disks_lifecycle_manager_yml"
                    >Disks_lifecycle_manager.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/servers_yml"
                    >Servers.yml</xref></li>
              </ul>-->
            </li>
            <li><xref href="example_configurations.dita#example_configurations/without_dvr"
                >Configuring <keyword keyref="kw-hos"/> without DVR</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/without_l3agent"
                >Configuring <keyword keyref="kw-hos"/> with Provider VLANs and Physical Routers
                Only</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/twosystems"
                >Considerations When Installing Two Systems on One Subnet</xref></li>
          </ul>
        </li>

      </ul>
    </section>
    <section id="example_configs"><title><keyword keyref="kw-hos"/> Example Configurations</title>
      <p>This section briefly describes the various example configurations and their capabilities.
        It also describes in detail, for the entry-scale-kvm-vsa example, how you can adapt the
        input model to work in your environment.</p>
      <!--<p><keyword keyref="kw-hos-phrase"/> ships with two classes of sample cloud models: examples
        and tech-preview. The models in the examples directory have been qualified by our Quality
        Engineering team, while the tech-preview models are more experimental.</p>-->
      <p>The following pre-qualified examples are shipped with <keyword keyref="kw-hos-phrase"
        />:</p>
      
      
      <table frame="all" rowsep="1" colsep="1" id="table_examples">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <thead>
            <row>
              <entry>Name</entry>
              <entry>Location</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><xref keyref="entryscale_kvm_vsa">Entry-scale KVM with VSA model</xref></entry>
              <entry><codeph>~/helion/examples/entry-scale-kvm-vsa</codeph></entry>                           
            </row>
            <row>
              <entry><xref keyref="entryscale_kvm_dedicated">Entry-scale KVM with VSA model with Dedicated Cluster for Metering, Monitoring, and
                Logging</xref> </entry>
              <entry><codeph>~/helion/examples/entry-scale-kvm-vsa-mml</codeph></entry>                           
            </row>
            <row>
              <entry><xref keyref="entryscale_swift">Entry-scale Swift-only model</xref></entry>
              <entry><codeph>~/helion/examples/entry-scale-swift</codeph></entry>                           
            </row>
            <row>
              <entry><xref keyref="entryscale_ceph_multinetwork">Entry-scale KVM with Ceph model</xref></entry>
              <entry><codeph>~/helion/examples/entry-scale-kvm-ceph</codeph></entry>                           
            </row>
            <row>
              <entry><xref keyref="entryscale_esx">Entry-scale ESX model</xref></entry>
              <entry><codeph>~/helion/examples/entry-scale-esx</codeph></entry>                           
            </row>
            <row>
              <entry><xref keyref="midscale_kvm_vsa">Mid-scale KVM with VSA model</xref></entry>
              <entry><codeph>~/helion/examples/mid-scale-kvm-vsa</codeph></entry>                           
            </row>
            
            
          </tbody>
        </tgroup>
      </table>

      <p>The entry-scale systems are designed to provide an entry-level solution that can be scaled
        from a small number of nodes to a moderately high node count (approximately 100 compute
        nodes, for example).</p>
      <p>In the mid-scale model, the cloud control plane is subdivided into a number of dedicated
        service clusters to provide more processing power for individual control plane elements.
        This enables a greater number of resources to be supported (compute nodes, Swift object
        servers). This model also shows how a segmented network can be expressed in the <keyword
          keyref="kw-hos"/> model.</p>
    </section>
    
    





<!--
    <section id="alternative"><title>Alternative Configurations</title>
      <p>In <keyword keyref="kw-hos-phrase"/> there are alternative configurations that we recommend
        for specific purposes and this section we will outline them.</p>
    </section>
-->







    
    
    <section id="standalone"><title>Using a Standalone Lifecycle-Manager Node</title>
      <p>All of the examples described above host the lifecycle manager on one of the control nodes.
        It is also possible to deploy this service on a dedicated node, as shown below: </p>
      <p><image href="../../media/examples/entry_scale_kvm_vsa_shared.png"/></p>
      <p><xref href="../../media/examples/entry_scale_kvm_vsa_shared_lg.png" scope="external"
          format="html">Download a high-resolution version</xref></p>
      <p>A typical use case for wanting to run the dedicated lifecycle manager is to be able to test
        the deployment of different configurations without having to re-install the first server.
        Some administrators might also prefer the additional security of keeping all of the
        configuration data on a separate server from those that users of the cloud connect to
        (although all of the data can be encrypted and SSH keys can be password protected).</p>
      <p>This requires the following changes to the configuration files:</p>
      <ul>
        <li>Update <uicontrol>control_plane.yml</uicontrol> to add the lifecycle manager.</li>
        <li>Update <uicontrol>server_roles.yml</uicontrol> to add the lifecycle manager role.</li>
        <li>Update <uicontrol>net_interfaces.yml</uicontrol> to add the interface definition for the
          lifecycle manager.</li>
        <li>Create a <uicontrol>disks_lifecycle_manager.yml</uicontrol> file to define the disk
          layout for the lifecycle manager.</li>
        <li>Update <uicontrol>servers.yml</uicontrol> to add the dedicated lifecycle manager
          node.</li>
      </ul>
    </section>
    <section id="control_plane_yml"><title>Control_plane.yml</title>
      <p>The snippet below shows the addition of a single node cluster into the control plane to
        host the lifecycle manager service. Note that, in addition to adding the new cluster, you
        also have to remove the lifecycle manager component from the <codeph>cluster1</codeph> in
        the examples:</p>
      <codeblock>
  clusters:
     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1	
       allocation-policy: strict
       service-components:
         - lifecycle-manager
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-server</codeblock>
      <p>This specifies a single node of role <codeph>LIFECYCLE-MANAGER-ROLE</codeph> hosting the
        lifecycle manager.</p>
    </section>
    <section id="server_roles_yml"><title>Server_roles.yml</title>
      <p>The snippet below shows the insertion of the new server roles definition:</p>
      <codeblock>
   server-roles:
      
      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS	
      
      - name: CONTROLLER-ROLE</codeblock>
      <p>This defines a new server role which references a new interface-model and disk-model to be
        used when configuring the server.</p>
    </section>
    <section id="net_interfaces_yml"><title>Net-interfaces.yml</title>
      <p>The snippet below shows the insertion of the network-interface info:</p>
      <codeblock>
    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</codeblock>
      <p>This assumes that the server uses the same physical networking layout as the other servers
        in the example. For details on how to modify this to match your configuration, see <xref
          keyref="localizing_inputmodel/netinterfaces"
          >Net_interfaces.yml</xref>.</p>
    </section>
    <section id="disks_lifecycle_manager_yml"><title>Disks_lifecycle_manager.yml</title>
      <p>In the examples, disk-models are provided as separate files (this is just a convention, not
        a limitation) so the following should be added as a new file named
          <codeph>disks_lifecycle_manager.yml</codeph>:</p>
      <codeblock>---
   product:
      version: 2
        
   disk-models:
   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5
        
     volume-groups:
       - name: hlm-vg
         physical-volumes:
           - /dev/sda_root
        
       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</codeblock>
    </section>
    <section id="servers_yml"><title>Servers.yml</title>
      <p>The snippet below shows the insertion of an additional server used for hosting the
        lifecycle manager. Provide the address information here for the server you are running on,
        i.e., the node where you have installed the <keyword keyref="kw-hos"/> ISO.</p>
      <codeblock>
  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required
        
     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: &#60;your IP address here>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       # ipmi information is not needed 
          
     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</codeblock>
    </section>



    <section id="without_dvr"><title>Configuring <keyword keyref="kw-hos"/> without DVR</title>
      <p>By default in the KVM model, the Neutron service utilizes distributed routing (DVR). This
        is the recommended setup because it allows for high availability. However, if you would like
        to disable this feature, here are the steps to achieve this.</p>
      <p>On your lifecycle manager, make the following changes:</p>
      <ol>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/neutron.conf.j2</codeph> file, change
          the line below from: <codeblock>router_distributed = {{ router_distributed }}</codeblock>
          <p>to:</p>
          <codeblock>router_distributed = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/ml2_conf.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_distributed_routing = True</codeblock>
          <p>to:</p>
          <codeblock>enable_distributed_routing = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/l3_agent.ini.j2</codeph> file, change
          the line below from: <codeblock>agent_mode = {{ neutron_l3_agent_mode }}</codeblock>
          <p>to:</p>
          <codeblock>agent_mode = legacy</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, remove
          the following values from the Compute resource <codeph>service-components</codeph> list:
          <codeblock>
   - neutron-l3-agent
   - neutron-metadata-agent</codeblock></li>
      </ol>
    </section>

    <section id="without_l3agent"><title>Configuring <keyword keyref="kw-hos"/> with Provider VLANs
        and Physical Routers Only</title>
      <p>Another option for configuring Neutron is to use provider VLANs and physical routers only,
        here are the steps to achieve this.</p>
      <p>On your lifecycle manager, make the following changes:</p>
      <ol>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/neutron.conf.j2</codeph> file, change
          the line below from: <codeblock>router_distributed = {{ router_distributed }}</codeblock>
          <p>to:</p>
          <codeblock>router_distributed = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/ml2_conf.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_distributed_routing = True</codeblock>
          <p>to:</p>
          <codeblock>enable_distributed_routing = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/dhcp_agent.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</codeblock>
          <p>to:</p>
          <codeblock>enable_isolated_metadata = True</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, remove
          the following values from the Compute resource <codeph>service-components</codeph> list:
          <codeblock>
  - neutron-l3-agent
  - neutron-metadata-agent</codeblock></li>
      </ol>
    </section>
 
    <section id="twosystems"><title>Considerations When Installing Two Systems on One Subnet</title>
      <p>If you wish to install two separate <ph conkeyref="HOS-conrefs/product-title"/> systems
        using a single subnet, you will need to consider the following notes.</p>
      <p>The <codeph>ip_cluster</codeph> service includes the <codeph>keepalived</codeph> daemon
        which maintains virtual IPs (VIPs) on cluster nodes. In order to maintain VIPs, it
        communicates between cluster nodes over the VRRP protocol.</p>
      <p>A VRRP virtual routerid identifies a particular VRRP cluster and must be unique for a
        subnet. If you have two VRRP clusters with the same virtual routerid, causing a clash of
        VRRP traffic, the VIPs are unlikely to be up or pingable and you are likely to get the
        following signature in your <codeph>/etc/keepalived/keepalived.log</codeph>:</p>
      <codeblock>Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: ip address associated with VRID not present in received packet : 10.2.1.11
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: one or more VIP associated with VRID mismatch actual MASTER advert
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: bogus VRRP packet received on br-bond0 !!!
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: VRRP_Instance(VI_2) ignoring received advertisment...</codeblock>
      <p>To resolve this issue, our recommendation is to install your separate <ph
          conkeyref="HOS-conrefs/product-title"/> systems with VRRP traffic on different
        subnets.</p>
      <p>If this is not possible, you may also assign a unique routerid to your separate <ph
          conkeyref="HOS-conrefs/product-title"/> system by changing the
          <codeph>keepalived_vrrp_offset</codeph> service configurable. The routerid is currently
        derived using the <codeph>keepalived_vrrp_index</codeph> which comes from a configuration
        processor variable and the <codeph>keepalived_vrrp_offset</codeph>.</p>
      <p>For example, </p>
      <ol>
        <li>Log in to your lifecycle manager.</li>
        <li>Edit your <codeph>~/helion/my_cloud/config/keepalived/defaults.yml</codeph> file and
          change the value of the following line: <codeblock>keepalived_vrrp_offset: 0</codeblock>
          <p>Change the off value to a number that uniquely identifies a separate vrrp cluster. For
            example:</p>
          <p><codeph>keepalived_vrrp_offset: 0</codeph> for the 1st vrrp cluster on this subnet.</p>
          <p><codeph>keepalived_vrrp_offset: 1</codeph> for the 2nd vrrp cluster on this subnet.</p>
          <p><codeph>keepalived_vrrp_offset: 2</codeph> for the 3rd vrrp cluster on this
          subnet.</p></li>
        <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
            repo</xref>, as follows:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "changing Admin password"</codeblock></li>
        <li>Run the configuration processor with this command:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Use the playbook below to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>If you are making this change after your initial install, run the following reconfigure
          playbook to make this change in your environment:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts FND-CLU-reconfigure.yml
        </codeblock></li>
      </ol>
    </section>
 
  
  </body>
</topic>
