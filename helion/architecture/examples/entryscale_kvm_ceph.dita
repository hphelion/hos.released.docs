<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="entryscale_kvm_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with Ceph Model</title>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <section id="about">
      <p>This example provides a KVM-based cloud using Ceph for both block and object storage.</p>
    </section>

    <section id="vlan">
      <p>The network traffic is segregated into the following VLANs:</p>
      <ul>
        <li><b>Cloud Management</b> - This is the network that will be used for all internal traffic
          between the cloud services.</li>
        <li><b>OSD Internal</b> - This is the network that will be used for internal traffic of
          cluster among Ceph OSD servers. Only Ceph OSD servers will need connectivity to this
          network.</li>
        <li><b>OSD Client</b> - This is the network that Ceph clients will use to talk to Ceph
          Monitor and OSDs. Cloud controllers, Nova Compute, Ceph Monitor, OSD and Rados Gateway
          servers will need connectivity to this network.</li>
      </ul>
      <p>This diagram below illustrates the physical networking used in this configuration.</p>
      <p><image href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_ceph_1000.png"/></p>
      <p><xref href="../../../media/hos.docs/exampleconfigs/entry_scale_kvm_ceph_5000.png"
          scope="external" format="html">Download a high-resolution version</xref></p>
      <p>This configuration is based on the <codeph>entry-scale-kvm-ceph</codeph> cloud input model
        which is included with the HPE Helion OpenStack distro. You will need to make the changes
        outlined below prior to the deployment of your Ceph cluster.</p>
    </section>

    <section id="recommended_minimums"><title>Recommended hardware minimums for the entry-scale KVM
        with Ceph model</title>
      <p>The table below lists out the key characteristics needed per server role for this
        configuration.</p>
      <table>
        <tgroup cols="8">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <colspec colname="c6" colnum="6"/>
          <colspec colname="c7" colnum="7"/>
          <colspec colname="c8" colnum="8"/>
          <thead id="thead">
            <row>
              <entry morerows="1">Node Type</entry>
              <entry morerows="1">Role Name</entry>
              <entry morerows="1">Required Number</entry>
              <entry namest="c4" nameend="c7" align="center">Server Hardware - Minimum Requirements
                and Recommendations</entry>
            </row>
            <row>
              <entry>Disk </entry>
              <entry>Memory</entry>
              <entry>Network</entry>
              <entry>CPU </entry>
            </row>
          </thead>
          <tbody>
            <row id="row_deployer">
              <entry>Dedicated lifecycle manager (optional)</entry>
              <entry>Lifecycle-manager</entry>
              <entry>1</entry>
              <entry>300 GB</entry>
              <entry>8 GB</entry>
              <entry>1 x 10 GB with PXE Support</entry>
              <entry>8 CPU (64-bit) cores total (can be Intel or AMD)</entry>
            </row>
            <row id="row_control_plane">
              <entry>Control Plane</entry>
              <entry>Controller</entry>
              <entry>3</entry>
              <entry>
                <ul>
                  <li>1 x 600 GB (minimum) - operating system drive</li>
                  <li>2 x 600 GB (minimum) - Data drive</li>
                </ul>
              </entry>
              <entry>64 GB</entry>
              <entry>2 x 10 GB with one PXE enabled NIC port</entry>
              <entry>8 CPU (64-bit) cores total (can be Intel or AMD)</entry>
            </row>
            <row id="row_compute">
              <entry>Compute (KVM hypervisor)</entry>
              <entry>Compute</entry>
              <entry>1-3</entry>
              <entry>2 X 600 GB (minimum)</entry>
              <entry>32 GB (memory must be sized based on the virtual machine instances hosted on
                the Compute node)</entry>
              <entry>2 x 10 GB with one PXE enabled NIC port</entry>
              <entry>8 CPU (64-bit) cores total (can be Intel or AMD) with hardware virtualization
                support. The CPU cores must be sized based on the VM instances hosted by the Compute
                node.</entry>
            </row>
            <row id="row_block_storage">
              <entry>CEPH-OSD</entry>
              <entry>ceph-osd</entry>
              <entry>0 or 3 (which will provide the recommended redundancy)</entry>
              <entry>3 X 600 GB (minimum)</entry>
              <entry>32 GB</entry>
              <entry>2 x 10 GB with one PXE enabled NIC port</entry>
              <entry>8 CPU (64-bit) cores total (can be Intel or AMD)</entry>
            </row>
            <row>
              <entry>RADOS Gateway</entry>
              <entry>radosgw</entry>
              <entry>2</entry>
              <entry>2 x 600 GB (minimum)</entry>
              <entry>32 GB</entry>
              <entry>2 x 10 GB with one PXE enabled NIC port</entry>
              <entry>8 CPU (64-bit) cores total (can be Intel or AMD)</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section id="ceph3_nicmappings"><title>nic_mappings.yml</title>
      <p>Ensure that your baremetal server NIC interfaces are correctly specified in the
          <codeph>~/helion/my_cloud/definition/data/nic_mappings.yml</codeph> file and that they
        meet the server requirements.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>nic-mappings:

## NIC specification for controller nodes.  A bonded interface is used for the management 
## network while a separate interface is used to connect to the Ceph nodes.
  - name: DL360p_4PORT
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
       - logical-name: hed2
         type: simple-port
         bus-address: "0000:08:00.0"
         
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:09:00.0"
       
       - logical-name: hed4
         type: simple-port
         bus-address: "0000:0a:00.0"
         
## NIC specification for compute nodes. One interface is used for the management 
## network while the second interface is used to connect to the Ceph nodes.
  - name: MY-2PORT-SERVER
    physical-ports:
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:04:00.0"

       - logical-name: hed4
         type: simple-port
         bus-address: "0000:04:00.1"

## NIC specification for OSD nodes. The first interface is used for management network
## traffic. The second interface is used for client or public traffic. The third
## interface is used for internal OSD traffic.
  - name: MY-4PORT-SERVER
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:06:00.0"

       - logical-name: hed2
         type: simple-port
         bus-address: "0000:06:00.1"

       - logical-name: hed3
         type: simple-port
         bus-address: "0000:06:00.2"

       - logical-name: hed4
         type: simple-port
         bus-address: "0000:06:00.3"</codeblock>
    </section>

    <section id="ceph3_servers"><title>servers.yml</title>
      <p>Ensure that your servers in the
          <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file are mapped to the
        correct NIC interface.</p>
      <p>An example with the bolded line for <codeph>nic-mapping</codeph> illustrating this:</p>
      <codeblock># Controller Nodes
  - id: controller1
    ip-addr: 10.13.111.138
    server-group: RACK1
    role: CONTROLLER-ROLE
    <b>nic-mapping: HP-DL360-4PORT</b>
    mac-addr: "f0:92:1c:05:69:10"
    ilo-ip: 10.12.8.214
    ilo-password: password
    ilo-user: admin
    
# Compute Nodes
  - id: compute1
    ip-addr: 10.13.111.139
    server-group: RACK1
    role: COMPUTE-ROLE
    <b>nic-mapping: MY-2PORT-SERVER</b>
    mac-addr: "83:92:1c:55:69:b0"
    ilo-ip: 10.12.8.215
    ilo-password: password
    ilo-user: admin
        
# OSD Nodes
  - id: osd1
    ip-addr: 10.13.111.140
    server-group: RACK1
    role: OSD-ROLE
    <b>nic-mapping: MY-4PORT-SERVER</b>
    mac-addr: "d9:92:1c:25:69:e0"
    ilo-ip: 10.12.8.216
    ilo-password: password
    ilo-user: admin

# Ceph RGW Nodes
  - id: rgw1
    ip-addr: 192.168.10.12
    role: RGW-ROLE
    server-group: RACK1
    nic-mapping: MY-2PORT-SERVER
    mac-addr: "8b:f6:9e:ca:3b:62"
    ilo-ip: 192.168.9.12
    ilo-password: password
    ilo-user: admin

  - id: rgw2
    ip-addr: 192.168.10.13
    role: RGW-ROLE
    server-group: RACK2
    nic-mapping: MY-2PORT-SERVER
    mac-addr: "8b:f6:9e:ca:3b:63"
    ilo-ip: 192.168.9.13
    ilo-password: password
    ilo-user: admin </codeblock>
    </section>

    <section id="ceph3_netinterfaces"><title>net_interfaces.yml</title>
      <p>Define a new interface set for your OSD interfaces in the
          <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>
- name: CONTROLLER-INTERFACES
  network-interfaces:
## This bonded interface is used by the controller 
## nodes for cloud management traffic.
    - name: BOND0
      device:
         name: bond0
      bond-data:
         options:
            mode: active-backup
            miimon: 200
            primary: hed1
         provider: linux
         devices:
            - name: hed1
            - name: hed2
        network-groups:
        - EXTERNAL-API
        - EXTERNAL-VM
        - GUEST
        - MANAGEMENT
## This interface is used to connect the controller
## node to the Ceph nodes so that any Ceph client
## like cinder-volume can route data directly to
## Ceph over this interface.
    - name: ETH2
      device:
        name: hed3
      network-groups:
        - OSD-CLIENT
        
- name: COMPUTE-INTERFACES
  network-interfaces:
    - name: HETH3
      device:
         name: hed3
      network-groups:
         - EXTERNAL-VM
         - GUEST
         - MANAGEMENT
## This interface is used to connect the compute node
## to the Ceph cluster so that a workload VM can route
## data traffic to the Ceph cluster over this interface.
    - name: HETH4
       device:
          name: hed4
       network-groups:
          - OSD-CLIENT
        
- name: OSD-INTERFACES
  network-interfaces:
## This defines the interface used for management 
## traffic like logging, monitoring, etc.
    - name: HETH1
      device:
          name: hed1
      network-groups:
        - MANAGEMENT
## This defines the interface used for client
## or data traffic.
    - name: HETH2
      device:
          name: hed2
      network-groups:
        - OSD-CLIENT
## This defines the interface used for internal
## cluster communication among OSD nodes.
    - name: HETH3
      device:
          name: hed3
      network-groups:
        - OSD-INTERNAL

   - name: RGW-INTERFACES
     network-interfaces:
       - name: BOND0
         device:
            name: bond0
         bond-data:
            options:
                mode: active-backup
                miimon: 200
                primary: hed3
            provider: linux
            devices:
              - name: hed3
              - name: hed4
         network-groups:
           - MANAGEMENT
           - OSD-CLIENT</codeblock>
    </section>

    <section id="ceph3_networkgroups"><title>network_groups.yml</title>
      <p>Define the OSD network group in the
          <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file:</p>
      <codeblock>#
# OSD client
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-CLIENT
  hostname-suffix: osdc
  
  component-endpoints
    - ceph-monitor
    - ceph-osd
    
#
# OSD internal
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-INTERNAL
  hostname-suffix: osdi
  
  component-endpoints:
    - ceph-osd-internal</codeblock>
    </section>

    <section id="ceph3_networks"><title>networks.yml</title>
      <p>Define the OSD VLAN in the <codeph>~/helion/my_cloud/definition/data/networks.yml</codeph>
        file.</p>
      <p>The example below defines two separate network VLANs:</p>
      <codeblock>
- name: OSD-CLIENT-NET
  vlanid: 112
  tagged-vlan: true
  cidr: 192.168.187.0/24
  gateway-ip: 192.168.187.1
  network-group: OSD-CLIENT

- name: OSD-INTERNAL-NET
  vlanid: 116
  tagged-vlan: true
  cidr: 192.168.200.0/24
  gateway-ip: 192.168.200.1
  network-group: OSD-INTERNAL</codeblock>
    </section>

    <section id="ceph3_servergroups"><title>server_groups.yml</title>
      <p>Add the OSD network to the server groups in the
          <codeph>~/helion/my_cloud/definition/data/server_groups.yml</codeph> file, indicated by
        the bold portion below:</p>
      <codeblock>
- name: CLOUD
  server-groups:
   - AZ1
   - AZ2
   - AZ3
  networks:
   - EXTERNAL-API-NET
   - EXTERNAL-VM-NET
   - GUEST-NET
   - MANAGEMENT-NET
   <b>- OSD-CLIENT-NET</b>
   <b>- OSD-INTERNAL-NET</b></codeblock>
    </section>

    <section id="ceph3_firewallrules"><title>firewall_rules.yml</title>
      <p>Modify the firewall rules in the
          <codeph>~/helion/my_cloud/definition/data/firewall_rules.yml</codeph> file to allow OSD
        nodes to be pingable via the OSD network, indicated by the bold portion below:</p>
      <codeblock>
- name: PING
  network-groups:
  - MANAGEMENT
  - GUEST
  - EXTERNAL-API
  <b>- OSD-CLIENT
  - OSD-INTERNAL</b>
  rules:
  # open ICMP echo request (ping)
  - type: allow
    remote-ip-prefix:  0.0.0.0/0
    # icmp type
    port-range-min: 8
    # icmp code
    port-range-max: 0
    protocol: icmp</codeblock>
    </section>

    <section id="ceph3_readme"><title>Edit the README.html and README.md Files</title>
      <p>You can edit the <codeph>~/helion/my_cloud/definition/README.html</codeph> and
          <codeph>~/helion/my_cloud/definition/README.md</codeph> files to reflect the OSD network
        group information if you wish. This change does not have any semantic implication and only
        assists with the readability of your model.</p>
    </section>
  </body>
</topic>
