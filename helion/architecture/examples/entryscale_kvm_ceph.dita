<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="entryscale_kvm_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-Scale KVM with Ceph Model</title>
    <body><!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        

      <p>This example provides a KVM-based cloud using Ceph for both block and object storage.</p>
      <ul>
        <li><b>Management traffic</b> - primarily includes all admin related operations such as pool
          creation, crush map modification, user creation, etc.</li>
        <li><b>Client (data) traffic</b> - primarily includes client requests sent to OSD
          daemons.</li>
        <li><b>Cluster (replication) traffic</b> - primarily includes replication and recovery data
          traffic among OSD daemons.</li>
      </ul>
      <p>The table below lists out the key characteristics needed per server role for this
        configuration.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_jnw_yss_s5">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>Server role</entry>
              <entry>Quantity</entry>
              <entry>Compute Requirement</entry>
              <entry>Network Requirement</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Controller</entry>
              <entry>3</entry>
              <entry>2x 10 core 2.66 GHz<p>96 - 128 GB RAM</p></entry>
              <entry>2x 10Gb Dual Port NIC</entry>
            </row>
            <row>
              <entry>Compute (KVM hypervisor)</entry>
              <entry>1 (minimum)</entry>
              <entry>2x 12 core 2.66 GHz (ES-2690v3) Intel Xeon<p>256 GB RAM</p></entry>
              <entry>1x 10Gb Dual Port NIC</entry>
            </row>
            <row>
              <entry>OSD</entry>
              <entry>3 (minimum)</entry>
              <entry>RAM is dependent upon the number of disks. 1 GB per TB of disk capacity is
                recommended.</entry>
              <entry>2x 10Gb Dual Port NIC</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>This diagram below illustrates the physical networking used in this configuration.</p>
      <p><image href="../../../media/hos.docs/entry_scale_kvm_ceph_three_network.png"/></p>
      <p><xref href="../../../media/hos.docs/entry_scale_kvm_ceph_three_network_lg.png" scope="external"
          format="html">Download a high-resolution version</xref></p>
      <p>This configuration is based on the <codeph>entry-scale-kvm-ceph</codeph> cloud input model
      which is included with the HPE Helion OpenStack distro. You will need to make the changes
      outlined below prior to the deployment of your Ceph cluster.</p>
        
    <section id="ceph3_nicmappings"><title>nic_mappings.yml</title>
      <p>Ensure that your baremetal server NIC interfaces are correctly specified in the
          <codeph>~/helion/my_cloud/definition/data/nic_mappings.yml</codeph> file and that they
        meet the server requirements.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>nic-mappings:

## NIC specification for controller nodes.  A bonded interface is used for the management 
## network while a separate interface is used to connect to the Ceph nodes.
  - name: DL360p_4PORT
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
       - logical-name: hed2
         type: simple-port
         bus-address: "0000:08:00.0"
         
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:09:00.0"
       
       - logical-name: hed4
         type: simple-port
         bus-address: "0000:0a:00.0"
         
## NIC specification for compute nodes. One interface is used for the management 
## network while the second interface is used to connect to the Ceph nodes.
  - name: MY-2PORT-SERVER
    physical-ports:
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:04:00.0"

       - logical-name: hed4
         type: simple-port
         bus-address: "0000:04:00.1"

## NIC specification for OSD nodes. The first interface is used for management network
## traffic. The second interface is used for client or public traffic. The third
## interface is used for internal OSD traffic.
  - name: MY-4PORT-SERVER
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:06:00.0"

       - logical-name: hed2
         type: simple-port
         bus-address: "0000:06:00.1"

       - logical-name: hed3
         type: simple-port
         bus-address: "0000:06:00.2"

       - logical-name: hed4
         type: simple-port
         bus-address: "0000:06:00.3"</codeblock>
    </section>
        
    <section id="ceph3_servers"><title>servers.yml</title>
      <p>Ensure that your servers in the
          <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file are mapped to the
        correct NIC interface.</p>
      <p>An example with the bolded line for <codeph>nic-mapping</codeph> illustrating this:</p>
      <codeblock># Controller Nodes
  - id: controller1
    ip-addr: 10.13.111.138
    server-group: RACK1
    role: CONTROLLER-ROLE
    <b>nic-mapping: HP-DL360-4PORT</b>
    mac-addr: "f0:92:1c:05:69:10"
    ilo-ip: 10.12.8.214
    ilo-password: password
    ilo-user: admin
    
# Compute Nodes
  - id: compute1
    ip-addr: 10.13.111.139
    server-group: RACK1
    role: COMPUTE-ROLE
    <b>nic-mapping: MY-2PORT-SERVER</b>
    mac-addr: "83:92:1c:55:69:b0"
    ilo-ip: 10.12.8.215
    ilo-password: password
    ilo-user: admin
        
# OSD Nodes
  - id: osd1
    ip-addr: 10.13.111.140
    server-group: RACK1
    role: OSD-ROLE
    <b>nic-mapping: MY-4PORT-SERVER</b>
    mac-addr: "d9:92:1c:25:69:e0"
    ilo-ip: 10.12.8.216
    ilo-password: password
    ilo-user: admin</codeblock>
    </section>
        
    <section id="ceph3_netinterfaces"><title>net_interfaces.yml</title>
      <p>Define a new interface set for your OSD interfaces in the
          <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>
- name: CONTROLLER-INTERFACES
  network-interfaces:
## This bonded interface is used by the controller 
## nodes for cloud management traffic.
    - name: BOND0
      device:
         name: bond0
      bond-data:
         options:
            mode: active-backup
            miimon: 200
            primary: hed1
         provider: linux
         devices:
            - name: hed1
            - name: hed2
        network-groups:
        - EXTERNAL-API
        - EXTERNAL-VM
        - GUEST
        - MANAGEMENT
## This interface is used to connect the controller
## node to the Ceph nodes so that any Ceph client
## like cinder-volume can route data directly to
## Ceph over this interface.
    - name: ETH2
      device:
        name: hed3
      network-groups:
        - OSD-CLIENT
        
- name: COMPUTE-INTERFACES
  network-interfaces:
    - name: HETH3
      device:
         name: hed3
      network-groups:
         - EXTERNAL-VM
         - GUEST
         - MANAGEMENT
## This interface is used to connect the compute node
## to the Ceph cluster so that a workload VM can route
## data traffic to the Ceph cluster over this interface.
    - name: HETH4
       device:
          name: hed4
       network-groups:
          - OSD-CLIENT
        
- name: OSD-INTERFACES
  network-interfaces:
## This defines the interface used for management 
## traffic like logging, monitoring, etc.
    - name: HETH1
      device:
          name: hed1
      network-groups:
        - MANAGEMENT
## This defines the interface used for client
## or data traffic.
    - name: HETH2
      device:
          name: hed2
      network-groups:
        - OSD-CLIENT
## This defines the interface used for internal
## cluster communication among OSD nodes.
    - name: HETH3
      device:
          name: hed3
      network-groups:
        - OSD-INTERNAL</codeblock>
    </section>
        
    <section id="ceph3_networkgroups"><title>network_groups.yml</title>
      <p>Define the OSD network group in the
          <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file:</p>
      <codeblock>#
# OSD client
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-CLIENT
  hostname-suffix: osdc
  
  component-endpoints
    - ceph-monitor
    - ceph-osd
    
#
# OSD internal
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-INTERNAL
  hostname-suffix: osdi
  
  component-endpoints:
    - ceph-osd-internal</codeblock>
    </section>
        
    <section id="ceph3_networks"><title>networks.yml</title>
      <p>Define the OSD VLAN in the <codeph>~/helion/my_cloud/definition/data/networks.yml</codeph>
        file.</p>
      <p>The example below defines two separate network VLANs:</p>
      <codeblock>
- name: OSD-CLIENT-NET
  vlanid: 112
  tagged-vlan: true
  cidr: 192.168.187.0/24
  gateway-ip: 192.168.187.1
  network-group: OSD-CLIENT

- name: OSD-INTERNAL-NET
  vlanid: 116
  tagged-vlan: true
  cidr: 192.168.200.0/24
  gateway-ip: 192.168.200.1
  network-group: OSD-INTERNAL</codeblock>
    </section>
        
    <section id="ceph3_servergroups"><title>server_groups.yml</title>
      <p>Add the OSD network to the server groups in the
          <codeph>~/helion/my_cloud/definition/data/server_groups.yml</codeph> file, indicated by
        the bold portion below:</p>
      <codeblock>
- name: CLOUD
  server-groups:
   - AZ1
   - AZ2
   - AZ3
  networks:
   - EXTERNAL-API-NET
   - EXTERNAL-VM-NET
   - GUEST-NET
   - MANAGEMENT-NET
   <b>- OSD-CLIENT-NET</b>
   <b>- OSD-INTERNAL-NET</b></codeblock>
    </section>
        
    <section id="ceph3_firewallrules"><title>firewall_rules.yml</title>
      <p>Modify the firewall rules in the
          <codeph>~/helion/my_cloud/definition/data/firewall_rules.yml</codeph> file to allow OSD
        nodes to be pingable via the OSD network, indicated by the bold portion below:</p>
      <codeblock>
- name: PING
  network-groups:
  - MANAGEMENT
  - GUEST
  - EXTERNAL-API
  <b>- OSD-CLIENT
  - OSD-INTERNAL</b>
  rules:
  # open ICMP echo request (ping)
  - type: allow
    remote-ip-prefix:  0.0.0.0/0
    # icmp type
    port-range-min: 8
    # icmp code
    port-range-max: 0
    protocol: icmp</codeblock>
    </section>
        
    <section id="ceph3_readme"><title>Edit the README.html and README.md Files</title>
      <p>You can edit the <codeph>~/helion/my_cloud/definition/README.html</codeph> and
          <codeph>~/helion/my_cloud/definition/README.md</codeph> files to reflect the OSD network
        group information if you wish. This change does not have any semantic implication and only
        assists with the readability of your model.</p>
    </section>
    </body>
</topic>
