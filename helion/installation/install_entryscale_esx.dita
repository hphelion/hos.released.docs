<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_esx">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for Helion Entry-scale Cloud with
    ESX</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/installation/install_entryscale_esx.html"
        scope="external" format="html">Click here</xref> for a frameless version of this page where
      the links should work.</note>
    <p>This document describes the procedure for the deployment of an ESX cloud using input model
      and adding more ESX hosts to an already activated cluster.</p>
    <p> It contains the following topics:<ul id="ul_wrs_dwq_lt">
        <li><xref href="#install_esx/prereqs" format="dita">Prerequisites</xref></li>
        <li><xref href="#install_esx/deployCloud" format="dita">Deploy ESX Cloud</xref></li>
        <li><xref href="#install_esx/prepAndDeploy" format="dita">Prepare and Deploy ESX Computes
            and OVSvAPPs</xref></li>
      </ul></p>
    <section>
      <note type="important">Before you start your ESX cloud deployment ensure that you read the
        following instructions carefully.</note>
    </section>
    <section conref="install_entryscale_kvm.dita#install_kvm/important_notes"/>
    <section conref="install_entryscale_kvm.dita#install_kvm/prereqs"/>
    <section id="prereqs">
      <title>Prerequisite</title>
      <p>ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>
      <p>
        <ul id="ul_shv_s4b_2t">
          <li>The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul
              id="ul_py3_j5l_ft">
              <li>Installing and configuring vCenter Server</li>
              <li>Installing and configuring ESX server and ESX cluster</li>
              <li>Installing and configuring shared datastores</li>
              <li>Establishing network connectivity between the ESX network and the HPE Helion
                management network</li>
            </ul></li>
          <li>The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>
          <li>Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>
          <li>The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>
          <li>Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>
          <li>Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps.</li>
          <li>It is recommended to have a common shared storage for all the ESXi hosts in a
            particular cluster. </li>
          <li>Ensure that you have enabled HA (High Availability) and DRS (Distributed Resource
            Scheduler) settings in a cluster configuration before running the installation. DRS/HA
            is disabled only for OVSvApp. This is done so that it does not move to a different host.
            If you do not enable DRS/HA prior to installation then you will not be able to disable
            it only for OVSvApp. As a result DRS/HA can migrate OVSvApp to different host, which
            will create a network loop.<p>
              <note>No two clusters should have the same name across datacenters in a given
                vCenter.</note>
            </p></li>
        </ul>
      </p>
    </section>
    <section id="deployCloud"><b>Deploy ESX Cloud</b><p>At a high level, here are the steps to
        configure and deploy ESX cloud:</p><p><image href="../../media/esx/esx_deploy.jpg"
          id="image_kjt_zlm_ft"/></p></section>
    <section>
      <title>Procedure to Deploy ESX cloud</title>
    </section>
    <p>The following topics in this section explain how to deploy ESX cloud.</p>
    <section conref="install_entryscale_kvm.dita#install_kvm/setup_deployer"/>
    <section id="Configure">
      <title>Prepare and Deploy Cloud Controllers</title>
      <ol id="ol_c1w_pfl_ft">
        <li>See a sample set of configuration files in the
            <codeph>~/helion/examples/entry-scale-esx</codeph> directory. The accompanying README.md
          file explains the contents of each of the configuration files. </li>
        <li>Copy the example configuration files into the required setup directory and edit them as
          required:
          <codeblock>cp -r ~/helion/examples/entry-scale-esx/* ~/helion/my_cloud/definition/</codeblock></li>
        <!--The configuration files for editing are available at <codeph>~/helion/my_cloud/definition/</codeph>. Refer to the <b><xref href="input_model.dita">Helion OpenStack 2.0 Input Model</xref></b> document for assistance with the configuration files. <note type="important">If you chose to use your first controller node as your deployer, ensure that your <codeph>servers.yml</codeph> file contains the <codeph>is-deployer: true</codeph> notation in your controller options. If you are using a dedicated deployer node you can omit this. Here is an example snippet of a <codeph>servers.yml</codeph> file where a user is using their first controller node as their deployer: <codeblock># Controllers
- id: controller1
  ip-addr: 192.168.10.3
  role: CONTROLLER-ROLE
  server-group: RACK1
  nic-mapping: HP-DL360-4PORT
  mac-addr: b2:72:8d:ac:7c:6f
  ilo-ip: 192.168.9.3
  ilo-password: password
  ilo-user: admin
  <b>is_deployer: true</b></codeblock></note>-->
        <li>Modify <codeph>~/helion/hos/ansible/hlm-deploy.yml</codeph> to uncomment the line
          containing <codeph>eon-deploy.yml</codeph>. You must comment the line containing
            <codeph>ceph-deploy.yml</codeph>, <codeph>vsa-deploy.yml</codeph>, and
            <codeph>cmc-deploy.yml</codeph>.
            <p><!--<li>Modify the <codeph>neutron.conf.j2</codeph> at <codeph>~/helion/my_cloud/config/<codeph>neutron/neutron.conf.j2</codeph></codeph>with the following values:<codeblock>router_distributed = False</codeblock><p> And add <codeph>mechanism_drivers = ovsvapp</codeph> at the end of the <codeph>neutron.conf.j2</codeph>.</p></li><li>Modify <codeph>ml2_conf.ini.j2</codeph> at <codeph>~/helion/my_cloud/config/<codeph>neutron/ml2_conf.ini.j2</codeph></codeph>with the following values:<codeblock>[ml2]
mechanism_drivers = ovsvapp, openvswitch, l2population
[agent]
enable_distributed_routing = False</codeblock></li>--></p></li>
        <li id="optional-eon">(<b>Optional</b>) To enable centralized logging for EON service, do
          the following:<ol id="ol_ykv_czv_st">
            <li>Change your directory to
              <codeblock>~/helion/my_cloud/config/logging/vars</codeblock></li>
            <li>Edit <codeph>eon-api-clr.yml</codeph> file to set the value of
                <b>centralized_logging</b> to <b>true</b> as shown in the following
              sample:<codeblock> - centralized_logging:
        enabled: true
        format: rawjson
      files:
      - /var/log/eon/eon-api-json.log
      log_rotate:
      - daily
      - compress
      - missingok
      - notifempty
      - copytruncate
      - maxsize 45M
      - rotate 5
      - create 640 eon eon </codeblock></li>
            <li>Edit <codeph>eon-conductor-clr.yml</codeph> file to set the value of
                <b>centralized_logging</b> to <b>true</b> as shown in the following
              sample:<codeblock> - centralized_logging:
        enabled: true
        format: rawjson
      files:
      - /var/log/eon/eon-conductor-json.log
      log_rotate:
      - daily
      - compress
      - missingok
      - notifempty
      - copytruncate
      - maxsize 45M
      - rotate 5
      - create 640 eon eon </codeblock></li>
          </ol></li>
        <li>Commit your cloud deploy configuration to the<xref href="using_git.dita"> local git
            repo</xref>, as follows: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock>
          <note>This step needs to be repeated any time you make changes to your configuration files
            before you move onto the following steps. See <xref href="using_git.dita">Using Git for
              Configuration Management</xref> for more information.</note></li>
      </ol>
    </section>
    <p>Then you need to run the following commands to complete your configuration. These commands
      also verify your configuration is correct.</p>
    <ol id="ol_og4_jy4_st">
      <li>Run the following playbook which confirms that there is iLo connectivity for each of your
        nodes so that they are accessible to be re-imaged in a later step:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</codeblock></li>
      <li>Run the configuration processor, as follows:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
    </ol>
    <p>If you receive an error during either of these steps then there is an issue with one or more
      of your configuration files. We recommend that you verify that all of the information in each
      of your configuration files is correct for your environment and then commit those changes to
      git using the instructions above.</p>

    <section conref="install_entryscale_kvm.dita#install_kvm/provision"/>
    <section conref="install_entryscale_kvm.dita#install_kvm/deploy"/>
    <section id="prepAndDeploy">
      <title>Prepare and Deploy ESX Computes and OVSvAPPs </title>
      <p>The following sections describe the procedure to install and configure ESX compute and
        OVSvAPPs on vCenter.</p>
      <ul id="ul_lns_fjl_ft">
        <li><xref href="#install_esx/deploy_template" format="dita">Deploy Helion Linux Shell VM
            Template</xref></li>
        <li><xref href="#install_esx/prepare_esx_cloud_deployment" format="dita">Preparation for ESX
            Cloud Deployment</xref></li>
      </ul>
    </section>
    <section id="deploy_template"><b>Deploy Helion Linux Shell VM Template</b><p>The first step in
        deploying the ESX compute proxy and OVSvAPPs is to create a VM template that will make it
        easier to deploy the ESX compute proxy for each Cluster and OVSvAPPs on each ESX server.
        </p><p>Perform the following steps to deploy a template:<ol id="ol_qdt_ljs_ft">
          <li>Import the <codeph>hlm-shell-vm.ova</codeph> in the vCenter using the vSphere client.
            The <codeph>hlm-shell-vm.ova</codeph> template is present in the following
            location:<codeblock>location /media/cdrom/hos-2.0.0/ hlm-shell-vm.ova</codeblock></li>
          <li>In the vSphere Client, click <b>File</b> and then click <b>Deploy OVF
            Template</b></li>
          <li>Follow the instructions in the wizard to specify the data center, cluster, and node to
            install. Refer to the VMWare vSphere documentation as needed.</li>
        </ol></p></section>
    <section id="prepare_esx_cloud_deployment"><b>Preparation for ESX Cloud Deployment</b><p>This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. </p><p>
        <ol id="ol_xpc_zqs_ft">
          <li>Login to the lifecycle manager.</li>
          <li>Source <codeph>service.osrc</codeph>.</li>
          <li><xref href="#install_esx/register-vcenter" format="dita">Register a vCenter
              Server</xref></li>
          <li><xref href="#install_esx/register-network" format="dita">Register ESX Cloud Network
              Configuration</xref></li>
          <li><xref href="#install_esx/import-cluster" format="dita">Import Clusters</xref></li>
          <li><xref href="#install_esx/activate-cluster" format="dita">Activate Clusters</xref><ol
              id="ol_isb_wcc_vt">
              <li><xref href="#install_esx/modify-volume-config" format="dita">Modify the Volume
                  Configuration File</xref></li>
              <li><xref href="#install_esx/commit-your-cloud" format="dita">Commit your Cloud
                  Definition</xref></li>
              <li><xref href="#install_esx/deploy-compute-proxy-ovsvapps" format="dita">Deploy ESX
                  Compute Proxy and OVSvApps</xref></li>
            </ol></li>
        </ol>
      </p></section>
    <p><b>Manage vCenters and Clusters</b></p>
    <p>The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>
    <p id="register-vcenter"><b>Register a vCenter Server</b></p>
    <p>vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol id="ol_xdj_5js_ft">
        <li>Add a vCenter using EON python
            client.<codeblock><codeph># eon vcenter-add --name &lt;vCenter Name> --ip-address &lt;vCenter IP address> --username &lt;vCenter Username> --password &lt;vCenter Password> --port &lt;vCenter Port></codeph></codeblock><p>where:
              <ul id="ul_bp3_yjs_ft">
              <li>vCenter Name - the identical name of the vCenter server.</li>
              <li>vCenter IP address - the IP address of the vCenter server.</li>
              <li>vCenter Username - the admin privilege username for the vCenter.</li>
              <li>vCenter Password - the password for the above username.</li>
              <li>vCenter Port - the vCenter server port. By default it is 443. <note
                  type="important">Please do not change the vCenter Port unless you are certain it
                  is required to do so.</note></li>
            </ul></p><p><b>Sample
            Output:</b><codeblock><codeph># eon vcenter-add --name vc01 --ip-address 10.1.200.41 --username administrator@vsphere.local --password password --port 443</codeph>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
      </ol></p>
    <section><b>Show vCenter</b><ol id="ol_wh5_rsb_lt">
        <li>Show vCenter using EON python
              client.<codeblock># eon vcenter-show &lt;vCenter ID></codeblock><p><b>Sample
              Output:</b><codeblock><codeph># eon vcenter-show BC9DED4E-1639-481D-B190-2B54A2BF5674 </codeph>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| datacenters| DC1                                  |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
      </ol></section>
    <section id="register-network"><b>Register ESX Cloud Network Configuration</b>
      <p>This involves getting a sample network information template. Fill the details of the
        template and use that template to register the cloud network configuration for the
        vCenter.</p>
      <p>
        <ol>
          <li>Execute the following command to get the network information
              template:<codeblock><codeph># eon get-network-info-template --filename &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For
              example:<codeblock># eon get-network-info-template --filename net_conf.json</codeblock></p><p>Sample
              file of <codeph>net_conf.json</codeph> is shown
              below:<codeblock>{
    "network": {
        # Deployer Network details
        # This network should be reachable from the Deployer node
        "deployer_network": {
            #Deployer Portgroup Name. Will create if not exist.
            "deployer_pg_name": "CONF-PG",

            #Single VLAN id for CONF/Deployer network
            "deployer_vlan": "33",

            #VLAN type. Accepted values are "vlan", "trunk", "none"
            "vlan_type": "trunk",

            #DHCP setting for deployer network
            "enable_deployer_dhcp": "false",

            #CIDR and gateway for deployer network only when "enable_deployer_dhcp" is "no"
            "deployer_cidr": "10.20.18.0/23",

            # IP range from the CIDR. Will use whole pool from CIDR if ip_range is not provided.
	    "ip_range": {
	        "start_address": "10.20.18.50",
		"end_address": "10.20.18.90"
	    },

            "deployer_gateway_ip": "10.20.18.1",

            #Deployer Node's PXE IP Address
            "deployer_node_ip": "10.20.16.2"

            # NIC Teaming Configurations

            # We support only single DVS for Mgmt and Deployer.
            # The management_network NIC Teaming configs, will be applied to deployer_network as well.
        },

        #Management Network details
        "management_network": {
            #Mgmt DVS name. Will create if not exist.
            "mgmt_dvs_name": "MGMT-DVS",

            #Physical NIC name(s) for Mgmt DVS. Physical NICs(s) must be unassigned to other uplinks.
            "mgmt_nic_name": "vmnic3",

            #Mgmt Portgroup Name. Will create if not exist.
            "mgmt_pg_name": "MGMT-PG",

            #VLAN id/range for MGMT network (and GUEST network if GUEST is carried over MGMT)
            "mgmt_vlan": "0,1-4094",

            #VLAN type. Accepted values are "vlan", "trunk", "none"
            "vlan_type": "trunk",

            #Interface order: Example eth1
            "mgmt_interface_order": "eth1",

            # Make sure you have exact same NIC teaming configurations for both deployer_network and management_network.

            # NIC Teaming Configurations

            #Provided physical NIC name(s) will be configured as active uplink(s) from mgmt_nic_name.
            #If not provided then the first entry in mgmt_nic_name will be configured as active and
            #the remaining as standby.
            "active_nics": "",

            #Load Balancing. Please choose the corresponding number

            # 1 -&gt; Route based on the originating virtual port
            #(Choose an uplink based on the virtual port where the traffic entered the virtual switch)

            # 2 -&gt; Route based on IP hash
            #(Choose an uplink based on a hash of the source and destination IP addresses of each packet.
            #For non-IP packets, whatever is at those offsets is used to compute the hash)

            # 3 -&gt; Route based on source MAC hash (Choose an uplink based on a hash of the source Ethernet.)

            # 4 -&gt; Route based on physical NIC load (Choose an uplink based on the current loads of physical NICs)

            # 5 -&gt; Use explicit failover order (Always use the highest order uplink, from the list of Active adapters,
            #which passes failover detection criteria)

            "load_balancing": "1",

            #Network Failover Detection. Please choose the corresponding number

            # 1 -&gt; Link Status. (Relies solely on the link status that the network adapter provides. This option detects failures,
            #such as cable pulls and physical switch power failures, but not configuration errors, such as a physical switch port
            #being blocked by spanning tree or that is misconfigured to the wrong VLAN or cable pulls on the other side of a physical switch.

            # 2-&gt; Beacon Probing. (Sends out and listens for beacon probes on all NICs in the team and uses this information, in addition to
            #link status, to determine link failure. This detects many of the failures previously mentioned that are not detected by link status alone.

            "network_failover_detection": "1",

            #Notify Switches(yes/no).

            #If you select Yes, whenever a virtual NIC is connected to the Switch or whenever that virtual NIC?s traffic would be routed over
            #a different physical NIC in the team because of a failover event, a notification is sent out over the network to update the lookup
            #tables on physical switches. In almost all cases, this process is desirable for the lowest latency of failover occurrences and
            #migrations with vMotion.

            "notify_switches": "yes"
        },

        #Data network details. Uncomment if you want to use data_network.
        #"data_network": {
            #Data/Uplink DVS name. Will create if not exist.
        #    "data_dvs_name": "GUEST-DVS",

            #Physical NIC name(s) for Data/Uplink DVS. Physical NICs(s) must be unassigned to other uplinks.
        #    "data_nic_name": "vmnic2, vmnic1",

            #Data Portgroup Name. Will create if not exist.
        #    "data_pg_name": "GUEST-PG",

            #VLAN id for Data/GUEST network
        #    "data_vlan": "50, 60-70, 1034-200",

            #VLAN type. Accepted values are "vlan", "trunk", "none"
        #    "vlan_type": "trunk",

            #Interface order: Example eth3
        #    "data_interface_order": "eth3",
            # NIC Teaming Configurations

            #Provided physical NIC name(s) will be configured as active uplink(s) from data_nic_name.
            #If not provided then the first entry in data_nic_name will be configured as active and the remaining as standby.

        #    "active_nics": "vmnic2",

            #Load Balancing. Please choose the corresponding number

            # 1 -&gt; Route based on the originating virtual port
            #(Choose an uplink based on the virtual port where the traffic entered the virtual switch)

            # 2 -&gt; Route based on IP hash
            #(Choose an uplink based on a hash of the source and destination IP addresses of each packet.
            #For non-IP packets, whatever is at those offsets is used to compute the hash)

            # 3 -&gt; Route based on source MAC hash (Choose an uplink based on a hash of the source Ethernet.)

            # 4 -&gt; Route based on physical NIC load (Choose an uplink based on the current loads of physical NICs)

            # 5 -&gt; Use explicit failover order (Always use the highest order uplink, from the list of Active adapters,
            #which passes failover detection criteria)

        #    "load_balancing": "1",

            #Network Failover Detection. Please choose the corresponding number

            # 1 -&gt; Link Status. (Relies solely on the link status that the network adapter provides. This option detects failures,
            #such as cable pulls and physical switch power failures, but not configuration errors, such as a physical switch port
            #being blocked by spanning tree or that is misconfigured to the wrong VLAN or cable pulls on the other side of a physical switch.

            # 2-&gt; Beacon Probing. (Sends out and listens for beacon probes on all NICs in the team and uses this information, in addition to
            #link status, to determine link failure. This detects many of the failures previously mentioned that are not detected by link status alone.

        #    "network_failover_detection": "1",

            #Notify Switches(yes/no).

            #If you select Yes, whenever a virtual NIC is connected to the Switch or whenever that virtual NIC?s traffic would be routed over
            #a different physical NIC in the team because of a failover event, a notification is sent out over the network to update the lookup
            #tables on physical switches. In almost all cases, this process is desirable for the lowest latency of failover occurrences and
            #migrations with vMotion.

        #    "notify_switches": "yes"
        #},

        # Trunk Network details
        "trunk_network": {
            #Trunk DVS name. Will create if not exist.
            "trunk_dvs_name": "TRUNK-DVS",

            #Trunk portgroup name. Will create if not exist.
            "trunk_pg_name": "TRUNK-PG",

            #Interface order: Example eth2
            "trunk_interface_order": "eth2"
        },
 
       #Tenant network type
        "tenant_network_type": "vlan",

        #VLAN Range for trunk network. It's applicable for both VXLAN &amp; VLAN deployment
        #Please don't edit this field and value. Changing this value will have no effect !
        #This field in future will be removed and is available for backward compatibility.
        #The value must be 1-4094.

        "vlan_range": "1-4094"
    },

    "template": {
        #Provide the template/appliance name that will be used for cloning Computeproxy and OVSvApp VMs
        "template_name": "hlm-shell-vm"
    },

    "vmconfig": {
        #Number of CPUs for OVSvApp/Computeproxy VM
        "cpu": "4",

        #Amount of RAM for OVSvApp/Computeproxy VM(in Mega Byte)
        "memory_in_mb": "4096",

        #SSH public key content for OVSvAPP/Computeproxy password less login.
        #Carefully copy the public key and paste it within the double quotes.
        "ssh_key": "&lt;deployer-ssh-pub-key-contents&gt;"
    }
}</codeblock></p></li>
          <li>Modify the template (json file) as per your
              environment.<codeblock>vi &lt;<b>NETWORK_CONF_FILENAME</b>></codeblock><p>For
              example:<codeblock>vi net_conf.json</codeblock></p></li>
          <li>Use the template to register Cloud Network Configuration. This sets the network
            information for a vCenter which is used to deploy and configure compute proxy and
            OVSvAPP VMs during the cluster activation.
              <codeblock><codeph># eon set-network-info --vcenter-id &lt;vCenter ID> --datacenter-name &lt;datacenter name> --config-json &lt;NETWORK_CONF_FILENAME></codeph></codeblock><p>For
              example:
              <codeblock># eon set-network-info --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --datacenter DC1 --config-json net_conf.json</codeblock></p><p>
              <note>The vcenter ID is generated when you register a vCenter.</note>
            </p></li>
          <li>Execute the following command to view the list of clusters for the given
              vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
              Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c21 | Cluster1 | DC1        | not_imported  |
+------------+----------+------------+---------------+</codeblock></li>
        </ol>
      </p></section>
    <section id="import-cluster"><b>Import Cluster </b><p>You can use one or more ESX clusters for
        ESX Cloud Deployment. When an Import Cluster is invoked, required ESX Compute Proxy and
        OVSvApp nodes are deployed.</p><p>vCenter can have multiple clusters, but the current
        release of <keyword keyref="kw-hos"/> supports only sequential import of clusters
        (one-by-one). Therefore, you can import only one cluster at a time using the <codeph>eon
          cluster-import</codeph> command.</p><p>
        <ol>
          <li> Import the cluster for the EON database under the given vCenter. <codeblock><codeph># eon cluster-import --vcenter-id &lt;vCenter ID> --cluster-name &lt;Cluster Name> --cluster-moid &lt;Cluster Moid></codeph></codeblock>where:<p>
              <ul id="ul_r4g_rjs_ft">
                <li>vCenter ID - ID of the vcenter containing the cluster.</li>
                <li>Cluster Name - the name of the cluster that needs to be imported.</li>
                <li>Cluster Moid - Moid of the cluster that needs to be imported.</li>
              </ul>
            </p><p><b>Sample
              Output</b><codeblock># eon cluster-import --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-name Cluster1 --cluster-moid domain-c21
+--------------+---------------------------+
| Property     | Value                     |
+--------------+---------------------------+
| cpu_free     | 83071.73                  |
| cpu_total    | 83072                     |
| cpu_used     | 0.27                      |
| datacenter   | DC1                       |
| disk_free    | 1022.79                   |
| disk_total   | 1023.75                   |
| errors       | []                        |
| memory_free  | 496.82                    |
| memory_total | 511.76                    |
| memory_used  | 14.94                     |
| name         | Cluster1                  |
| state        | importing                 |
| switches     | ['TRUNK-DVS', 'MGMT-DVS'] |
+--------------+---------------------------+</codeblock></p><p>One
              vCenter can have multiple clusters. But it allows you to import only one cluster at a
              time.</p></li>
          <li> Execute the following command to view the list of clusters for the given
              vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
              Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c22 | Cluster2 | DC1        | imported      |
+------------+----------+------------+---------------+</codeblock></li>
        </ol>
      </p></section>
    <section id="activate-cluster"><b>Activate Clusters</b><note>You can activate the cluster only
        after the import status of the cluster is changed to <b>imported</b>.</note>
      <p>When you execute the active cluster command, the <codeph>server.yml</codeph> of the input
        model is updated with IP Addresses of compute proxy and OVSvApp.
        <!--VMs correspesponding to the clusters are being activated.--></p>
      <p>
        <ol>
          <li>Activate the cluster for the selected
                vCenter.<codeblock><codeph># eon cluster-activate --vcenter-id &lt;vCenter ID> --cluster-moid &lt;Cluster Moid> </codeph></codeblock><p><b>Sample
                Output</b></p><p>
              <codeblock># eon cluster-activate --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-moid domain-c22 
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property      | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| node_info     | {u'computeproxy': {u'pxe-mac-addr': u'00:50:56:b6:ce:1b', u'pxe-ip-addr': u'172.170.2.4', u'name': u'COMPUTEPROXY_Cluster1', u'cluster-moid': u'domain-c21'}, u'network_driver': {u'cluster_dvs_mapping': u'DC1/host/Cluster1:hlm-Trunk', u'Cluster1': [{u'host-moid': u'host-29', u'pxe-ip-addr': u'172.170.2.3', u'esx_hostname': u'10.1.200.33', u'ovsvapp_node': u'ovsvapp-10-1-200-33', u'pxe-mac-addr': u'00:50:56:b6:5e:9a'}, {u'host-moid': u'host-25', u'pxe-ip-addr': u'172.170.2.2', u'esx_hostname': u'10.1.200.66', u'ovsvapp_node': u'ovsvapp-10-1-200-66', u'pxe-mac-addr': u'00:50:56:b6:56:e6'}]}} |
| resource_moid | domain-c21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| resource_name | Cluster1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| state         | activated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</codeblock>
            </p></li>
        </ol>
      </p></section>
    <section id="modify-volume-config"><b>Modify the Volume Configuration File</b>
      <p>Once the cluster is activated you must configure the volume.</p><p>Perform the following
        steps to modify the volume configuration files:</p><ol id="ol_bhx_n5p_st">
        <li>Change the directory. The <codeph>cinder.conf.j2</codeph> is present in following
          directories
            :<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock><p>OR<codeblock>cd /home/stack/helion/my_cloud/config/cinder</codeblock></p><p>It
            is recommended to modify the <codeph>cinder.conf.j2</codeph> present in
              <codeph>/home/stack/helion/my_cloud/config/cinder</codeph></p></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as follows:
          <codeblock># Configure the enabled backends
enabled_backends=&lt;unique-section-name>

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name = &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock></li>
      </ol></section>
    <section id="commit-your-cloud"><b>Commit your Cloud Definition</b>
      <p>
        <ol>
          <li> Add the cloud deployment definition to git
            :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations or other commit message';</codeblock></li>
          <li>Prepare your environment for deployment:
            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
        </ol>
      </p></section>
    <section id="deploy-compute-proxy-ovsvapps"><b>Deploy ESX Compute Proxy and OVSvApps</b>
      <p>Execute the following command to deploy an esx compute and
        OVSvApps:<codeblock>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*<b>esx-ovsvapp:*esx-compute</b>' 
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></p>
      <note>The variable <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the
          <b>name</b> key in the <codeph>resource-nodes</codeph> section in the
          <codeph>data/control_plane.yml</codeph> file
          (<codeph>/home/stack/helion/my_cloud/definition/data/control_plane.yml</codeph>).
      </note></section>
  </body>
</topic>
