<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="OctaviaInstall">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Octavia Installation</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="Overview">
      <title>Octavia Installation</title>
      <p>The <keyword keyref="kw-hos"/> Neutron LBaaS service supports several Load 
        Balancing providers. <keyword keyref="kw-hos"/> support both Octavia and the namespace 
        haproxy driver.  A user can specify which provider to use with the 
        <codeph>--provider</codeph> flag upon load balancer creation:
        <codeblock>neutron lbaas-loadbalancer-create --name &lt;name&gt; --provider [octavia|haproxy] &lt;subnet&gt;</codeblock>
      </p>
      <p>If you don't specify the provider it will automatically default to Octavia. The Octavia
        driver provides more functionality than the haproxy namespace driver which is deprecated.
        The haproxy namespace driver will be retied in a future version of <keyword keyref="kw-hos"
        />.</p>
      
      <p>There are additional drivers for 3rd party hardware load balancers. Please refer to the vendor directly. You can see a 
        list of available load balancing providers as follows:
        <codeblock>$ neutron service-provider-list
+----------------+----------+---------+
| service_type   | name     | default |
+----------------+----------+---------+
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
+----------------+----------+---------+</codeblock></p>
      <note>In the example above, the providers are listed twice. This is a limitation in <keyword
          keyref="kw-hos-phrase-30"/>. Also note that the Octavia load balancer provider is listed
        as the default.</note>

    </section>

  <section id="ExternalNetwork">
    <title>Create External Network</title>
      <p>You will need to create an external network and register an image to test LBaaS functionality.  If you have already created
        an external network and registered and image, this step can be skipped.</p>
      
      <ol>
        <li>You can run commands from the lifecycle manager or from a shell with access to the API nodes.
          Run the playbook which registers a Cirros image and creates a public network under
          Neutron. <note>Running this step configures 172.16.0.0/16 by default as the external
            network with name <i>ext-net</i> and registers a Cirros image.  You will need to
            change the IP for your external network.</note>
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock></li>
      </ol>
    
  </section>


  <section id="OctaviaProvider">
    <title>Octavia Load Balancing Provider</title>
    <p>The Octavia Load balancing provider bundled with <keyword keyref="kw-hos-phrase-30"/> is an
        operator grade load balancer for OpenStack. It is based on the Liberty version of Octavia.
        It differs from the namespace driver by starting a new nova virtual machine to house the
        haproxy performing the load balancing for each load balancer requested. A virtual machine
        for each load balancer requested provides a better separation of load balancers between
        tenants and makes it easier to grow load balancing capacity alongside compute node growth.
        Additionally, if the virtual machine fails for any reason Octavia will replace it with a new
        VM from a pool of spare VM's, assuming that the feature is configured.</p>
    
    <p><b>Prerequisites</b></p>
    <p>The Octavia Load Balancing Provider requires a provider network. This needs to be set up and
        configured in the hlm-input model prior to installing Octavia. Please refer to the
        corresponding Neutron document, <xref
          href="../networking/neutron_provider_networks.dita#neutron_provider_networks"/> for more
        information. 
      </p>
    
    <p><b>Installing the Amphora Image</b></p>
    <p>Octavia is utilizing Nova VMs for it's load balancing function and HPE provides images used to 
      boot those VM's called <codeph>octavia-amphora-haproxy</codeph>. 
      <note type="warning">Without these images the Octavia load balancer won't work.</note></p>
    <!-- Need download link and instructions to run it -->
      
    <p>You can download those images form the software depot. Once the image is downloaded 
      it needs to be placed on the lifecycle manager node and the image registered. </p>
    <ol>
      <li>Switch to the ansible directory and register the image by giving the full path and 
        name (e.g. <codeph>/tmp/octavia-amphora-haproxy-guest-image.tgz</codeph>) as 
        argument to service_package:
        <codeblock>$ cd ~/scratch/ansible/next/hos/ansible/
$ ansible-playbook -i hosts/verb_hosts -e service_package=&lt;image path/name&gt; service-guest-image.yml</codeblock></li>
      <li>Source the service user (this can be done on a different computer)
        <codeblock>$ . service.osrc</codeblock></li>
      <li>Verify if the image got uploaded and registered (this van be done on a computer 
        with access to the glance CLI client)
      <codeblock>$ glance image-list
+--------------------------------------+---------------------------------------+
| ID                                   | Name                                  |
+--------------------------------------+---------------------------------------+
|  01ff1f0d-fc35-4e3e-bae2-e7e2ee1f65b6 | cirros-0.3.3-x86_64                   |
| e64cb914-15d2-4ad8-a63c-b7c60a6c232e | octavia-amphora-x64-haproxy_hos-3.0.0 |
+--------------------------------------+---------------------------------------+
    </codeblock></li>
    </ol>
    <note>In rare circumstances the image won't be registered and you have to run the 
      whole registration again. Rest assured that if you run it by accident the system will only upload 
      a new image if the underlying image has changed.</note>
    <p>Please be aware that if you have already created load balancer they won;t receive the new image. 
      Only load balancers created after the image has been successfully created will use the new image. 
      If existing load balancers need to be switched to the new image please follow the instructions 
      in the admin guide.</p>
    <!-- Need link here to point to the Octavia admin guide when it's created. -->
 
    
    <p><b>Testing that the Octavia Load Balancer</b></p>

    <note>You should perform the following steps from a node that has a route to the private
      network. Using the examples from above, 10.1.0.0/24 should be reachable. </note>
    <ol>
      <li>SSH into both vm1 and vm2 in two separate windows and make them listen on your
        configured port. </li>
      <li>From one
        window.<codeblock>ssh cirros@&lt;ip address vm1>
          pass: &lt;password&gt;</codeblock></li>
      <li>From another window.
        <codeblock>ssh cirros@&lt;ip address vm2>
          pass: &lt;password&gt;</codeblock></li>
      <li>Start running web servers on both of the virtual machines.</li>
      <li>Create a webserv.sh script with below contents. In this example, the port is
        80.<codeblock>$ vi webserv.sh
          
#!/bin/bash
          
MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
    echo -e "HTTP/1.0 200 OK\r\n\r\nWelcome to $MYIP" | sudo nc -l -p 80
done
          
## Give it Exec rights
$ chmod 755 webserv.sh
          
## Start webserver
$ ./webserv.sh</codeblock></li>
      <li>Open a separate window. From the respective source node in external network (in case
        of accessing LBaaS VIP thorough its FIP) or in private network (in case of no FIP), add
        the respective IP address to the no_proxy env variable, if required. You can get the
        <i>VIP</i> from the <codeph>neutron lbaas-loadbalancer-list</codeph> for LBaaS v2 and
        <codeph>neutron lb-vip-list</codeph> for LBaaS v1.</li>
      <li>Run the following commands to test load balancing. In this example, the VIP IP address
        is 10.1.0.7 and when executing curl against the VIP, the responses are returned from the
        load balanced services.<codeblock>$ export no_proxy=$no_proxy,10.1.0.7
          
## Curl the VIP
$ curl -k 10.1.0.7
Welcome to 10.1.0.4
         
$ curl -k 10.1.0.7
Welcome to 10.1.0.5
          
$ curl -k 10.1.0.7
Welcome to 10.1.0.4</codeblock></li>
    </ol>
    
  </section>
    


  </body>
</topic>