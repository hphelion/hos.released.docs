<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="NSXv_config_vSphere">
  <title>Configure vSphere</title>
  <body><!--not tested-->
       
    <p>This installation is an example of a newly deployed environment with high availability enabled for the NSX-V edge pool services. In this example resource pools are used to organize resources and VMs on the control plane. However, VMware normally recommends you use folders in the <b>VMs and Templates</b> inventory if all you need to do is organize VMs.</p>
    
    <p>For best practices concerning NSX design, see the <xref href="https://communities.vmware.com/servlet/JiveServlet/downloadBody/27683-102-8-41631/NSX" scope="external" format="html">Reference Design: VMwareÂ® NSX for vSphere (NSX)Network Virtualization Design Guide</xref> (Clicking this link automatically downloads a PDF file instead of opening a Web page.)</p>
 
    <p>To configure the underlying vSphere environment, the Helion team used the following steps:</p>
    <ol>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_manager">Install NSX Manager</xref></li>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_controller">Add NSX Controllers</xref></li>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_preparec">Prepare Clusters for NSX Management</xref></li>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_VXTP">Configure VXLAN Transport Parameters</xref></li>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_segment">Assign Segment ID Pool</xref></li>
      <li><xref type="section" href="#NSXv_config_vSphere/NSXv_vs_transport">Create a Transport Zone</xref></li>
    </ol>
      
      <lines>
      </lines>
      
      <section id="NSXv_vs_manager">
        <title>Install NSX Manager</title>
        
       <p>The NSX Manager provides the graphical user interface and the REST APIs for creating, configuring, and monitoring NSX components, such as controllers, logical switches, and edge services gateways. The NSX Manager virtual machine is packaged as an Open Virtualization Appliance (OVA) file, which allows you to use the vSphere web client to import the NSX Manager into the datastore and virtual machine inventory.</p>
        
        <p>NSX installtion involves the deployment of several virtual appliances, and some configuration to allow communication across all of the physical and virtual devices. For more detailed information, refer to <xref href="https://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.install.doc/GUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" scope="external" format="html">VMware's NSX Installation Guide.</xref></p>
        
        <note type="important">To make the appliance installation easier, download the OVA file to the machine where vSphere client is installed.</note>
        
        <p>To install the NSX Manager:</p>
        <ol>
          <li>To locate the NSX Manager OVA file, either copy the download URL or download the OVA file onto your computer.</li>
            <li>In a browser, open vCenter.</li>
          <li>Select <b>VMs and Templates</b>, right-click your datacenter, and select <b>Deploy OVF Template</b>.</li>
          <li>Paste the download URL or click <b>Browse</b> to select the file on your computer.</li>
          <li>Select the checkbox <b>Accept extra configuration options</b>.
            <note type="note">This allows you to set IPv4 and IPv6 addresses, default gateway, DNS, NTP, and SSH properties during the installation, rather than configuring these settings manually after the installation.</note></li>
            <li>Accept the VMware license agreements.</li>
            <li>Select the location for the deployed NSX Manager
            <note>The name you type will appear in the vCenter inventory.
              The folder you select will be used to apply permissions to the NSX Manager.</note></li>
            <li>Select a host or cluster on which to deploy the NSX Manager appliance.</li>
              <li>Change the virtual disk format to <b>Thick Provision</b>, and select the destination datastore for the virtual machine configuration files and the virtual disks.</li>
            <li>Select the port group for the NSX Manager.</li>
          <li>Set the NSX Manager extra configuration options, ensuring the IP address for the NSX
          manager will resolve reverse DNS. <p>For example, this screen shows the final review
            screen after all the options are configured:</p><p><image
              href="../../media/NSXv_DeployOVF.png"/></p></li>
          <li>Click <b>Finish</b>.</li>
            </ol>
            
            <p>After the NSX Manager is installed, use the following steps to finish configuring your software appliance before deploying NSX Controllers.</p>
            
<ol>            
  <li>Open a web browser window and type in the hostname or IP address assigned to the NSX Manager.</li>
    <li>Accept the security certificate and then log in using the user name <b>admin</b> and the password set during installation.</li>
    <li>Once logged in, click on <b>Manager vCenter Registration</b>.</li>
      <li>Configure NSX manager for single sign on (SSO). <note type="important">SSO makes vSphere
            and NSX more secure by allowing the various components to communicate with each other
            through a secure token exchange mechanism, instead of requiring each component to
            authenticate a user separately. For more details, read VMware s documentation on <xref
              href="https://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.install.doc/GUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html"
              scope="external" format="html">Configure Single Sign On</xref></note></li>
    <li>Configure the NSX Manager to connect to the vCenter server.
        <note type="important">Use the following settings:
          <dl>
            <dlentry>
              <dt>Lookup Service Port</dt>
              <dd>443</dd>
            </dlentry>
            <dlentry>
              <dt>vCenter version</dt>
              <dd>6.0</dd>
            </dlentry>
            <dlentry>
              <dt>7444</dt>
              <dd>5.5</dd>
            </dlentry>
          </dl></note></li>
        </ol>
      </section>
    
        <section id="NSXv_vs_controller">
          <title>Add an NSX Controller</title>
          <p>NSX Controller is an advanced distributed-state management system that provides control plane functions for NSX logical switching and routing. The NSX controller serves as the central control point for all logical switches within a network and maintains information about all hosts, logical switches (VXLANs), and distributed logical routers.</p>
          
          <note type="important">No matter the size of the NSX deployment, VMware requires the
        following condition are met: <ul>
          <li>Each NSX Controller cluster contain three controller nodes. Having a different number
            of controller nodes is not supported.</li>
          <li>Before deploying NSX Controllers, you must deploy an NSX Manager appliance and
            register vCenter with NSX Manager.</li>
          <li>Determine the IP pool settings for your controller cluster, including the gateway and
            IP address range. DNS settings are optional.</li>
          <li>The NSX Controller IP network must have connectivity to the NSX Manager and to the
            management interfaces on the ESXi hosts.</li>
        </ul></note>
          <p>To add an NSX Controller:</p>
          <ol>
            <li>Before you begin this procedure, make sure you are logged out of the vSphere web client. If you are already logged in, then log out.</li>
             <li>Log into the vSphere web client (vCenter).</li>  
            <li>In vCenter, navigate to Home, select <b>Networking &amp; Security</b>, Installation, and then select the <b>Management</b> tab.</li>
              <li>In the <b>NSX Controller nodes</b> section, click the <b>Add Node</b> icon, represented by a green plus sign.
                <p>For example:</p>
                <image
                  href="../../media/NSX_add_controller_nodes_icon.png"/></li>
               <li>Enter the NSX Controller settings appropriate to your environment. <note
            type="important">NSX Controllers should be deployed to a vSphere Standard Switch or
            vSphere Distributed Switch port group which is not VXLAN based and has connectivity to
            the NSX Manager, other controllers, and to hosts via IPv4.</note><p>For example:</p>
          <image href="../../media/NSX_add_controller_DB.png"/></li>
               <li>If you have not already configured an IP pool for your controller cluster,
          configure one now by clicking New IP Pool. <note>Individual controllers can be in separate
            IP subnets, if necessary.</note></li>
               <li>Type and re-type a password for the controller, and then to save your changes click <b>OK</b>.</li>
               <li>After the first controller is completely deployed, deploy two additional
          controllers. <note type="important">Having three controllers is mandatory. We recommend
          configuring a DRS anti-affinity rule to prevent the controllers from residing on the same
          host.</note></li>
          </ol>
      </section>
    
    <section id="NSXv_vs_preparec">
      <title>Prepare Clusters for NSX Management</title><p>Host preparation is the process in which
        the NSX Manager does the following:</p><ol>
        <li>Installs NSX kernel modules on ESXi hosts that are members of vCenter clusters</li>
        <li>Builds the NSX control-plane and management-plane infrastructure. </li>
      </ol><p>NSX kernel modules packaged in VIB files run within the hypervisor kernel and provide
        services such as distributed routing, distributed firewall, and VXLAN bridging capabilities.
        To prepare your environment for network virtualization, you must install network
        infrastructure components on a per-cluster level for each vCenter server where needed. This
        deploys the required software on all hosts in the cluster. When a new host is added to this
        cluster, the required software is automatically installed on the newly added host.</p>
      Before beginning the NSX host preparation process, make sure the following conditions exist in
      your environment: <table frame="all" rowsep="1" colsep="1" id="table_prep_NSXv_clusters">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="25pt"/>
          <colspec colname="c2" colnum="2" colwidth="1*"/>
          <thead>
            <row>
              <entry>&#9744;</entry>
              <entry>Item</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry/>
              <entry>Register vCenter with NSX Manager and deploy NSX controllers.</entry>
            </row>
            <row>
              <entry/>
              <entry>Verify that DNS reverse lookup returns a fully qualified domain name when
                queried with the IP address of NSX Manager.</entry>
            </row>
            <row>
              <entry/>
              <entry>Verify that hosts can resolve the DNS name of vCenter server.</entry>
            </row>
            <row>
              <entry/>
              <entry>Verify that hosts can connect to vCenter Server on port 80.</entry>
            </row>
            <row>
              <entry/>
              <entry>Verify that the network time on vCenter Server and ESXi hosts is
                synchronized.</entry>
            </row>
            <row>
              <entry/>
              <entry>For each host cluster that will participate in NSX, verify that hosts within
                the cluster are attached to a common VDS. <note>For example, my deployment has a
                  cluster named Host1 and Host2. Host1 is attached to VDS1 and VDS2. Host2 is
                  attached to VDS1 and VDS3. When you prepare a cluster for NSX, you can only
                  associate NSX with VDS1 on the cluster. If you add another host (Host3) to the
                  cluster and Host3 is not attached to VDS1, it is an invalid configuration, and
                  Host3 will not be ready for NSX functionality. </note></entry>
            </row>
            <row>
              <entry/>
              <entry>If you have vSphere Update Manager (VUM) in your environment, you must disable
                it before preparing clusters for network virtualization. For information on how to
                check if VUM is enabled and how to disable it if necessary, see
                http://kb.vmware.com/kb/2053782. </entry>
            </row>
            <row>
              <entry/>
              <entry>Always make sure that the cluster is in the resolved state. If the
                  <b>Resolve</b> option does not appear in the cluster's Actions list, then it is in
                a resolved state.</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>To prepare clusters for NSX:</p><ol>
        <li>In vCenter, select <b>Home</b>, <b>Networking &amp; Security</b>, <b>Installation</b>,
          and then select the <b>Host Preparation</b> tab.</li>
        <li>For all clusters that will require NSX logical switching, routing, and firewalls, click
          the gear icon and then click <b>Install</b>. <note type="important">While installation is
            in progress, do not deploy, upgrade, or uninstall any service or component.</note></li>
        <li>Monitor the installation until the <b>Installation Status</b> column displays a green
          check mark. <note type="important">If the <b>Installation Status</b> column displays a red
            warning icon and says <b>Not Ready</b>, click <b>Resolve</b>. Clicking <b>Resolve</b>
            might result in a reboot of the host. If the installation is still not successful, click
            the warning icon. All errors are displayed. Take the required action and click
              <b>Resolve</b> again.</note></li>
        <li>When the installation is complete, the <b>Installation Status</b> column displays
            <b>6.2</b>. The <b>Uninstall</b> and the <b>Firewall</b> columns display <b>Enabled</b>.
          Both columns have a green check mark. If you see <b>Resolve</b> in the <b>Installation
            Status</b> column, click <b>Resolve</b> and then refresh your browser window.</li>
        <li>To verify the VIBs (esx-vsip and esx-vxlan) are installed and registered, SSH into a
          host within the prepared cluster.</li>
        <li>To list the name and version of the VIBs installed, run the following command: 
          <codeblock>esxcli software vib list | grep esx</codeblock>
          For example:
          <codeblock>[root@host:~] esxcli software vib list | grep esx
          ...
          esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
          esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
          ...</codeblock></li>
        <li> Click <b>OK</b> to save your changes.</li>
        <li> Repeat steps 5, 6, and 7 for all hosts within the prepared cluster.</li>
      </ol><note type="important">After host preparation: <ul>
          <li>a host reboot is not required</li>
          <li>If you add a host to a prepared cluster, the NSX VIBs automatically get installed on
            the host.</li>
          <li>If you move a host to an unprepared cluster, the NSX VIBs automatically get
            uninstalled from the host. In this case, a host reboot is required to complete the
            uninstall process.</li>
        </ul></note>
    </section>
    
    <section id="NSXv_vs_VXTP">
      <title>Configure VXLAN Transport Parameters</title> The VXLAN network is used for Layer 2
      logical switching across hosts, potentially spanning multiple underlying Layer 3 domains. You
      configure VXLAN on a per-cluster basis, where you map each cluster that is to participate in
      NSX to a vSphere distributed switch (VDS). When you map a cluster to a distributed switch,
      each host in that cluster is enabled for logical switches. The settings chosen here will be
      used in creating the VMkernel interface. <p>Configuring transport parameters involves
        selecting a VDS, a VLAN ID, an MTU size, an IP addressing mechanism, and a NIC teaming
        policy. The MTU for each switch must be set to 1550 or higher. By default, it is set to
        1600. This is also the recommended setting for integration with Helion OpenStack.. </p><p>To
        configure Transport Parameters: </p><ol>
        <li>In vCenter, navigate to <b>Home</b> , then <b>Networking &amp; Security</b>, then
            <b>Installation</b>.</li>
          <li>Select the <b>Host Preparation</b> tab. </li>
        <li>Click <b>Not Configured</b> in the VXLAN column.</li>
        <li>Set up logical networking.</li>
        <li> Click <b>OK</b> to save your changes.</li>
      </ol><p>For example:</p>
      <image href="../../media/NSX_transport_params.png"/>
      <note>Tips for setting up logical networking: <ul>
          <li>Plan your NIC teaming policy. Your NIC teaming policy determines the load balancing
            and failover settings of the VDS.</li>
          <li>Do not mix different teaming policies for different portgroups on a VDS where some use
            Etherchannel or LACPv1 or LACPv2 and others use a different teaming policy. If uplinks
            are shared in these different teaming policies, traffic will be interrupted. If logical
            routers are present, there will be routing problems. Such a configuration is not
            supported and should be avoided.</li>
          <li>The best practice for IP hash-based teaming (EtherChannel, LACPv1 or LACPv2) is to use
            all uplinks on the VDS in the team, and do not have portgroups on that VDS with
            different teaming policies.</li>
          <li>Plan the IP addressing scheme for the VXLAN tunnel end points (VTEPs). VTEPs are the
            source and destination IP addresses used in the external IP header to uniquely identify
            the ESX hosts originating and terminating the VXLAN encapsulation of frames. You can use
            either DHCP or manually configured IP pools for VTEP IP addresses.</li>
          <li>The Load Balance - SRCID as the VMKNic teaming policy is usually the most flexible out
            of all the available options. This allows each host to have a VTEP vmkernel interface
            for each dvuplink on the selected distributed switch (two dvuplinks gives two VTEP
            interfaces per ESXi host). </li>
          <li>A port group that will be used by the VTEP vmkernel interfaces will be automatically
            created on the specified distributed switch using the information provided. If not
            already done, create an IP pool on the network associated with the selected VLAN.</li>
          <li>For larger environments it may be better to use DHCP for the VMKNic IP
            Addressing.</li>
          <li>For more information and further guidance, see the <xref
              href="https://communities.vmware.com/docs/DOC-27683" scope="external" format="html"
              >VMwareÂ® NSX for vSphere Network Virtualization Design Guide</xref>.</li>
        </ul></note>
    </section>
      
    <section id="NSXv_vs_segment">
      <title>Assign a Segment ID Pool</title>
      <p>VXLAN segments are built between VXLAN tunnel end points (VTEPs). A hypervisor host is an example of a typical VTEP. Each VXLAN tunnel has a segment ID. You must specify a segment ID pool for each NSX Manager to isolate your network traffic. If an NSX controller is not deployed in your environment, you must also add a multicast address range to spread traffic across your network and avoid overloading a single multicast address.</p>
      
      <p>To assign a segment ID pool:</p>
      <ol>
        <li>In vCenter, navigate to <b>Home</b>, then <b>Networking &amp; Security</b>, then
            <b>Installation</b>.</li>
        <li>Select the <b>Logical Network Preparation </b>tab.</li>
        <li>Click <b>Segment ID</b>, and then <b>Edit</b>.</li>
        <li> Click <b>OK</b> to save your changes.</li>
      </ol>
          <note type="attention">For the purposes of this example, the following settings were used:
          <dl>
            <dlentry>
              <dt>Broadcast, Unknown unicast, and Multicast (BUM) replication</dt>
              <dd>unicast mode</dd>
            </dlentry>
            <dlentry>
              <dt>Enable multicast addressing</dt>
              <dd>unselected (if checked, clear the checkbox)</dd>
            </dlentry>
            <dlentry>
              <dt>Multicast addresses</dt>
              <dd>left blank</dd>
            </dlentry>
          </dl>
         For other environments, this example may not be the best configuration method. 
          </note>
    </section>
      
    <section id="NSXv_vs_transport">
      <title>Create a Transport Zone</title>
      <p>A transport zone controls to which hosts a logical switch can reach. It can span one or more vSphere clusters. Transport zones dictate which clusters and, therefore, which VMs can participate in the use of a particular network.</p>
      <p>An NSX environment can contain one or more transport zones based on your requirements. A host cluster can belong to multiple transport zones. A logical switch can belong to only one transport zone.</p>
       <p> NSX does not allow connection of VMs that are in different transport zones. The span of a logical switch is limited to a transport zone, so virtual machines in different transport zones cannot be on the same Layer 2 network. A distributed logical router cannot connect to logical switches that are in different transport zones. After you connect the first logical switch, the selection of further logical switches is limited to those that are in the same transport zone. Similarly, an edge services gateway (ESG) has access to logical switches from only one transport zone.</p>
    
      <p>For more details on transport zones, refer to the guidelines in VMWare's <xref
        href="https://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" scope="external" format="html">Add A Transport Zone</xref>  </p>
      
      <p>To create a transport zone:</p>
    <ol>
      <li>In vCenter, navigate to Home, then Networking &amp; Security, and then Installation.</li>
      <li>Select the Logical Network Preparation tab.</li>
      <li>Click Transport Zones, and then click the New Transport Zone (New Logical Switch) icon.</li>
      <li>In the New Transport Zone dialog box, type a name and an optional description for the transport zone.</li>
      <li>Select the control plane mode as Unicast.
        <note>Unicast: The control plane is handled by an NSX controller. All unicast traffic leverages optimized headend replication. No multicast IP addresses or special network configuration is required.</note></li>
      <li>Select the clusters to be added to the transport zone.</li>
      <li> Click <b>OK</b> to save your changes.</li>
    </ol>
      
    
    </section>
      
      
  </body>
</topic>
