<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd"><topic xml:lang="en-us" id="topic4797cgikdjm">
<title><ph conkeyref="HOS-conrefs/product-title"/> Sample activationtemplate.json File for  ESX
        Compute</title>
<prolog>
    <metadata>
        <othermeta name="layout" content="default"/>
        <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
        <othermeta name="role" content="Storage Administrator"/>
        <othermeta name="role" content="Storage Architect"/>
        <othermeta name="role" content="Michael B"/>
        <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
    </metadata>
</prolog>
<body>
    <p>The process for installing the Helion Entry Scale ESX, KVM with VSA Model requires creating a
      sample network information template and modifying the details of the template. You will use
      the template to register the cloud network configuration for the vCenter.  The template is in
      the JSON format.</p>
        <p>Execute the following command to get the activation template.</p>
        <codeblock>eon get-activation-template [--filename &lt;Activate JSON>] --type &lt;Type of the resource> </codeblock>
        <p>For example:</p>
        <p>
            <codeblock>eon get-activation-template --filename activationtemplate.json --type esxcluster </codeblock>
        </p>
        <p>A sample of <codeph>activationtemplate.json</codeph> file is shown below. <note type="warning">Update
                the sample template as per your environment. </note></p>
        <codeblock>{
 "hlm_version": "3.0",
 "input_model": {
  "server_group": "RACK1"
 }, 
 "run_playbook": true, 
 "network_properties": {
   "switches": [
   {
    "type": "dvSwitch", 
    "name": "MGMT-DVS", 
    "physical_nics": "vmnic1", 
    "mtu": "1500"
   }, 
   {
    "type": "dvSwitch", 
    "name": "TRUNK-DVS", 
    "physical_nics": "", 
    "mtu": "1500"
   }
  ], 
  "portGroups": [
   {
    "name": "ESX-CONF",
    "vlan_type": "trunk", 
    "vlan": "33",    
    "switchName": "MGMT-DVS",
    "nic_teaming": {
     "network_failover_detection": "1", 
     "notify_switches": "yes", 
     "load_balancing": "1", 
     "active_nics": "vmnic1"
    }
   }, 
   { 
    "name": "MGMT", 
    "vlan_type": "none",
    "vlan": "",
    "switchName": "MGMT-DVS", 
    "nic_teaming": {
     "network_failover_detection": "1", 
     "notify_switches": "yes", 
     "load_balancing": "1", 
     "active_nics": "vmnic1"
    }    
   },
   {
    "name": "GUEST",
    "vlan_type": "trunk", 
    "vlan": "34",    
    "switchName": "MGMT-DVS",
    "nic_teaming": {
     "network_failover_detection": "1", 
     "notify_switches": "yes", 
     "load_balancing": "1", 
     "active_nics": "vmnic1"
    },
    "cloud_network_type": "vxlan"
   },
   {
    "name": "TRUNK",
    "vlan_type": "trunk", 
    "vlan": "1-4094",   
    "switchName": "TRUNK-DVS"
   }
  ],
  "esx_conf_net": {
   "start_ip": "", 
   "cidr": "10.20.18.0/23", 
   "end_ip": "", 
   "gateway": "10.20.18.1", 
   "portGroup": "ESX-CONF"
  }, 
  "vm_config": [
   {
    "server_role": "ESX-COMPUTE-ROLE", 
    "template_name": "hlm-shell-vm", 
    "cpu": "4", 
    "memory_in_mb": "4096", 
    "nics": [
     {
      "device": "eth0", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "ESX-CONF"
     }, 
     {
      "device": "eth1", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "MGMT"
     }
    ]
   }, 
   {
    "server_role": "OVSVAPP-ROLE", 
    "template_name": "hlm-shell-vm", 
    "cpu": "4", 
    "memory_in_mb": "4096", 
    "nics": [
     {
      "device": "eth0", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "ESX-CONF"
     }, 
     {
      "device": "eth1", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "MGMT"
     },
     {
      "device": "eth2", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "GUEST"
     },
     {
      "device": "eth3", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "TRUNK"
     }
    ]
   }
  ],
  "lifecycle_manager": {
   "ip_address": "10.20.16.2", 
   "ssh_key": "ssh-rsa AAAAB3NzaC1yc2EA stack@company.org", 
   "user": "stack"
  }  
 }
}</codeblock>
        <p>The above template consists of the following sections:<ul>
                <li>Input model<ul id="ul_gf1_q3d_jw">
                        <li>Server group</li>
                    </ul></li>
                <li>Run playbook </li>
                <li>Network properties<ul id="ul_ezb_43d_jw">
                        <li>Distributed Virtual Switches to be created or reused</li>
                        <li>Port Groups to be created or reused</li>
                        <li>Virtual machine configuration (RAM, CPU and network interfaces)</li>
                        <li>Ansbile network</li>
                        <li>Lifecycle manager </li>
                    </ul></li>
            </ul>The main purpose of the activation template is to create an environment to host
                <codeph>nova-compute-proxy</codeph> and <codeph>OVSvApp</codeph> appliances. The
            environment here refers to the required virtual networking (DVS and port groups), VMs
            (RAM, CPU, NIC and associated server role) going to host the neutron and nova services,
            along with passwordless access from the lifecycle manager node.</p>
        <ul id="ul_clh_5ps_hw">
            <li><b>Input model</b><p>It describes the configuration of each entities in the input
                    model. <!--So, here we define the server group which is used as--></p><p>In the
                    following example we have specified the value of <codeph>server_group</codeph>
                    as RACK1.</p><codeblock>{
 "input_model": {
  "server_group": "RACK1"
 },</codeblock><p>
                    <!--In the input model under <codeph>server_groups.yml</codeph> file , you must define the server group.-->
                    <ul id="ul_ljd_1kd_jw">
                        <li><b>Server group</b><p>This key takes values specified in
                                    <codeph>server_groups.yml</codeph> file defined in input model.
                                Default server groups defined in input model are "RACK1", "RACK2",
                                "RACK3" based on different network definitions. This implies that
                                all the member hosts in the cluster are part of same RACK.</p></li>
                    </ul>
                </p></li>
            <li><b>Run playbook</b><p><codeph>run_playbook</codeph> provides an option for manually
                    or automatic execution of the ansible playbook to create an enviornment to host
                        <codeph>nova-compute-proxy</codeph> and <codeph>OVSvApp</codeph> appliances.
                    You can enter the details in the template and set the
                        <codeph>run_playbook</codeph> value to false to it manually.
                        </p><p><codeph>run_playbook</codeph> takes boolean values true (or)
                    false.<!-- For manual execution of the ansible playbooks set the value to false-->.
                    By default this is set to true.</p><p>In the following example we have set the
                        <codeph>run_playbook</codeph> as true. Therefore, the ansible file will be
                    executed automatically.<codeblock>"run_playbook": true</codeblock></p></li>
            <li><b>Network properties</b><p>You must define and configure the following network
                    properties.</p><p>
                    <ul id="ul_olz_s1r_jw">
                        <li><b>Distributed Virtual Switches</b><p>EON creates the virtual switch if
                                the specified switch does not exists in the vCenter. User needs to
                                provide the same kind of input required in creating one from the
                                vSphere webclient. </p><p> A minimum of two switch entries are
                                required of which one switch is OVSvApp trunk DVS with no
                                    <codeph>Physical_nics</codeph>.</p><p>In the following example
                                we have created a distributed virtual switch with a name MGMT-DVS
                                and used <codeph>vmnic1</codeph> as the uplink for the switch</p><p>
                                <codeblock>"switches": [
   {
    "type": "dvSwitch", 
    "name": "MGMT-DVS", 
    "physical_nics": "vmnic1", 
    "mtu": "1500"
   }, 
 ] </codeblock>
                            </p><p>The following table provides the list of parameters that must be
                                configured.</p><p>
                                <simpletable id="simpletable_ec4_fkd_jw">
                                    <sthead>
                                        <stentry>Parameter</stentry>
                                        <stentry>Description</stentry>
                                    </sthead>
                                    <strow>
                                        <stentry>type </stentry>
                                        <stentry> The category of switch. It can be Virtual Standard
                                            Switch (vss) or Distributed Virtual Switch (dvs).
                                            Currently, the supported type is only dvs.</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>name (String)</stentry>
                                        <stentry>Unique name for the switch</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>physical_nics</stentry>
                                        <stentry>Uplinks for the switch (comma separated string).
                                            Example: Two uplinks -- "vmnic0, vmnic1". Single uplink
                                            – 'vmnic0'. No uplinks – ''</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>MTU (Optional)</stentry>
                                        <stentry>Maximum Transmission Unit (MTU) packet size.
                                            Provide "mtu" value within the range of 1500 to
                                            9000.</stentry>
                                    </strow>
                                </simpletable>
                            </p></li>
                        <li><b>Port Groups</b><p>EON creates the portGroups if the specified PG is
                                unavailable. The portGroups key contains a JSON list of one or more
                                entries.</p><p>In the following example we have created a portGroup
                                with the name ESX-CONF with an active NIC. The VLAN port of ESX-CONF
                                allows multiple VLAN networks. Also, if there is any failover in the
                                switches then it is notified as the value is set to <b>yes</b> with
                                the failover detection set to
                                1.<codeblock>"portGroups": [
   {
    "name": "ESX-CONF",
    "vlan_type": "trunk", 
    "vlan": "33",    
    "switchName": "MGMT-DVS",
    "nic_teaming": {
     "network_failover_detection": "1", 
     "notify_switches": "yes", 
     "load_balancing": "1", 
     "active_nics": "vmnic1"
    }                                            
    “cloud_network_type”: “”                                 
  }      
]  </codeblock></p><p>The
                                following table provides the list of parameters that must be
                                configured.</p><p>
                                <simpletable id="simpletable_cjv_wws_hw">
                                    <sthead>
                                        <stentry>Parameter</stentry>
                                        <stentry>Description</stentry>
                                    </sthead>
                                    <strow>
                                        <stentry>name (String)</stentry>
                                        <stentry> Unique name for the portGroup to be created or
                                            used.</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>vlan_type (String)</stentry>
                                        <stentry>
                                            <p>The following are acceptable VLAN types:<ul
                                                  id="ul_nxd_rct_hw">
                                                  <li>none = untagged port</li>
                                                  <li>vlan = VLAN port and only the VLAN ID
                                                  mentioned in "VLAN" key is allowed</li>
                                                  <li>trunk = Port allows multiple VLAN networks as
                                                  mentioned by "VLAN" Key</li>
                                                </ul></p>
                                        </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>vlan</stentry>
                                        <stentry>Uplinks for the switch (comma separated string).
                                            Example: Two uplinks -- "vmnic0, vmnic1". Single uplink
                                            – 'vmnic0'. No uplinks – ''</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>switchName (String)</stentry>
                                        <stentry>Name of the Distributed Virtual Switch under which
                                            this port-group should be created </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>load_balancing (Int)</stentry>
                                        <stentry>
                                            <p>Specify how to choose an uplink. Acceptable values
                                                are 1 to 5.</p>
                                            <ol id="ol_yg2_gdt_hw">
                                                <li>Route based on the originating virtual port</li>
                                                <li>Route based on IP hash</li>
                                                <li>Route based on source MAC hash</li>
                                                <li>Route based on physical NIC load</li>
                                                <li>Use explicit failover order</li>
                                            </ol>
                                        </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>notify_switches (Boolean)</stentry>
                                        <stentry> Select whether to notify switches in the case of
                                            failover. </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>network_failover_detection (Int)</stentry>
                                        <stentry>
                                            <p>Specify the method to use for failover detection.</p>
                                            <ul id="ul_j21_kdt_hw">
                                                <li>Link Status only</li>
                                                <li>Beacon Probing</li>
                                            </ul>
                                        </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>active_nics</stentry>
                                        <stentry>(Comma separated string) Specify the uplinks to be
                                            used</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>cloud_network_type (Srting)</stentry>
                                        <stentry>(Optional) Specify the tenant network type (vlan or
                                            vxlan). In case of VLAN, the security policy for the
                                            port group is modified to enable 'Promiscuous Mode' and
                                            'Forged transmits'.</stentry>
                                    </strow>
                                </simpletable>
                            </p></li>
                        <li><b>Virtual machine configuration</b><p>EON creates one
                                    <codeph>nova-compute-proxy</codeph> virtual machine and 'N'
                                number of OVSvApp virtual machines where N represents the number of
                                ESXi hosts participating in the cluster. In other words, a OVSvApp
                                VM will be deployed per host.</p><p>In the following example we have
                                mentioned the server role as ESX-Compute-Role. This role is defined
                                in the <codeph>server_role.yml</codeph> file of the input model. We
                                require a template to create a virtual machine. The OVA i.e.
                                    <codeph>hlm-shell-vm</codeph>, is uploaded to the vCenter to
                                create a template. Four vCPU is configured along with 4096 mb
                                memory. The name of the network interface is eth0 and interface
                                model is
                                vmxnet3.<codeblock>vm_config": [
   {
    "server_role": "ESX-COMPUTE-ROLE", 
    "template_name": "hlm-shell-vm", 
    "cpu": "4", 
    "memory_in_mb": "4096", 
    "nics": [
     {
      "device": "eth0", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "ESX-CONF"
     }, 
     {
      "device": "eth1", 
      "type": "vmxnet3", 
      "pci_id": "", 
      "portGroup": "MGMT"
     }</codeblock></p><p>The
                                following table provides the list of parameters that must be
                                configured.</p><p>
                                <simpletable>
                                    <sthead>
                                        <stentry>Parameter</stentry>
                                        <stentry>Description</stentry>
                                    </sthead>
                                    <strow>
                                        <stentry>server_role (String) </stentry>
                                        <stentry>Specify the role associated to this appliance in
                                            the cloud input model</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>template_name (String) </stentry>
                                        <stentry>Name of the OVA uploaded to the vCenter to create
                                            the virtual machine</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>CPU (Int)</stentry>
                                        <stentry>Number of vCPUs to be configured</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>memory_in_mb (Int)</stentry>
                                        <stentry>Amount of memory to be configured in MB</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>Device (String) </stentry>
                                        <stentry>Network interface name. Generally in the format
                                            eth0, eth1… ethX</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>portGroup (String) </stentry>
                                        <stentry>Name of the virtual network (port-group) to be
                                            attached to the Interface </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>type (String) </stentry>
                                        <stentry>Type of interface model. Supported models are
                                            'vmxnet3' and 'e1000'</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>pci_id</stentry>
                                        <stentry><b>Please provide description</b></stentry>
                                    </strow>
                                    <strow>
                                        <stentry>nics</stentry>
                                        <stentry>List of network interfaces to be connected to the
                                            virtual machine</stentry>
                                    </strow>
                                </simpletable>
                            </p></li>
                        <li><b>Ansbile network</b><p>In ESX cloud, user require a separate network
                                for running ansible commands rather than sharing the management
                                network. IPAM is managed by EON and assigns IP addresses to virtual
                                machines from this network.</p><p>The following example provides a
                                network configuration that is defined under
                                    <codeph>esx_conf_net</codeph>.</p><p>
                                <codeblock>"esx_conf_net": {
   "start_ip": "", 
   "cidr": "10.20.18.0/23", 
   "end_ip": "", 
   "gateway": "10.20.18.1", 
   "portGroup": "ESX-CONF"
  }, </codeblock>
                            </p><p>The following table provides the list of parameters that must be
                                configured.</p><p>
                                <simpletable id="simpletable_yc1_qbr_jw">
                                    <sthead>
                                        <stentry>Parameter</stentry>
                                        <stentry>Description</stentry>
                                    </sthead>
                                    <strow>
                                        <stentry>portGroup (String) </stentry>
                                        <stentry>Name of the network interface for which IP address
                                            needs to be assigned</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>CIDR (String) </stentry>
                                        <stentry>Classless Inter-Domain Routing of ESX Configuration
                                            Network</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>Start_ip (String) </stentry>
                                        <stentry>First IPv4 allocation</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>end_ip (String) </stentry>
                                        <stentry>Last IPv4 allocation</stentry>
                                    </strow>
                                    <strow>
                                        <stentry>Gateway (String) </stentry>
                                        <stentry>IPv4 address of the network gateway</stentry>
                                    </strow>
                                </simpletable>
                            </p></li>
                        <li><b>Lifecycle Manager</b><p>For running ansible playbooks, EON requires
                                information regarding lifecycle manager and enabling password-less
                                login to the appliances.</p><p>The following example provides the
                                details of the lifecycle
                                manager.<codeblock>"lifecycle_manager": {
   "ip_address": "10.20.16.2", 
   "ssh_key": "ssh-rsa AAAAB3NzaC1yc2EA stack@company.org", 
   "user": "stack"</codeblock></p><p>The
                                following table provides the list of parameters that must be
                                configured.</p><p>
                                <simpletable frame="all" relcolwidth="1.0* 1.0*"
                                    id="simpletable_z3y_rbr_jw">
                                    <sthead>
                                        <stentry>Parameter</stentry>
                                        <stentry>Description</stentry>
                                    </sthead>
                                    <strow>
                                        <stentry>ip_address (String) </stentry>
                                        <stentry>IPv4 address of the lifecycle manager </stentry>
                                    </strow>
                                    <strow>
                                        <stentry>ssh_key (String) </stentry>
                                        <stentry>Contents of the public key stored in the lifecycle
                                            manager (<codeph>id_rsa.pub</codeph> file
                                                contents)<p>Required to enable password less login
                                                from the LCM to the appliances
                                                  (<codeph>nova-compute-proxy</codeph> and
                                                OVSvApp)</p></stentry>
                                    </strow>
                                    <strow>
                                        <stentry>user (String) </stentry>
                                        <stentry>HLM username. The default is 'stack' and supports
                                            other user accounts</stentry>
                                    </strow>
                                </simpletable>
                            </p></li>
                    </ul>
                </p></li>
        </ul>
    <section>
            <!--<title>Description of the fields and keys</title><p>The following fields and keys are in the <codeph>activationtemplate.json</codeph> file:</p><p><ul id="ul_clz_wrn_vv"><li><b>run_playbook</b>: This key takes boolean values true (or) false. For manual execution of the ansible playbooks set the to false. By default this is set to true.</li><li><b>server_group</b>: This key takes values specified in server_groups.yml defined in input model. Default server groups defined in input model are "RACK1", "RACK2", "RACK3" based on different network definitions. This implies that all the member hosts in the cluster are part of same RACK.</li><li><b>network_properties</b>: A minimum of two switch entries are required of which one switch is OVSvApp trunk DVS with no Physical_nics<ul id="ul_c1d_dws_xv"><li><codeph>physical_nics</codeph>: Provide a single or comma separated names. Physical NIC(s) must be unassigned to other uplinks.</li><li><codeph>mtu</codeph>: Provide "mtu" value within the range 1500 to 9000 to enable Jumbo Frames. </li></ul></li><li><b>portgroups</b>: A minimum of three portgroup entries are required. Portgroups that carry data traffic should have the <codeph>cloud_network_type</codeph> attribute. <ul id="ul_zp1_vk3_yv"><li>The <codeph>cloud_network_type</codeph> should have either <codeph>vlan</codeph> or <codeph>vxlan</codeph> value. The <codeph>vlan_type</codeph> are as follows: <ul id="ul_hdr_ml3_yv"><li><codeph>trunk</codeph> supports non-null values and ranges. For example: <codeph>33</codeph> or <codeph>300-500</codeph> or <codeph>300-500, 600, 700-800</codeph> </li><li><codeph>vlan</codeph> supports non null single vlan id and not range. </li><li><codeph>none</codeph> supports 0 or empty values </li><li><codeph>nic_teaming</codeph> explaination of the keys and their respective values: <ul id="ul_ktc_rl3_yv"><li><codeph>network_failover_detection</codeph> <ul id="ul_j1c_tl3_yv"><li><codeph>Link Status</codeph>: Relies solely on the link status that the network adapter provides. This option detects failures, such as cable pulls and physical switch power failures, but not configuration errors, such as a physical switch port being blocked by spanning tree or that is misconfigured to the wrong VLAN or cable pulls on the other side of a physical switch. </li><li><codeph>Beacon Probing</codeph>: Sends out and listens for beacon probes on all NI Cs in the team and uses this information, in addition to link status, to determine link failure. This detects many of the failures previously mentioned that are not detected by link status alone. </li><li><codeph>notify_switches</codeph>: If you select <codeph>Yes</codeph>, whenever a virtual NIC is connected to the switch or whenever that virtual NIC traffic would be routed over a different physical NIC in the team because of a failover event, a notification is sent out over the network to update the lookup tables on physical switches. In almost all cases, this process is desirable for the lowest latency of failover occurrences and migrations with vMotion. </li><li><codeph>load_balancing</codeph>: <ul id="ul_b4s_bm3_yv"><li>For a route based on the originating virtual port - choose an uplink based on the virtual port where the traffic entered the virtual switch.</li><li>For a route based on IP hash - choose an uplink based on a hash of the source and destination IP addresses of each packet. For non-IP packets, whatever is at those offsets is used to compute the hash. </li><li>For a route based on source MAC hash - choose an uplink based on a hash of the source Ethernet. </li><li>For a route based on physical NIC load - choose an uplink based on the current loads of physical NICs. </li><li>To use explicit failover order, always use the highest order uplink, from the list of Active adapters, which passes failover detection criteria.</li></ul></li><li><codeph>active_nics</codeph>: Provide <codeph>physical_nics</codeph> from the respective switch entry whivch will be configured as active uplinks. If not provided, the first entry in the <codeph>physical_nics</codeph> will be configured as active and the remaining as standby uplinks.</li></ul></li></ul></li></ul><ul id="ul_y15_cn3_yv"><li><codeph>vxlan</codeph>: For OVSvApp trunk any change in the <codeph>vlan</codeph> and <codeph>vlan_type</codeph> will be ignored. The OVSvApp trunk network has no uplinks. That is the reason that there is no <codeph>nic_teaming</codeph> section. If provided, the <codeph>nic_teaming</codeph> section will be ignored. </li></ul></li><li><b>vm_config</b>: The ESX Compute Proxy should have two NICs connecting to ESX-CONF and MGMT network. <ul id="ul_i14_mn3_yv"><li>The <codeph>device</codeph> values need not be in order but should start with 0 and increase based on requirement; do not skip a NIC value. For example: Configure the NICs as <codeph>eth3</codeph>, <codeph>eth1</codeph>, <codeph>eth0</codeph>, <codeph>eth2</codeph>. But you cannot configure it as <codeph>eth4</codeph>, <codeph>eth1</codeph>, <codeph>eth0</codeph>, <codeph>eth2</codeph>. For best practice, arrange your NICs in ascending order. </li></ul></li><li><b>esx_conf_net</b>: For the ESX CONF network configuration, the <codeph>portGroup</codeph> value should point to the ESX CONF portgroup name. You must provide the CIDR. The <codeph>start_ip</codeph> and <codeph>end_ip</codeph> are optional. If you specify one value, you must provide both values. </li><li><b>lifecycle_manager</b>: Provide the IP address of the lifecycle manager node. Also, provide a user name to configure a different user than inbuilt stack user.</li></ul></li></ul></p>-->
    
    <p>Return to installing <xref
        href="install_entryscale_esx_kvm_vsa.dita#install_esx/register-vcenter">Helion Entry Scale
        ESX, KVM with VSA Model</xref>.</p>
    </section>
  </body>
</topic>
