<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd"><topic xml:lang="en-us" id="topic4797cgikdjm">
<title><ph conkeyref="HOS-conrefs/product-title"/> Sample activationtemplate.json File for Helion
    Entry Scale ESX, KVM with VSA Model</title>
<prolog>
    <metadata>
        <othermeta name="layout" content="default"/>
        <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
        <othermeta name="role" content="Storage Administrator"/>
        <othermeta name="role" content="Storage Architect"/>
        <othermeta name="role" content="Michael B"/>
        <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
    </metadata>
</prolog>
<body>
    <p>The process for installing the Helion Entry Scale ESX, KVM with VSA Model requires creating a
      sample network information template and modifying the details of the template. You will use
      the template to register the cloud network configuration for the vCenter.  The template is in
      the JSON format.</p>
        <p>Execute the following command to get <codeph>activationtemplate.json</codeph> file.
            <codeblock>eon command</codeblock></p>
    <p>The following represents a sample <codeph>activationtemplate.json</codeph> file. Use values
            appropriate for your environment.</p>
        <p/>
        <p>
            <codeblock>vi activationtemplate.json

# Don't modify this value
"hlm_version": "3.0",

# Set to true if playbooks need to be run automatically on completing the installation of service VMâ€™s(OVSvApp and Compute proxy) and updating of input model
# Set it to false if playbooks will be manually run.
"run_playbook": True,

input_model": {
    "server_group": "RACK1"
},

"network_properties": {
	# 1. Minimum of two switch entries are required of which one switch is OVSvApp trunk DVS With no Physical_nics
	# 3. "physical_nics" can be a single or comma separated names. Physical NIC(s) must be unassigned to other uplinks.
	# 4. Provide "mtu" value within the range 1500 to 9000 to enable Jumbo Frames.

    "switches": [
        {
            "type": "dvSwitch",
            "name": "MGMT-DVS",
            "physical_nics": "vmnic1",
            "mtu": "1500"
        },

        {
		    # OVSvApp trunk without any "physical_nics"
            "type": "dvSwitch",
            "name": "TRUNK-DVS",
			"physical_nics": "",
            "mtu": "1500"
        }
    ],

	# 1. Minimum three portgroup entries are required.

	# 2. Portgroups that will carry data traffic should have the "cloud_network_type" attribute.
	# 3. "cloud_network_type" should have either "vlan" or "vxlan" value.

	# 4. "vlan_type" "trunk" supports non null values and ranges. Eg: 33 or 300-500 or 300-500, 600, 700-800
	#    "vlan_type" "vlan" supports non null single vlan id and not range.
	#    "vlan_type" "none" supports 0 or empty values

	# 5. "nic_teaming" explaination of the keys and their respective values:
	    # "network_failover_detection"
            # 1 -> Link Status. (Relies solely on the link status that the network adapter provides. This option detects failures,
            #such as cable pulls and physical switch power failures, but not configuration errors, such as a physical switch port
            #being blocked by spanning tree or that is misconfigured to the wrong VLAN or cable pulls on the other side of a physical switch.

            # 2-> Beacon Probing. (Sends out and listens for beacon probes on all NICs in the team and uses this information, in addition to
            #link status, to determine link failure. This detects many of the failures previously mentioned that are not detected by link status alone.

		# "notify_switches"
            #If you select "Yes", whenever a virtual NIC is connected to the Switch or whenever that virtual NIC?s traffic would be routed over
            #a different physical NIC in the team because of a failover event, a notification is sent out over the network to update the lookup
            #tables on physical switches. In almost all cases, this process is desirable for the lowest latency of failover occurrences and
            #migrations with vMotion.

		# "load_balancing"
			# 1 -> Route based on the originating virtual port
			#(Choose an uplink based on the virtual port where the traffic entered the virtual switch)

			# 2 -> Route based on IP hash
			#(Choose an uplink based on a hash of the source and destination IP addresses of each packet.
			#For non-IP packets, whatever is at those offsets is used to compute the hash)

			# 3 -> Route based on source MAC hash (Choose an uplink based on a hash of the source Ethernet.)

			# 4 -> Route based on physical NIC load (Choose an uplink based on the current loads of physical NICs)

			# 5 -> Use explicit failover order (Always use the highest order uplink, from the list of Active adapters,
			#which passes failover detection criteria)

		# "active_nics"
            #Provided "physical_nics" from the respective switch entry will be configured as active uplinks.
            #If not provided then the first entry in the "physical_nics"  will be configured as active and
            #the remaining as standby uplinks.

    "portGroups": [
        {
            "name": "ESX-CONF",
            "vlan": "33",
            "vlan_type": "trunk",
            "switchName": "MGMT-DVS",
            "nic_teaming": {
                "network_failover_detection": "1",
                "notify_switches": "yes",
                "load_balancing": "1",
                "active_nics": "vmnic1"
            }
        },

        {
            "name": "MGMT",
            "vlan": "",
            "vlan_type": "none",
            "switchName": "MGMT-DVS",
            "nic_teaming": {
                "network_failover_detection": "1",
                "notify_switches": "yes",
                "load_balancing": "1",
                "active_nics": "vmnic1"
            },
            "cloud_network_type": "vxlan"
        },

        {
		    # For OVSvApp trunk any change in the "vlan" and "vlan_type" will be ignored.
			# The OVSvApp trunk network has no uplinks. That's why there is no "nic_teaming" section.
			# Provided "nic_teaming" section will be ignored.
            "name": "TRUNK",
            "vlan": "1-4094",
            "vlan_type": "trunk",
            "switchName": "TRUNK-DVS"
        }
    ],

	# 1. ESX Compute Proxy should have two "nics" connecting to ESX-CONF &amp; MGMT network.
	# 2. "device" values need not be in order but should start with 0 and increase based on requirement.
	# 3. Eg: I can configure the nics as eth3, eth1, eth0, eth2 But I cannot configure it as eth4, eth1, eth0, eth2
	#    For best practice, arrange your nics allways in order.

    "vm_config": [
        {
            "server_role": "ESX-COMPUTE-ROLE",
            "template_name": "hlm-shell-vm",
            "cpu": "4",
            "memory_in_mb": "4096",
            "nics": [
                {
                    "device": "eth0",
                    "portGroup": "ESX-CONF",
                    "type": "vmxnet3",
                    "pci_id": ""
                },
                {
                    "device": "eth1",
                    "portGroup": "MGMT",
                    "type": "vmxnet3",
                    "pci_id": ""
                }
            ]
        },

        {
            "server_role": "OVSVAPP-ROLE",
            "template_name": "hlm-shell-vm",
            "cpu": "4",
            "memory_in_mb": "4096",
            "nics": [
                {
                    "device": "eth0",
                    "portGroup": "ESX-CONF",
                    "type": "vmxnet3",
                    "pci_id": ""
                },
                {
                    "device": "eth1",
                    "portGroup": "MGMT",
                    "type": "vmxnet3",
                    "pci_id": ""
                },
                {
                    "device": "eth2",
                    "portGroup": "TRUNK",
                    "type": "vmxnet3",
                    "pci_id": ""
                }
            ]
        }
    ],

	# ESX CONF network configuration
	# 1. "portGroup" value should point the ESX CONF portgroup name
	# 2. It is mandatory to provide "cidr"
	# 3. "start_ip" &amp; "end_ip" are optional. Either provide both values or don't provide at all.

    "esx_conf_net": {
        "portGroup": "ESX-CONF",
        "cidr": "10.20.18.0/23",
        "start_ip": "",
        "end_ip": "",
        "gateway": "10.20.18.1"
    },

	# "ip_address" is the IP of deployer node
	# Provide "user" name to configure a different user than inbuilt stack user.

    "lifecycle_manager": {
        "ip_address": "10.20.16.2",
        "ssh_key": "ssh-rsa AAAAB3NzaC1yc2EA stack@company.org",
        "user": "stack"
    }
}

Configuration of PG carrying Data in the single Nic VxLAN model
+---------------------+-------------------------------+
|   Input model	|     Net_conf.json Vxlan       |
+---------------------+-------------------------------+
| tagged-vlan: true   | "vlan": "33",                 |
| vlanid: 33          |  "vlan_type":"trunk",         |
+---------------------+-------------------------------+
| tagged-vlan: false  |  "vlan": "",                  |
| vlanid:             |  "vlan_type":"none",          |
+---------------------+-------------------------------+
| tagged-vlan: false  |  If the physical uplink       |
| vlanid: 33          |  switch is tagged to          |
|			|  particular vlan              |                               
|                     |  "vlan": "33",                |
|                     |  "vlan_type":"vlan",          |
+---------------------+-------------------------------+


Configuration of PG carrying Data in the single Nic VLAN model

+--------------------------+-------------------------------+
|   Input model	     |     Net_conf.json Vlan        |
+--------------------------+-------------------------------+
| tagged-vlan: false       | "vlan": "0, 1231-1331",       |
| vlanid:                  |                               |
| tenant-vlan-id:1231-1331 | "vlan_type":"trunk",          |
+--------------------------+-------------------------------+</codeblock>
        </p>
    <section>
      <title>Description of the fields and keys</title>
      <p>The following fields and keys are in the <codeph>activationtemplate.json</codeph> file:</p>
    
    <p>
      <ul id="ul_clz_wrn_vv">
        <li><b>run_playbook</b>: This key takes boolean values true (or) false. For manual execution
          of the ansible playbooks set the to false. By default this is set to true.</li>
        <li><b>server_group</b>: This key takes values specified in server_groups.yml defined in
          input model. Default server groups defined in input model are "RACK1", "RACK2", "RACK3"
          based on different network definitions. This implies that all the member hosts in the
          cluster are part of same RACK.</li>
        <li><b>network_properties</b>: A minimum of two switch entries are required of which one
          switch is OVSvApp trunk DVS With no Physical_nics<ul id="ul_c1d_dws_xv">
            <li><codeph>physical_nics</codeph>: Provide a single or comma separated names. Physical
              NIC(s) must be unassigned to other uplinks.</li>
            <li><codeph>mtu</codeph>: Provide "mtu" value within the range 1500 to 9000 to enable
              Jumbo Frames. </li>
          </ul></li>
        <li><b>portgroups</b>: A minimum of three portgroup entries are required. Portgroups that
          carry data traffic should have the <codeph>cloud_network_type</codeph> attribute. <ul
            id="ul_zp1_vk3_yv">
            <li>The <codeph>cloud_network_type</codeph> should have either <codeph>vlan</codeph> or
                <codeph>vxlan</codeph> value. The <codeph>vlan_type</codeph>: <ul id="ul_hdr_ml3_yv">
                <li><codeph>trunk</codeph> supports non-null values and ranges. For example:
                    <codeph>33</codeph> or <codeph>300-500</codeph> or <codeph>300-500, 600,
                    700-800</codeph>
                </li>
                <li><codeph>vlan</codeph> supports non null single vlan id and not range. </li>
                <li><codeph>none</codeph> supports 0 or empty values </li>
                <li><codeph>nic_teaming</codeph> explaination of the keys and their respective
                  values: <ul id="ul_ktc_rl3_yv">
                    <li><codeph>network_failover_detection</codeph>
                      <ul id="ul_j1c_tl3_yv">
                        <li><codeph>Link Status</codeph>: Relies solely on the link status that the
                          network adapter provides. This option detects failures, such as cable
                          pulls and physical switch power failures, but not configuration errors,
                          such as a physical switch port being blocked by spanning tree or that is
                          misconfigured to the wrong VLAN or cable pulls on the other side of a
                          physical switch. </li>
                        <li><codeph>Beacon Probing</codeph>: Sends out and listens for beacon probes
                          on all NI Cs in the team and uses this information, in addition to link
                          status, to determine link failure. This detects many of the failures
                          previously mentioned that are not detected by link status alone. </li>
                        <li><codeph>notify_switches</codeph>: If you select <codeph>Yes</codeph>,
                          whenever a virtual NIC is connected to the switch or whenever that virtual
                          NIC traffic would be routed over a different physical NIC in the team
                          because of a failover event, a notification is sent out over the network
                          to update the lookup tables on physical switches. In almost all cases,
                          this process is desirable for the lowest latency of failover occurrences
                          and #migrations with vMotion. </li>
                        <li><codeph>load_balancing</codeph>: <ul id="ul_b4s_bm3_yv">
                            <li>For a route based on the originating virtual port, choose an uplink
                              based on the virtual port where the traffic entered the virtual
                              switch).</li>
                            <li>For a route based on IP hash, choose an uplink based on a hash of
                              the source and destination IP addresses of each packet. For non-IP
                              packets, whatever is at those offsets is used to compute the hash) </li>
                            <li>For a route based on source MAC hash, choose an uplink based on a
                              hash of the source Ethernet. </li>
                            <li>For a route based on physical NIC load, choose an uplink based on
                              the current loads of physical NICs) </li>
                            <li>To use explicit failover order, always use the highest order uplink,
                              from the list of Active adapters, which passes failover detection
                              criteria.</li>
                          </ul></li>
                        <li><codeph>active_nics</codeph>: Provide <codeph>physical_nics</codeph>
                          from the respective switch entry will be configured as active uplinks. If
                          not provided then the first entry in the <codeph>physical_nics</codeph>
                          will be configured as active and the remaining as standby uplinks.</li>
                      </ul></li>
                  </ul></li>
              </ul><ul id="ul_y15_cn3_yv">
                <li><codeph>vxlan</codeph>:  For OVSvApp trunk any change in the
                    <codeph>vlan</codeph> and <codeph>vlan_type</codeph> will be ignored. The
                  OVSvApp trunk network has no uplinks. That's why there is no
                    <codeph>nic_teaming</codeph> section. If provided the
                    <codeph>nic_teaming</codeph> section will be ignored. </li>
              </ul></li>
            <li><b>vm_config</b>:  The ESX Compute Proxy should have two NICs connecting to ESX-CONF
              &amp; MGMT network. <ul id="ul_i14_mn3_yv">
                <li>The <codeph>device</codeph> values do not be in order but should start with 0
                  and increase based on requirement; do not skip a NIC value. For example: Configure
                  the NICs as <codeph>eth3</codeph>, <codeph>eth1</codeph>, <codeph>eth0</codeph>,
                    <codeph>eth2</codeph>. But you cannot configure it as <codeph>eth4</codeph>,
                    <codeph>eth1</codeph>, <codeph>eth0</codeph>, <codeph>eth2</codeph>. For best
                  practice, arrange your NICs in ascending order. </li>
              </ul></li>
            <li><b>esx_conf_net</b>:  For the ESX CONF network configuration, the
                <codeph>portGroup</codeph> value should point to the ESX CONF portgroup name. You
              must provide the CIDR.  The <codeph>start_ip</codeph> and <codeph>end_ip</codeph> are
              optional. If you specify one value, you must provide both values. </li>
            <li><b>lifecycle_manager</b>: Provide the IP address of the lifecycle manager node.
              Also, provide a user name to configure a different user than inbuilt stack user.</li>
          </ul></li>
      </ul>
    </p>
    
    <p>Return to installing <xref
        href="installing_esx_kvm_vsa.dita#install_esx/register-vcenter">Helion Entry Scale
        ESX, KVM with VSA Model</xref>.</p>
    </section>
  </body>
</topic>
