<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Usage of Ceph Storage in Helion OpenStack  </title>
    <abstract>
        <shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
            backend.</shortdesc>
    </abstract>
    <body>
        <!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section id="expandCollapse"
            ><!-- <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv> <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv> --></section>
        <p>Ceph is a very versatile storage technology and facilitates the utilization of storage
            via multiple protocols. Ceph's integration with Helion OpenStack services provides
            support for various storage use cases, such as the use of the storage pool for
            cinder-volumes as well as glance data stores. Helion OpenStack further simplifies
            administrative overhead by providing Ansible playbooks that automate tasks related to
            various storage use cases, such as: <ul id="ul_gx3_sdd_jw">
                <li>Creating a storage pool</li>
                <li>Deploying a Ceph client component on a client node</li>
                <li>Configuring OpenStack services with a storage pool</li>
            </ul></p>
        <p>This section provides guidance for the following scenarios commonly encountered in
            various Ceph implementations:</p>
        <p>
            <ol id="ol_ix2_bdd_jw">
                <li><xref href="#config_ceph/setup-deployer-node" format="dita">Setup Deployer as
                        Client Node</xref></li>
                <li><xref href="#config_ceph/using-core-ceph-openstack-service" format="dita">Using
                        Core Ceph for OpenStack Services</xref><ol id="ol_kxs_nqf_kw">
                        <li><xref href="#config_ceph/preq" format="dita">Prerequisite</xref></li>
                        <li><xref href="#config_ceph/glance-data-store" format="dita">Use Ceph
                                Storage Pool as Glance Data Store</xref></li>
                        <li><xref href="#config_ceph/cinder-volume-backend" format="dita">Use Ceph
                                Storage Pool as Cinder Volume Backend</xref></li>
                        <li><xref href="#config_ceph/cinder-backup-device" format="dita">Use Ceph
                                Storage Pool as Cinder Backup Device</xref></li>
                    </ol></li>
                <li><xref href="#config_ceph/rados-gw-object-storage" format="dita">Use RadosGateway
                        as Object Storage</xref></li>
            </ol>
        </p>
        <p> Ceph storage pool can be used an as ephemeral storage backend for nova as well, though
            this is not<?oxy_custom_start type="oxy_content_highlight" color="166,89,220"?> formally
            supported<?oxy_custom_end?>. </p>
        <section id="setup-deployer-node"><title>Set up Deployer as Client Node</title>
            
            <p>By default, your deployer node is not configured as a Ceph client node, and as a
                result, you can not perform Ceph operations from it. Generally, it is more helpful
                to set up a deployer node as an admin client node in order to perform various Ceph
                operations. Doing so will help you manage various aspects of a Ceph cluster without
                logging to the monitor node. You can turn your deployer node into client node by
                executing the following
                playbook.<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</codeblock></p><p>After successful execution of the above playbook, your deployer node will be configured with an
                admin key ring and subsequently act as an admin node.</p>
            
        </section>
        <section id="using-core-ceph-openstack-service">
            <title>Using Core Ceph OpenStack Services</title>
            <p>The use of cinder for glance, cinder volume, and cinder backup requires pool creation
                and configuring ceph-client on respective OpenStack Service nodes. To simplify the
                tasks required for this setup, the playbook <codeph>ceph-client-prepare.yml</codeph>
                has been provided. This playbook reads the storage pool configuration specified in
                    <codeph>ceph_user_model.yml</codeph> and sets up an OpenStack client node
                accordingly. To implement this configuration, perform the following: <ol
                    id="ol_g2z_rrf_kw">
                    <li>Define user model </li>
                    <li>Run <codeph>hosts/verb_hosts ceph-client-prepare.yml </codeph><p>The default
                            user model provided with <keyword keyref="kw-hos-tm"/> is shown below.
                            The default configuration contains storage pool configuration for
                            cinder-volume, glance and cinder-backup. You can edit the file if you do
                            not intend to use Ceph for all services, or want to change pool
                            attributes based on your cluster configuration. For example: if you have
                            large number of disk then you can increase PG number for a given
                            pool.<!-- Is this referring to the size of individual disks or the total number of disks? --></p><p>
                            <codeblock>---

product:
   version: 2

ceph_user_models:
    - user:
        name: cinder
        type: openstack
        secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
      pools:
        - name: volumes
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 100
          usage:
            consumer: cinder-volume
        - name: vms
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 100
          usage:
            consumer: nova
        - name: images
          attrs:
            creation_policy: lazy
            permission: rx
    - user:
        name: glance
        type: openstack
      pools:
        - name: images
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 128
          usage:
            consumer: glance-datastore
    - user:
        name: cinder-backup
        type: openstack
      pools:
        - name: backups
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 128
          usage:
            consumer: cinder-backup</codeblock>
                        </p><p>The following table provides the descriptions of the parameters
                            listed above. </p><p>
                            <table frame="all" rowsep="1" colsep="1" id="table_c4f_hjw_nw">
                                <tgroup cols="4">
                                    <colspec colname="c1" colnum="1" colwidth="1.1*"/>
                                    <colspec colname="c2" colnum="2" colwidth="1*"/>
                                    <colspec colname="newCol3" colnum="3" colwidth="1.38*"/>
                                    <colspec colname="newCol4" colnum="4" colwidth="1.34*"/>
                                    <thead>
                                        <row>
                                            <entry>Paramter</entry>
                                            <entry>Value</entry>
                                            <entry>Description</entry>
                                            <entry>Recommendation</entry>
                                        </row>
                                    </thead>
                                    <tbody>
                                        <row>
                                            <entry>user.name</entry>
                                            <entry>A user defined string</entry>
                                            <entry>Defines user name who can access a set of pools.
                                                A user is created with same name in the Ceph
                                                system.</entry>
                                            <entry>It is recommended to retain default names for
                                                some of the well-known OpenStack users as described
                                                in the default user model. For example: cinder user
                                                for volume access who needs to access volumes, vms,
                                                images pool as defined in default model.</entry>
                                        </row>
                                        <row>
                                            <entry>user.type</entry>
                                            <entry>openstack | user</entry>
                                            <entry>Indicates whether the user is specific for
                                                OpenStack services. Presently, it does not have
                                                semantic implication.</entry>
                                            <entry>Use <b>openstack</b> for pools used by
                                                cinder-volume, cinder-backup, glance and nova
                                                services. For other services, you can choose
                                                  <b>user</b>.</entry>
                                        </row>
                                        <row>
                                            <entry>user.secret_id</entry>
                                            <entry>UUID instance</entry>
                                            <entry>A unique UUID. </entry>
                                            <entry>Generate a value for your cluster before setting
                                                up the cinder client. The <codeph>libvirt</codeph>
                                                process needs it to access the cluster while
                                                attaching a block device from Cinder</entry>
                                        </row>
                                        <row>
                                            <entry>pools.name</entry>
                                            <entry>A user defined string</entry>
                                            <entry>
                                                <p>A user defined name.</p>
                                            </entry>
                                            <entry>Name has to be unique in Ceph namespace.</entry>
                                        </row>
                                        <row>
                                            <entry>pools.attrs.creation_policy</entry>
                                            <entry>eager | lazy</entry>
                                            <entry><p>If creation_policy is ‘eager’, the playbooks
                                                  will create the user.</p> The creation_policy is
                                                set to ‘lazy’, the pool will be created externally
                                                (not by Ceph ansible playbooks) out of band.</entry>
                                            <entry>It is recommended to retain default values for
                                                pools meant to be consumed by the OpenStack
                                                services.</entry>
                                        </row>
                                        <row>
                                            <entry>pools.attrs.type</entry>
                                            <entry>Replicated</entry>
                                            <entry>Defines the type of pool. Ceph supports erasure
                                                coded and replicated pool. </entry>
                                            <entry>Retain value as replicated if you do want to use
                                                  <codeph>ceph-client-prepare.yml</codeph> for pool
                                                creation. <p>It does not mean that you cannot create
                                                  erasure code pool with your cluster deployed using
                                                  Helion OpenStack. It just means that erasure codes
                                                  pools are not officially supported and you have to
                                                  take help of technical support.</p></entry>
                                        </row>
                                        <row>
                                            <entry>pools.attrs.replica_size</entry>
                                            <entry>1..n </entry>
                                            <entry>Replica count.</entry>
                                            <entry>It is recommended to use 3 as replica count
                                                because it is used in most common scenarios.</entry>
                                        </row>
                                        <row>
                                            <entry>pools.attrs.permission</entry>
                                            <entry>rwx</entry>
                                            <entry>Read, write and execute permission a given user
                                                will have on given pool.</entry>
                                            <entry>Retain the values for OpenStack user as mentioned
                                                in the default user model. Altering the values might
                                                affect your client setup configuration.</entry>
                                        </row>
                                        <row>
                                            <entry>pools.attrs.pg</entry>
                                            <entry>Placement group number</entry>
                                            <entry>Number of placement groups.</entry>
                                            <entry>Default value is 128. You can change the value of
                                                this parameter if your disk size is larger.</entry>
                                        </row>
                                        <row>
                                            <entry>pools.usage.consumer</entry>
                                            <entry>Pre-defined choices</entry>
                                            <entry>Defines consumer of pool. It indicates which
                                                services is expected to consume a given pool with
                                                what access. It has semantic meaning for pre-defined
                                                following choices:<ol id="ol_zp1_s5d_lw">
                                                  <li>nova</li>
                                                  <li>glance-datastore</li>
                                                  <li>cinder-volume</li>
                                                  <li>cinder-back-up</li>
                                                </ol></entry>
                                            <entry>Do not pick the value from (glance-datastore,
                                                nova, cinder-volume, cinder-backup) for user defined
                                                pools. For user defined pool, you can skip
                                                it.</entry>
                                        </row>
                                    </tbody>
                                </tgroup>
                            </table>
                        </p></li>
                </ol></p>
        </section>
        <section id="preq"><b>Prerequisites</b>
            <p>In order for Ceph to be used as a backend for volumes, the nodes running these (
                Cinder, Glance and Nova Compute) services should have the Ceph client installed on
                them. Use the <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph
                client on these nodes. </p><codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
            <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
            <p>
                <note>The steps below install required packages and configure existing client nodes
                    (i.e. Cinder, Glance and Nova Compute nodes) to use the Ceph cluster. However,
                    for any new client nodes added later on that need to be configured to use the
                    Ceph cluster, just execute the above playbook with the addition of the
                        <codeph>--limit &lt;new-client-node></codeph> switch.</note>
            </p></section>
               <section id="glance-data-store"><title>Use Ceph Storage Pool as Glance Data Store</title>
                   
            <p>To enable Ceph as a Glance data
                store, perform the following:<ol id="ol_syr_kwd_jw">
                    <li>Login to lifecycle manager.</li>
                    <li>Make the following changes in the the
                            <codeph>~/helion/my_cloud/config/glance/glance-api.conf.j2</codeph>
                        file: <ol id="ol_pc1_zqm_2w">
                            <li>Copy the following content and paste them in the
                                    <codeph>glance-api.conf.j2</codeph> file under
                                    <b>[glance_store]</b> :
                                <codeblock><b>[glance_store]</b>
default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</codeblock></li>
                            <li>In the same file, comment out the following references to Swift:
                                <codeblock>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</codeblock></li>
                        </ol>
                    </li>
                    <li><b>IMPORTANT:</b> If you have pre-existing images in your Glance repo and
                        want to use exclusively Ceph as a backend, use the Glance CLI or other tool
                        back-up your images prior to configuring Ceph as your Glance backend: <ol
                            id="ol_fg1_zqm_2w">
                            <li>Snapshot or delete all Nova instances using those images.</li>
                            <li>Download the images locally that you want to save.</li>
                            <li>Delete all of the images from Glance.</li>
                        </ol><p>After you have finished the Ceph configuration you will need to
                            re-add those images.</p></li>
                    <li>Commit your configuration to the <xref href="../installation/using_git.dita"
                            >local git repo</xref>, as follows:
                        <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
                    <li>Run the configuration processor:
                        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory:
                        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Run the Glance reconfigure playbook to configure Ceph as a Glance backend:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
                </ol></p>
                   
               </section>
        <p>After the successful execution of the above command, glance is configured to use Ceph as
            a data store. You can use glance operations to upload images which will be stored in the
            Ceph cluster.</p>
        <section id="cinder-volume-backend">
            <title>Use Ceph Storage Pool as Cinder Volume Backend</title>
            
            <p>To enable Ceph as a Cinder volume backend, perform the following:</p>
            <p>
                <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Make the following changes to the
                            <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file:
                            <ol id="ol_pwz_yqm_2w">
                            <li>Add your Ceph backend to the <codeph>enabled_backends</codeph>
                                section:
                                    <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock><note
                                    type="important">If you are using multiple backend types, you
                                    can use a comma delimited list here. For example, if you are
                                    going to use both VSA and Ceph backends, specify
                                        <codeph>enabled_backends=vsa-1,ceph1</codeph>.</note></li>
                            <li>[OPTIONAL] If you want your volumes to use a default volume type,
                                enter the name of the volume type in the <codeph>[DEFAULT]</codeph>
                                section with the syntax below. <b>Make note of this value when you
                                    create your volume type later.</b>
                                <p>
                                    <note type="important">If you do not specify a default type then
                                        your volumes will default to a non-redundant RAID
                                        configuration. It is recommended that you create a volume
                                        type and specify it here that meets your environments
                                        needs.</note>
                                </p><codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li>
                            <li>Uncomment the <codeph>ceph</codeph> section and fill the values as
                                per your cluster information. If you have more than one cluster, you
                                will need to add another similar section with its respective values.
                                In the following example only one cluster is added.
                                    <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock><p>where:</p><table
                                    frame="all" rowsep="1" colsep="1" id="ceph_volume">
                                    <tgroup cols="2">
                                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                                        <colspec colname="c2" colnum="2" colwidth="1.39*"/>
                                        <thead>
                                            <row>
                                                <entry>Value</entry>
                                                <entry>Description</entry>
                                            </row>
                                        </thead>
                                        <tbody>
                                            <row>
                                                <entry>rbd_secret_uuid</entry>
                                                <entry>The secret_id value from the
                                                  <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                                  file, highlighted below:: <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>&lt;secret ID will be here></b>
pools:
    - name: volumes</codeblock>
                                                  <note type="important">You should generate and use
                                                  your own secret ID. You can use any UUID
                                                  generation tool.</note></entry>
                                            </row>
                                            <row>
                                                <entry>rbd_user</entry>
                                                <entry>The username value from the
                                                  <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                                  file, highlighted
                                                  below:<codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: &lt;secret ID will be here>
pools:
    - name: volumes</codeblock></entry>
                                            </row>
                                            <row>
                                                <entry>rbd_pool</entry>
                                                <entry>The pool name value from the
                                                  <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                                  file, highlighted below:
                                                  <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                                            </row>
                                            <row>
                                                <entry>rbd_ceph_conf</entry>
                                                <entry>The Ceph configuration file location, usually
                                                  <codeph>/etc/ceph/ceph.conf</codeph></entry>
                                            </row>
                                            <row>
                                                <entry>volume_driver</entry>
                                                <entry>The Cinder volume driver. Leave this as the
                                                  default value specified for Ceph.</entry>
                                            </row>
                                            <row>
                                                <entry>volume_backend_name</entry>
                                                <entry>The name given to the Ceph backend.
                                                  <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                                            </row>
                                        </tbody>
                                    </tgroup>
                                </table>
                            </li>
                        </ol></li>
                        <li>To enable attaching Ceph volumes to Nova provisioned instances, make the
                            following changes to the
                                <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph>
                            file: <ol id="ol_p31_zqm_2w">
                                <li>Uncomment the Ceph backend lines and edit them as follows:
                                        <codeblock>[libvirt]
rbd_user = &lt;ceph-user>
rbd_secret_uuid = &lt;secret-uuid></codeblock><p>where:</p><table
                                        frame="all" rowsep="1" colsep="1" id="nova_volume">
                                        <tgroup cols="2">
                                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                                        <colspec colname="c2" colnum="2" colwidth="1.63*"/>
                                        <thead>
                                            <row>
                                                <entry>Value</entry>
                                                <entry>Description</entry>
                                            </row>
                                        </thead>
                                        <tbody>
                                            <row>
                                                <entry>rbd_user</entry>
                                                <entry>The username value from the
                                                  <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                                  file, highlighted
                                                  below:<codeblock>- user:
            name: <b>cinder</b>
            type: openstack
            secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                                            </row>
                                            <row>
                                                <entry>rbd_secret_uuid</entry>
                                                <entry>The secret_id value from the
                                                  <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                                  file, highlighted
                                                  below:<codeblock>- user:
           name: <b>cinder</b>
           type: openstack
           secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                                            </row>
                                        </tbody>
                                    </tgroup>
                                    </table><note>To attach a volume provisioned out of a
                                        newly-added Ceph backend to an existing OpenStack instance,
                                        the instance must be rebooted after the new backend has been
                                        added.</note></li>
                            </ol></li>
                    <li>Commit your configuration to the <xref href="../installation/using_git.dita"
                            >local git repo</xref>, as follows:
                        <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
                    <li>Run the configuration processor:
                        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory:
                        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Run the Cinder Reconfigure Playbook:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
                        <li>If Nova has been configured to attach Ceph backend volumes, run the Nova
                            reconfigure playbook:
                            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
                </ol>
            </p>
              
        </section>
        <p>After the successful execution of the above command, the cinder-volume is configured to
            use Ceph as data store. You can use volume lifecycle operations as described at  <xref
                href="../installation_verification.dita#install_verification/volume_verify"/></p>
        <section id="cinder-backup-device"><title>Use Ceph Storage Pool as Cinder Backup Device</title>
            
                <p>To enable Cinder backup device to Ceph, make the following changes to the
                        <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol
                        id="ol_f11_zqm_2w">
                        <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values:
                                <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock><p>where:</p><table
                                frame="all" rowsep="1" colsep="1" id="ceph_backup">
                                <tgroup cols="2">
                                <colspec colname="c1" colnum="1" colwidth="1*"/>
                                <colspec colname="c2" colnum="2" colwidth="1.27*"/>
                                <thead>
                                    <row>
                                        <entry>Value</entry>
                                        <entry>Description</entry>
                                    </row>
                                </thead>
                                <tbody>
                                    <row>
                                        <entry>backup_driver</entry>
                                        <entry>The Cinder volume driver. Leave this as the default
                                            value specified for Ceph.</entry>
                                    </row>
                                    <row>
                                        <entry>backup_ceph_conf</entry>
                                        <entry>The Ceph configuration file location, usually:
                                                <codeph>/etc/ceph/ceph.conf</codeph>.</entry>
                                    </row>
                                    <row>
                                        <entry>backup_ceph_user</entry>
                                        <entry>The user name value from the
                                                <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                            file, highlighted below::
                                            <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                                    </row>
                                    <row>
                                        <entry>backup_ceph_pool</entry>
                                        <entry>The pool name value from the
                                                <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>
                                            file, highlighted below::
                                            <codeblock>pools:
    - name: <b>backups</b></codeblock></entry>
                                    </row>
                                </tbody>
                            </tgroup>
                            </table></li>
                        <li>Commit your configuration to the <xref
                                href="../installation/using_git.dita">local git repo</xref>, as
                            follows:
                            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
                        <li>Run the configuration processor:
                            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                        <li>Update your deployment directory:
                            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                        <li>Run the Cinder Reconfigure Playbook:
                            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml </codeblock></li>
                    </ol></p>
            </section>
        <p>After the successful execution of the above command, the cinder-backup service is
            configured to use Ceph as data store. You can use volume lifecycle operations as
            described <xref
                href="../../operations/blockstorage/creating_voltype.dita#creating_voltype/associate_volumetype"
            />.</p>
        <!--<p>After the successful execution of the above command, the cinder-volume is configured to use Ceph as data store. You can use volume lifecycle operations as described at  <xref href="../installation_verification.dita#install_verification/volume_verify"/></p>-->
        <section id="rados-gw-object-storage"><title>Use RADOS Gateway to Access Objects Using S3/Swift AP</title>
            
                <p>RADOS Gateway provides object storage functionality through S3 and Swift API.
                Unlike the OpenStack service, which uses Keystone as its only authentication
                backend, RADOS Gateway can use its built-in authentication mechanism in addition to
                Keystone for user authentication. From an end-user perspective, the following are
                the primary channels for accessing objects (also expressed pictorially):</p>
                <p>
                    <ol>
                        <li>Keystone users using <ol id="ol_uc3_dfd_jw">
                                <li>Swift API</li>
                                <li>S3 API</li>
                            </ol></li>
                        <li>RADOS Gateway users using <ol id="ol_pf1_cfd_jw">
                                <li>Swift API</li>
                                <li>S3 API</li>
                            </ol></li>
                    </ol>
                </p>
                <p>Pictorial view of above perspective:</p>
                <p><image href="../../../media/ceph/ceph/rgw.jpg" id="image_lf4_tmt_jw"/></p>
                <p>Example 1 uses Keystone as the authentication backend while example 2 uses RADOS
                Gateway (internal authentication backend). The default deployment of RADOS Gateway
                in HPE Helion OpenStack 3.0 enables (1.a), (2.a) and (2.b) only.</p>
                <p>Enabling Keystone users to access the S3 API (option 1.b) requires Keystone to be
                configured as the default authentication backend. In this configuration, user
                authentication requests are first routed to Keystone, then on to RADOS Gateway as a
                secondary authority. All authentication requests will follow this pattern, whether
                the user in question is a Keystone or non-Keystone user (i.e., use case 2.b) Please
                note that this authentication pattern will cause serious performance degredation
                when RADOS Gateway users attempt to authenticate, due to the request being routed
                first to Keystone, then on to RADOS Gateway. </p>
                <p>Our observations, based on lab tests performed on 10 TB Ceph clusters, is that
                enabling S3 API access for Keystone users degrades the performance of S3 API access
                for RADOS Gateway users by roughly 50%. We ran rest-bench tool for 10 seconds with
                concurrency 16 and object size 4096 (4k) bytes on a Ceph cluster deployed using the
                    <codeph>entry-scale-kvm-ceph</codeph> input model. Our lab finding is that the
                system performed an average of 1350 write operations (object size 4K) when keystone
                was enabled as the default authentication backend. This is in contrast to 2500 write
                operations (object size 4K) with a system running the default configuration. This
                illustrates a roughly 49% degradation. Please note that the numbers presented here
                are for illustration purposes only, and are meant to provide  insight into the
                degree of degradation involved in a particular configuration. </p>
            <p>In spite of this drawback, this configuration can reduce the administrative overhead
                of user management. Since all authentication requests are initially routed to
                Keystone, all user accounts can be created and managed through that service. In this
                case, a database of credentials specific to the S3 authentication is not necessary.
                Standard administrative tools like Horizon can be used instead. Please note that
                option (1.b) and its related configuration is not necessary if you do not intend to
                enable S3 API access for Keystone users. </p>
                <p><b>Steps to Access S3 API for RADOS Gateway User</b></p>
                <p>Perform the following:<ol id="ol_asx_xfd_jw">
                        <li>Create a user by executing the following command:
                                <codeblock>sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}" --email="{email}"</codeblock><p>Example:<codeblock>sudo radosgw-admin user create --uid=rgwuser --display-name="RadosGatewayUser" --email=rgwuser@test.com</codeblock></p></li>
                        <li>Perform object operation. You can use S3 clients. Example:
                                <codeph>python-boto</codeph>.<!-- Should there be an object operation example here? --></li>
                    </ol></p>
                <p><b>Steps to Access Swift API for RADOS Gateway User</b></p>
                <p>Perform the following:</p>
                <p>
                    <ol id="ol_up5_2gd_jw">
                        <li>Create a user by executing the following command:
                                <p><codeblock>sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}" --email="{email}"</codeblock>
                                Example: </p><p>
                                <codeblock>sudo radosgw-admin user create --uid=rgwuser --display-name="RadosGatewayUser" --email=rgwuser@test.com</codeblock>
                            </p></li>
                        <li>Create sub-user by executing the following command:  <p>
                                <codeblock>sudo radosgw-admin subuser create --uid={username} --subuser={username}:{subusername} --access=full --key-type=swift --gen-secret</codeblock>
                            </p><p> Example: </p><p>
                                <codeblock>sudo radosgw-admin subuser create --uid="rgwuser" --subuser="rgwuser:rgwswiftuser" --access=full --key-type=swift --gen-secret</codeblock>
                            </p></li>
                        <li>Perform object operation using Swift CLI or any compatible clients.</li>
                    </ol>
                </p>
                <p><b>Steps to Access Swift API for Keystone User</b></p>
                <p>When RADOS Gateway is deployed as per the default
                    <codeph>entry-scale-kvm-ceph</codeph> model, it exists alongside OpenStack Swift
                (since the Keystone service type for RADOS Gateway defaults to ceph-object-store).
                In this (co-existing) mode, OpenStack Horizon communicates only with OpenStack Swift
                for all the Object Store operations. However, the OpenStack Swift command line
                interface can be tuned to communicate with both OpenStack Swift and RADOS Gateway as
                    below:<ul id="ul_p4x_4zd_jw">
                    <li><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?># to
                        interact with Swift (default)
                        <codeblock><codeph>export OS_SERVICE_TYPE=object-store</codeph></codeblock></li>
                    <li># to interact with RADOS
                        Gateway<?oxy_custom_end?><codeblock> <codeph>export OS_SERVICE_TYPE=ceph-object-store</codeph></codeblock></li>
                </ul></p>
                <p> One can use the Swift CLI to perform object operations on Ceph.</p>
            <p><b>Example</b>:</p>
            <p>In the following example we login to to the monitor node to list the objects in Ceph
                using the credentials available in <codeph>~/service.osrc</codeph>
                file.<codeblock>source ~/service.osrc</codeblock></p>
            <p>Execute the following command to list objects in Ceph object store:</p>
            <p>
                <codeblock>export OS_SERVICE_TYPE=ceph-object-store
swift list</codeblock>
            </p>
            <p>Execute the following command to list objects in Swift object
                store:<codeblock>export OS_SERVICE_TYPE=object-store
swift list</codeblock></p>
            <p>
                <b>Steps to Access S3 API for Keystone User</b>
            </p>
                <p>As mentioned above, S3 API access for keystone user is not enabled by default. It
                requires fthe ollowing sets of changes:<ol id="ol_sxv_rky_jw">
                    <li>Configure RADOS Gateway to use keystone for authenticating S3 requests.</li>
                    <li>Change RADOS Gateway endpoints to use port 35357 in
                            <codeph>ceph.conf</codeph> of RADOS Gateway. Ensure that iptables rules
                        are configured to allow access over port 35357. </li>
                    <li>Create EC2 credentials.<p>Execute the following commands to generate the EC2
                            credentials for the OpenStack
                            user:<pre>openstack ec2 credentials create --project &lt;project-name&gt; --user &lt;user-name&gt;</pre></p><p>For
                            more information, refer to OpenStack documentation.</p><p>
                            <note type="important">Please contact Professional Services team for
                                details on how to perform above steps. </note>
                        </p></li>
                </ol></p>
            
        </section>
    </body>
</topic>
