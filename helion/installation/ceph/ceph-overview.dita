<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Overview</title>
    <abstract>
        <shortdesc outputclass="hdphidden">ceph overview.</shortdesc>
    </abstract>
    <body>
        <!--not tested. Should we define RBD? Also, let's include Ceph's block storage ability.-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section>
            <p>Ceph is a distributed object storage solution which can horizontally scale up to
                multiple petabytes of storage and is fundamentally based on a reliable autonomic
                distributed object store (RADOS) object-based storage system.</p>
            <p><keyword keyref="kw-hos-phrase"/> supports and utilizes the Firefly version of
                Ceph.</p>
            <p>Ceph stores all data as objects, however, it provides various optional components
                which act as gateways to support different storage protocols. For example, RBD
                (RADOS Block Device) provides support for the block storage protocol and the RADOS
                Gateway provides support for the S3/Swift API protocol for object storage
                access.</p>
            <p>The salient features of Ceph are:</p>
            <ul>
                <li>Hardware Agnostic - Use of any commodity hardware is supported.</li>
                <li>Scalable - Scales horizontally up to multiple petabytes (there is no theoretical
                    limit).</li>
                <li>Self healing, self managing.</li>
                <li>High Availability - No single point of failure.</li>
                <li>Supports various client protocols for block device access, object storage access
                    using S3/Swift API etc.</li>
            </ul>
            <p>Ceph is flexible in its deployment. Though all data is stored as objects, Ceph can be
                configured with optional components that provide access to the data through multiple
                storage protocols. The addition of such components should only be done if use of the
                related storage protocol is necessary. For example, if you are not going to support
                the S3/Swift protocol then deployment of the RADOS gateway is not necessary.</p>
            <p>For easy readability, this documentation is categorized into sections that segregate
                the various aspects of a Ceph cluster deployment. For instance, the topics
                describing hardware configuration, service configuration parameters, and deployment
                architecture are categorized into separate sections. In keeping with this
                sub-component organization, each major section is further categorized into the
                following subsections:</p>
            <ul>
                <li>Core Ceph</li>
                <li>RADOS gateway</li>
            </ul>
            <p>Also, in each section we describe recommended vs. supported configurations. We
                encourage the use of our recommended configurations whenever possible, although the
                supported configurations are fully tested and validated. The supported configuration
                should be used in place of the recommended configuration only when necessary, and a
                thorough assessment of the benefits and drawbacks of each option should be performed
                before choosing a supported configuration rather than a recommended one.</p>
            <p><b>Core Ceph</b></p>
            <p>Core Ceph is comprised of two primary components: Object Storage Daemon (OSD) and
                Ceph monitor. These components are mandatory for the functioning of the cluster. The
                following table provides a brief description of the components.</p>
            <p>
                <table frame="all" rowsep="1" colsep="1" id="ceph1">
                    <tgroup cols="2">
                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                        <colspec colname="c2" colnum="2" colwidth="3.88*"/>
                        <thead>
                            <row>
                                <entry>Components</entry>
                                <entry>Description</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry><b>Object Storage Daemon (OSD)</b></entry>
                                <entry>A Ceph Object Storage Daemon (OSD) stores data, handles data
                                    replication, recovery, backfilling, rebalancing, and provides
                                    some monitoring information to Ceph Monitors by checking other
                                    Ceph daemons for a
                                        heartbeat.<p><!--The default <keyword keyref="kw-hos-tm"/>configuration makes three copies of your data (but it can be adjusted).--></p></entry>
                            </row>
                            <row>
                                <entry><b>Ceph Monitor</b></entry>
                                <entry>A Ceph Monitor maintains maps of the cluster state including
                                    the monitor map, the OSD map, the placement group (PG) map, and
                                    the CRUSH map. It also maintains a history (called an "epoch")
                                    of each state change in the Ceph Monitors, Ceph OSD daemons, and
                                    PGs.</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
            </p>
            <p><b>RADOS Gateway</b></p>
            <p>The RADOS Gateway service is an object storage interface that allows an end user to
                perform HTTP-based CRUD operations on an object. It supports both the OpenStack
                Swift and Amazon S3 REST APIs. Note that unlike other OpenStack services which rely
                solely on the Keystone component for user management, RADOS Gateway uses the
                following users and user management services: (see more details at <xref
                    href="usage_ceph_storage_helion.dita#config_ceph/rados-gw-object-storage">Use
                    RADOS Gateway to access objects using S3/Swift API</xref> ).</p>
            <ul>
                <li>Keystone</li>
                <li>RADOS Gateway user (managed by Ceph itself and does not require Keystone)</li>
            </ul>
            <p>RADOS Gateway is an optional component. We advise deploying RADOS Gateway only if you
                need to access your object storage using the Swift or S3 API. Features of RADOS
                Gateway are:</p>
            <ul>
                <li>RADOS Gateway is configured to run in a simple (non-federated or single region)
                    mode. </li>
                <li>The HAProxy on the <keyword keyref="kw-hos-tm"/> controller node acts as a load
                    balancer (in least connection mode, the load balancer selects the server with
                    the least number of connections) for RADOS Gateway servers. </li>
                <li>Provides OpenStack Keystone integration (users with the appropriate roles can
                    access Swift APIs served by <codeph>radosgw</codeph>). </li>
                <li>The default <keyword keyref="kw-hos-tm"/> configuration installs RADOS Gateway
                    on standalone nodes. </li>
                <li>Amazon S3 APIs can be accessed only by RADOS Gateway users. </li>

                <li>The RADOS Gateway external and internal endpoints are SSL/TLS enabled including
                    the public endpoints represented by HAProxy.<p> </p></li>
            </ul>
        </section>
        <section>
            <title>Deployment Architecture</title>
            <p>The following points needs to be considered for Ceph deployment:</p>
            <ul>
                <li>Ceph networking </li>
                <li>Placement of service components (like OSD, monitor, RADOS Gateway) across nodes.
                    For example, the RADOS Gateway and monitor components can be deployed on
                    standalone nodes or together on the same node</li>
            </ul>
            <p id="ceph-networking"><b>Ceph networking</b></p>
            <p>Rather than routing requests to a specific gateway for storage operations, Ceph
                clients transmit traffic directly to OSD daemons. OSD daemons perform data
                replication and participate in recovery activities. In general, a Ceph storage pool
                is configured to maintain 3 replicas of any stored object. This can result in a
                large amount of network traffic as the daemons maintain up-to-date versions of all
                the object replicas. A single transaction on an object in a Ceph storage pool
                results in 3 times the initial transaction traffic as the 3 replicas are kept in a
                consistent state. Therefore, a 4 MB write transaction results in 12 MB (4MB * 3
                replicas) of network data movement in the Ceph clusters. In addition, a Ceph cluster
                is communicative by design, resulting in an abundance of network traffic.
                Considering these points, it is very important to segregate the different types of
                Ceph network traffic. The Ceph data traffic is grouped into three main
                categories:</p>
            <ul>
                <li>Management traffic: monitoring, logging etc </li>
                <li>Client traffic (often termed as data traffic): client requests sent to OSD
                    daemons </li>
                <li>Cluster traffic (often termed as replication traffic): data replication and
                    recovery traffic among OSD daemons.</li>
            </ul>
            <p>For a high performance cluster, proper network configuration is very important. We
                recommend segregating the different categories of traffic on separate networks. For
                a cluster of reasonable size (a few TBs), the use of at least two networks is
                recommended, i.e., a single network for management and client data traffic
                (front-side) and a second network for cluster traffic (back-side). For a large Ceph
                cluster, we recommended segregating each traffic category onto its own network.
                Segregating network traffic also provides for a more secure environment, as the
                network used for Ceph cluster traffic does not require direct internet connectivity.
                Apart from the virtual separation of networks (using VLANs), it's important to
                consider the physical NICs used for Ceph servers as well. We strongly recommend the
                use of bonded NICs to provide redundancy and prevent a single point of failure.
                Also, please note the difference between network separation through the use of VLANs
                and through the use of separate NICs. In a multi-network model, network segregation
                through VLANs should be prioritized and ideally complimented by NIC separation.
                Please see below for our recommended order of priority for the method of network
                segregation:</p>
            <ol>
                <li>Separation of VLAN</li>
                <li>Separation of NIC for VLANs.</li>
            </ol>
            <p>The network architecture that you choose will depend on how many NICs you have. As
                stated earlier, we recommend using bonded interfaces when possible. The following
                configuration examples may help you choose your preferred setup:</p>
            <ul>
                <li>6 NICs configured as 3 bonded interfaces: The first bonded interface for the
                    management traffic VLAN, the second for OSD client traffic VLAN, and the third
                    for OSD internal traffic VLAN (ideally
                    preferred<!--, will done by very few customers hosting very large ceph clusters-->) </li>
                <li>4 NICs configured as 2 bonded interfaces: The first bonded interface for the
                    Management traffic VLAN, the second interface for the Client traffic and Cluster
                    traffic VLANs (mostly
                    used<!-- practical pattern followed. I'm unclear on which networks share an interface-->) </li>
                <li>2 NICs configured as one bonded interface: Interface used for all VLANs
                    (<!--pattern followed in cases where we have--> for a few NICs only)</li>
            </ul>
            <p><keyword keyref="kw-hos-tm"/>Ceph software offers significant flexibility when
                defining and deploying OpenStack based clouds, and provides the ability to implement
                a wide variety of configurations. This allows you to design, model and deploy a
                cloud based on your requirements. Based on your needs and limitations, one of the
                following network architectures can be implemented:</p>
            <ul>
                <li>Single VLAN deployment: A single VLAN is used for all traffic. Primarily
                    implemented on a small cluster. </li>
                <li>Two VLAN deployment: One VLAN is used for cloud management and client traffic
                    and another VLAN is used for Ceph internal
                    traffic.<!--I'm unclear on which traffic should share a VLAN. There doesn't seem to be a consistent naming convention for the traffic types--></li>
                <li>Three VLAN deployment: Separate VLANs are used for management, client, and
                    internal traffic.</li>
            </ul>
            <p>As mentioned above, you may choose to make a single interface a member of all VLANs,
                or keep each interface isolated to a single VLAN. You may also choose a combination
                of the above options. For a large cluster, you can use a separate bonded interface
                for each VLAN, which in turn necessitates the need to have at least 6 NICs for OSD
                nodes and 4 NICs for monitor nodes.</p>
            <p><b>Placement of service component</b></p>
            <p>Although there are multiple options for the placement of the service components, this
                section details only the recommended configuration. The section explains placement
                of the following components:</p>
            <ul>
                <li>Core Ceph (i.e. OSD and monitors)</li>
                <li>Rados Gateway </li>
            </ul>
            <p>For details on alternative supported deployment architecture, please refer to <xref
                    href="#config_ceph/alternative-supported-architecture" format="dita">Alternative
                    supported architecture</xref>.</p>
            <p><b>Core Ceph</b></p>
            <p>The following deployment architecture is recommended to avoid a single point of
                failure for a Ceph cluster.</p>
            <ul>
                <li>Three monitor nodes (to retain odd number criteria of monitor quorum).</li>
                <li>At least three OSD nodes. This ensures that any storage object is replicated on
                    three separate physical nodes, as long as the replica count of the Ceph storage
                    pool is set to three. The recommended configuration for the storage pool is to
                    set the replica count to three. </li>
            </ul>
            <p>As mentioned in <xref href="#config_ceph/ceph-networking" format="dita">Ceph
                    networking</xref>, the use of separate VLANs to segregate Ceph traffic types is
                recommended. In this type of architecture, a Cloud management network is used for
                logging and/or monitoring, an OSD client network is used for Ceph client traffic,
                and an OSD internal network is used for internal Ceph traffic, such as
                replication.</p>
            <p><b>Rados Gateway</b></p>
            <p>The recommended configuration involves deploying at least two RADOS Gateway instances
                on standalone nodes. Access to these instances is provided by a HAProxy front end.
            </p> The following diagram illustrates the physical architecture of RADOS Gateway and it
            reflects the <codeph>entry-scale-kvm-ceph</codeph> configuration. <p><image
                    href="../../../media/ceph/ceph/physical-architecture_ceph.jpg"
                    id="image_m25_12x_lw"/></p><p>The default example model
                    (<codeph>entry-scale-kvm-ceph</codeph>) is the recommended configuration for
                deployment of a Ceph cluster with RADOS Gateway.</p>
            <p><b>Ceph deployment architecture</b></p>
            <p>The following diagram illustrates the deployment architecture of Ceph.</p>
            <p><image href="../../../media/ceph/ceph/ceph_rgw_architecture.png"
                    id="image_t5j_lzp_kw"/></p>
            <p>The above diagram illustrates the Ceph deployment scenario of
                    <codeph>entry-scale-kvm-ceph input model</codeph>. Monitors are deployed on
                three controller nodes. Three standalone nodes are used for OSD, and two standalone
                nodes are used for RADOS Gateway, with access provided by HAProxy front ends running
                on controller nodes. Management, client, and internal data traffic are separated
                through the use of independent VLANs for each traffic type.</p>
        </section>
        <section id="alternative-supported-architecture">
            <title>Alternative supported architecture</title>
            <p><keyword keyref="kw-hos-version-30"/> also supports an alternate deployment
                architecture. Consider the following points for alternate supported architecture
                    deployment:<ul id="ul_asz_bqq_kw">
                    <li>Core networking</li>
                    <li>Placement of service components<ul id="ul_hst_dqq_kw">
                            <li>Core Ceph<ul id="ul_p13_2qq_kw">
                                    <li>Monitor on standalone node</li>
                                </ul></li>
                        </ul><ul id="ul_dym_lst_fw">
                            <li>RadosGateway <ul id="ul_tjm_nst_fw">
                                    <li>RadosGateway on dedicated monitor nodes </li>
                                    <li>RadosGateway on controller nodes</li>
                                </ul></li>
                        </ul></li>
                </ul></p>
            <p><b>Core networking</b></p>
            <p>Rather than routing requests to a specific gateway, Ceph clients transmit traffic
                directly to OSD daemons for storage operations. For more information refer to <xref
                    href="#config_ceph/ceph-networking" format="dita"> Ceph networking</xref>.</p>
            <p><b>Placement of service components</b></p>
            <p><b>Core Ceph</b></p>
            <p>The architecture choices supported in <keyword keyref="kw-hos-tm"/> 3.0 are as
                follows:</p>
            <ul>
                <li>Monitor on standalone node - You can deploy the Ceph monitor service on a
                    dedicated cluster or resource node(s). Ensure you modify your environment after
                    installing the lifecycle manager. For more details, refer to <xref
                        href="alternative-supported-choice.dita#config_ceph/deploying-monitor-on-standalone-node"
                        >Install a monitor service on a dedicated resource node</xref>.</li>
            </ul>
            <p><b>RADOS Gateway</b></p>
            <p>RADOS Gateway service can be co-hosted with other <keyword keyref="kw-hos-tm"/>
                services as listed below.</p>
            <ul>
                <li>Alternate RADOS Gateway deployment architecture choice <ul id="ul_jjw_bsq_kw">
                        <li>Co-hosted on cluster nodes hosting monitor service components</li>
                        <li>Co-hosted on controller nodes</li>
                    </ul>
                    <p>The default <codeph>entry-scale-kvm-ceph</codeph> input model deploys the
                            <codeph>radosgw</codeph> services on 2 dedicated cluster nodes. However,
                        RADOS Gateway can also be installed on cluster nodes hosting Ceph monitor
                        service or on Controller nodes. Refer to <xref
                            href="alternative-supported-choice.dita#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                            >Installing RADOS Gateway on (dedicated) cluster node(s) that host Ceph
                            Monitor service</xref> or <xref
                            href="alternative-supported-choice.dita#config_ceph/install-rados-gateway-on-controller-nodes"
                            >Installing RADOS Gateway on controller nodes</xref> for more
                        details.</p><p>
                        <note>Because the RADOS Gateway service will be sharing server resources
                            with multiple services, these alternate configurations will result in
                            sub-optimal performance, as compared to the default
                            configuration.</note>
                    </p></li>
            </ul>
        </section>
        <section>
            <title>Hardware Recommendation</title>
            <p>For Hardware recommendation, refer to <xref
                    href="../../recommended_hardware_minimums.dita#rec_min/min_requirements_kvm_ceph"
                /></p>
        </section>
        <section>
            <title>Additional Resources for Ceph Configuration</title>
        </section>
    </body>
</topic>
