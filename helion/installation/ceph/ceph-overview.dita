<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Overview</title>
    <abstract>
        <shortdesc outputclass="hdphidden">ceph overview.</shortdesc>
    </abstract>
    <body>
        <!--not tested. Should we define RBD? Also, let's include Ceph's block storage ability.-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section>
            <title>Overview</title>
            <p>Ceph is a distributed object storage solution which can horizontally scale up to
                multiple petabytes of storage and is fundamentally based on a reliable autonomic
                distributed object store (RADOS) object-based storage system. <keyword
                    keyref="kw-hos-phrase"/> supports and utilizes the Firefly version of Ceph. Ceph
                stores all data as objects, however, it provides various optional components which
                act as a gateways to support different storage protocols. For example, RBD (RADOS
                Block Device) provides support for the block storage protocol, and the RADOS Gateway
                provides support for the S3/Swift API protocol for object storage access.</p>
            <p>The salient features of Ceph are:</p>
            <ul>
                <li>Hardware Agnostic - Use of any commodity hardware.</li>
                <li>Scalable - Scales horizontally up to multiple petabytes (there is no theoretical
                    limit).</li>
                <li>Self healing, self managing.</li>
                <li>High Availability - No single point of failure.</li>
                <li>Supports various client protocols for block device access, object storage access
                    using S3/Swift API etc.</li>
            </ul>
            <p>Ceph is flexible in its deployment. Though all data are stored as objects, Ceph can
                be configured with optional components that provide access to the data through
                multiple storage protocols. The addition of such components should only be done if
                use of the related storage protocol is necessary. For example: If you are not going
                to support the S3/Swift protocol then deployment of the RADOS gateway is not
                necessary. </p>
            <p>For easy readability, this documentation is categorized into sections that
                segregate<!--is categorized into sections to segregate--> the various aspects of a
                Ceph cluster deployment. For instance, the topics describing hardware configuration,
                service configuration parameters, and deployment architecture are categorized into
                separate sections. In keeping with this sub-component organization, each major
                section is further categorized into following subsections: <ul id="ul_enh_4sp_kw">
                    <li>Core Ceph</li>
                    <li>RADOS gateway</li>
                </ul></p>
            <p>Also, in each section we describe recommended vs. supported configurations. We
                encourage the use of our recommended configurations whenever possible, although the
                supported configurations are fully tested and validated. The supported configuration
                should be used in place of the recommended configuration only when necessary, and a
                thorough assessment of the benefits and drawbacks of each option should be performed
                before choosing a supported configuration rather than a recommended one.</p>
            <p><b>Core Ceph</b></p>
            <p>Core Ceph is comprised of  two primary components: OSD (Object Storage Daemon) and
                monitor. These components are mandatory for the functioning of the cluster. The
                following table provides a brief description of the components.</p>
            <p>
                <table frame="all" rowsep="1" colsep="1" id="ceph1">
                    <tgroup cols="2">
                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                        <colspec colname="c2" colnum="2" colwidth="3.88*"/>
                        <thead>
                            <row>
                                <entry>Components</entry>
                                <entry>Description</entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry><b>OSD</b></entry>
                                <entry>A Ceph Object Storage Daemon (OSD) stores data, handles data
                                    replication, recovery, backfilling, rebalancing, and provides
                                    some monitoring information to Ceph Monitors by checking other
                                    Ceph daemons for a
                                        heartbeat.<p><!--The default <keyword keyref="kw-hos-tm"/>configuration makes three copies of your data (but it can be adjusted).--></p></entry>
                            </row>
                            <row>
                                <entry><b>Monitor</b></entry>
                                <entry>A Ceph Monitor maintains maps of the cluster state including
                                    the monitor map, the OSD map, the placement group (PG) map, and
                                    the CRUSH map. It also maintains a history (called an "epoch")
                                    of each state change in the Ceph Monitors, Ceph OSD daemons, and
                                    PGs.</entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
            </p>
            <p><b>RADOS Gateway</b></p>
            <p>The RADOS Gateway service is an object storage interface that allows an end user to
                perform HTTP-based CRUD operations on an object. It supports both the OpenStack
                Swift and Amazon S3 REST APIs. Note that unlike other OpenStack services which rely
                solely on the Keystone component for user management, RADOS Gateway uses the
                following users and user management services: (see more details at <xref
                    href="usage_ceph_storage_helion.dita#config_ceph/rados-gw-object-storage">Use
                    RADOS Gateway to access objects using S3/Swift API</xref> ).</p>
            <ul>
                <li>Keystone</li>
                <li>RADOS Gateway user (managed by Ceph itself and does not require Keystone)</li>
            </ul>
            <p>RADOS Gateway is an optional component. We advise deploying RADOS Gateway only if you
                need to access your object storage using the Swift or S3 API. Features of RADOS
                Gateway are:</p>
            <ul>
                <li>RADOS Gateway is configured to run in a simple (non-federated or single region)
                    mode. </li>
                <li>The HAProxy on the <keyword keyref="kw-hos-tm"/> controller node acts as a load
                    balancer (in least connection mode, the load balancer selects the server with
                    the least number of connections) for RADOS Gateway servers. </li>
                <li>Provides OpenStack Keystone integration (users with the appropriate roles can
                    access Swift APIs served by <codeph>radosgw</codeph>). </li>
                <li>The default <keyword keyref="kw-hos-tm"/> configuration installs RADOS Gateway
                    on standalone nodes. </li>
                <li>Amazon S3 APIs can be accessed only by RADOS Gateway users. </li>
                <li>The RADOS Gateway external and internal endpoints are SSL/TLS enabled, including
                    the public endpoints represented by HAProxy.<p> </p>.</li>
            </ul>
        </section>
        <section>
            <title>Deployment Architecture</title>
            <p>The following points needs to be considered for Ceph deployment:</p>
            <ul>
                <li>Ceph networking </li>
                <li>Placement of service components (like OSD, monitor, RADOS Gateway) across nodes.
                    For example, the RADOS Gateway and monitor components can be deployed on
                    standalone nodes or together on the same node</li>
            </ul>
            <p><b>Ceph networking</b></p>
            <p>Rather than routing requests to a specific gateway for storage operations, Ceph
                clients transmit traffic directly to OSD daemons. OSD daemons perform data
                replication and participate in recovery activities. In general, a Ceph storage pool
                is configured to maintain 3 replicas of any stored object. This can result in a
                large amount of network traffic as the daemons maintain up-to-date versions of the
                object replicas. A single transaction on an object in a Ceph storage pool results in
                3 times the initial transaction traffic as the 3 replicas are kept in a consistent
                state. Therefore, a 4 MB write transaction results in 12 MB (4MB * 3 replicas) of
                network data movement in the Ceph clusters. In addition, a Ceph cluster is
                communicative by design, resulting in an abundance of network traffic. Considering
                these points, it is very important to segregate various Ceph data traffic. The Ceph
                data traffic is categorized into three segments: <ul id="ul_iqq_shn_fw">
                    <li>Management traffic: monitoring, logging etc </li>
                    <li>Client traffic (often termed as data traffic): client requests sent to OSD
                        daemons </li>
                    <li>Cluster traffic (often termed as replication traffic): data replication and
                        recovery traffic among OSD daemons.</li>
                </ul></p><p>For a high performance cluster, proper network configuration is very important. Segregate
                different categories of  traffic on separate networks. For a cluster of reasonable
                size (a few TBs), it is recommended to have a cluster with at least two networks,
                i.e., single network for management and client data traffic (front-side) and a
                cluster (back-side) network. For large Ceph cluster, it is recommended to segregate
                all three traffics. Segregating networks helps for a secure connection because a
                cluster network is not required to be connected to the internet directly. It allows
                OSD daemons to keep communicating without intervention so that placement groups can
                be brought to active and clean state relatively, whenever required. Apart from
                separation of network (using VLANs), we need to consider NICs used for Ceph servers
                too. We strongly recommend to use bonded NICs to prevent single point of failure.
                Note that the network (and hence VLAN) separation is different from NIC separation
                though linked to each other. In case of multi-network model, emphasis is on VLAN
                separation and ideally should be complimented by NIC separation as mentioned below
                in order of priority. <ol id="ol_dwc_rbl_jw">
                    <li>Separation of VLAN</li>
                    <li>Separation of NIC for VLANs.</li>
                </ol> In this case, it all depends on how many NICs one have with a preference given
                to bonded interface. So, the following guideline will help: <ol id="ol_ssd_tbl_jw">
                    <li>Three bonded interface with 6 NICs: first for management, second for OSD
                        client and third for OSD internal (ideally
                        preferred<!--, will done by very few customers hosting very large ceph clusters-->) </li>
                    <li>Two bonded interface with 4 NICs with management VLANs hooked to first
                        bonded interface while OSD networks hooked to second bonded interface
                        (mostly used<!-- practical pattern followed-->) </li>
                    <li>Only one bonded interface with two NICs for all VLANs
                        (<!--pattern followed in cases where we have--> for a few NICs only)</li>
                </ol></p><p><keyword keyref="kw-hos-tm"/>Ceph software offers significant
                flexibility when defining and deploying OpenStack based clouds. It provides the
                capability to implement a wide variety of different Ceph configurations. This allows
                you to design, model and deploy cloud based on your requirements. One can have
                following VLAN choices based on types of traffic:</p><p>
                <ol id="ol_m3x_vhn_fw">
                    <li>A single VLANs based deployment: A single VLAN is used for all traffic and
                        primarily it is meant for a small cluster. </li>
                    <li>Two VLANs based deployment: One VLAN is used for cloud management and client
                        traffic and another VLAN is used for Ceph internal traffic.</li>
                    <li>Three VLANs based deployment: A separate VLAN is used for management,
                        client, and internal traffic.<p>As mentioned above, you can decided to link
                            all VLANs to a same bonded interface or to a separate bonded interface
                            or a combination of above. For a large cluster, you can use a separate
                            bonded interface which in turn necessiates the need to have at least 6
                            NICs for OSD nodes and 4 NICs for monitor nodes.</p></li>
                </ol>
            </p>
            <p><b>Placement of service component</b></p>
            <p>Although there are multiple choice of placing service component, this section focus
                only about recommended deployment composition. The section explains deployment of
                placement of following components:<ul id="ul_qll_xbd_jw">
                    <li>Core Ceph (i.e. OSD and monitors)</li>
                    <li>Rados Gateway </li>
                </ul>For details on alternative supported deployment architecture, please refer to
                    <xref href="#config_ceph/alternative-supported-architecture" format="dita"
                    >Alternative supported architecture.</xref></p>
            <p/>
            <p>
                <ul id="ul_tyr_tnq_kw">
                    <li><b>Core Ceph</b><p>The following deployment composition is recommended to
                            avoid single point of failure for the deployment of Ceph cluster.<ol
                                id="ol_ecm_qtq_kw">
                                <li>Three monitor (to retain odd number criteria of monitor quorum)
                                    nodes </li>
                                <li>At least three OSD nodes. This ensures that the object is
                                    replicated on three separate physical nodes, if replica count of
                                    pool is set to three. The recommended configuration for the
                                    storage pool is to set replica count to three. </li>
                            </ol></p><p>As mentioned in <xref href="#config_ceph/ceph-networking"
                                format="dita">Ceph networking</xref>, it is recommend to use a
                            separate VLANS to separate various Ceph traffic. Cloud management
                            network is used for logging and/or monitoring, OSD client network is
                            used for Ceph client traffic and OSD internal network is used for
                            internal Ceph traffic like replication.</p></li>
                    <li><p><b>Rados Gateway</b></p><p>It is recommended to deploy at least two
                            instances of RADOS Gateway on standalone node
                            <?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>front
                            ended by<?oxy_custom_end?> HAProxy. </p> The following diagram
                        illustrates the physical architecture of RADOS Gateway and it reflects the
                            <codeph>entry-scale-kvm-ceph</codeph> configuration. <p><image
                                href="../../../media/ceph/ceph/physical-architecture_ceph.jpg"
                                id="image_m25_12x_lw"/></p><p>The default example model
                                (<codeph>entry-scale-kvm-ceph</codeph>) is recommended mechanism to
                            deploy Ceph cluster with RADOS Gateway. </p></li>
                </ul>
            </p>
            <p/>
            <p><b>Ceph deployment architecture</b></p>
            <p>The following diagram illustrates the deployment architecture of Ceph.</p>
            <p>
                <image href="../../../media/ceph/ceph/ceph_rgw_architecture.png"
                    id="image_t5j_lzp_kw"/></p>
            <p>The above diagram illustrates the Ceph deployment scenario of
                    <codeph>entry-scale-kvm-ceph input model</codeph>. Monitors are deployed on
                three controller node. Three standalone nodes are used for OSD and two standalone
                nodes are used for RADOS Gateway front ended by HAProxy running on controller nodes.
                And Management, client and internal data traffics are separated using independent
                VLANs.</p>
        </section>
        <section>
            <title>Alternative supported architecture</title>
            <p><keyword keyref="kw-hos-tm"/> 3.0 also supports an alternate deployment architecture.
                Consider the following points for alternate supported architecture deployment.<ul
                    id="ul_asz_bqq_kw">
                    <li>Core networking</li>
                    <li>Placement of service components<ul id="ul_hst_dqq_kw">
                            <li>Core Ceph<ul id="ul_p13_2qq_kw">
                                    <li>Monitor on standalone node</li>
                                </ul></li>
                        </ul><ul id="ul_dym_lst_fw">
                            <li>RadosGateway <ul id="ul_tjm_nst_fw">
                                    <li>RadosGateway on dedicated monitor nodes </li>
                                    <li>RadosGateway on controller nodes</li>
                                </ul></li>
                        </ul></li>
                </ul></p>
        </section>

        <section><b>Core networking</b><p>Ceph clients transmits traffic direct to OSD daemons for
                storage operations instead of client routing request to a specific gateway. For more
                information refer to <xref href="#config_ceph/ceph-networking" format="dita">Ceph
                    networking</xref>.</p></section>
        <p/>
        <section><b>Placement of service components</b><p>
                <ul id="ul_edw_xql_hw">
                    <li><b>Core Ceph</b><p>The architecture choices which is supported in <keyword
                                keyref="kw-hos-tm"/> 3.0 is as follows.<ul id="ul_fnt_wtq_kw">
                                <li>Monitor on standalone node<p>You can deploy the Ceph monitor
                                        service on a dedicated cluster or resource node (s). Ensure
                                        you modify your environment after installing the lifecycle
                                        manager. For more details, refer to <xref
                                            href="alternative-supported-choice.dita#config_ceph/deploying-monitor-on-standalone-node"
                                            >Install a monitor service on a dedicated resource
                                            node</xref>. </p></li>
                            </ul></p></li>
                </ul>
                <ul id="ul_r3n_nrq_kw">
                    <li><b>RADOS Gateway</b><p>RADOS Gateway service can be co-hosted with other
                                <keyword keyref="kw-hos-tm"/> services as listed below.</p><p>
                            <ul>
                                <li>Alternate RADOS Gateway deployment architecture choice <ul
                                        id="ul_jjw_bsq_kw">
                                        <li>Co-hosted on cluster nodes hosting monitor service
                                            components</li>
                                        <li>Co-hosted on controller nodes</li>
                                    </ul>
                                    <p>The default <codeph>entry-scale-kvm-ceph</codeph> input model
                                        deploys the <codeph>radosgw</codeph> services on 2 dedicated
                                        cluster nodes. However, RADOS Gateway can also be installed
                                        on cluster node hosting Ceph monitor service or on
                                        Controller nodes. Refer to <xref
                                            href="alternative-supported-choice.dita#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                                            >Installing RADOS Gateway on (dedicated) cluster node(s)
                                            that host Ceph Monitor service</xref> or <xref
                                            href="alternative-supported-choice.dita#config_ceph/install-rados-gateway-on-controller-nodes"
                                            >Installing RADOS Gateway on controller nodes</xref> for
                                        more details.</p><p>
                                        <note>Because the RADOS Gateway service will be sharing
                                            server resources with multiple services, these alternate
                                            configurations will result in sub-optimal performance,
                                            as compared to the default configuration.</note>
                                    </p></li>
                            </ul>
                        </p></li>
                </ul>
            </p></section>
        <p/>
        <section>
            <title>Hardware Recommendation</title>
            <p>For Hardware recommendation, refer to <xref
                    href="../../recommended_hardware_minimums.dita#rec_min/min_requirements_kvm_ceph"
                /></p>
        </section>
    </body>
</topic>
