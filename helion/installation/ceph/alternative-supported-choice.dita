<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Alternative Supported Choices</title>
    <abstract>
        <shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
            backend.</shortdesc>
    </abstract>
    <body>
        <!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section>
            <p>This section provides insight on how one can alter the
                    <codeph>entry-scale-kvm-ceph</codeph> input model to deploy Ceph with various
                other supported options. It is recommended that you evaluate all pros and cons
                before selecting a supported configuration in place of a recommended one. The
                various configuration choices can impact the performance and scale of your cluster.
                For any technical details, please consult the technical support team. The
                configuration choices detailed here are for reference purposes, and you can combine
                one or more of them to suit your needs. The content is categorized in the following
                ways:</p>
            <ul>
                <li>Core Ceph <ul>
                        <li><xref href="#config_ceph/deploying-monitor-on-standalone-node"
                                format="dita">Installing monitor on standalone node</xref></li>
                        <li><xref href="#config_ceph/single-vlan-for-all-ceph-traffic" format="dita"
                                >Using Single VLAN for all Ceph traffic (management, client and
                                internal OSD)</xref></li>
                        <li><xref href="#config_ceph/using-two-vlan" format="dita">Using Two VLANs
                                for Ceph (one for (management, client) and another for (internal
                                OSD) traffic)</xref></li>
                    </ul>
                </li>
                <li>RADOS Gateway <ul>
                        <li><xref
                                href="#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                                format="dita">Installing RADOS Gateway on Dedicated Monitor Cluster
                                Node</xref></li>
                        <li><xref href="#config_ceph/install-rados-gateway-on-controller-nodes"
                                format="dita">Installing RADOS Gateway on Controller
                            Node</xref></li>
                        <li><xref href="#config_ceph/install-more-two-rados-gateway-servers"
                                format="dita">Installing More Than Two RADOS Gateway as Standalone
                                Node</xref></li>
                    </ul>
                </li>
                <li><xref href="#config_ceph/managing-ceph-cluster-post-deployment" format="dita"
                        >Managing Ceph Cluster Post Deployment</xref></li>
            </ul>
        </section>
        <section>
            <title>Core Ceph</title>
        </section>
        <section id="deploying-monitor-on-standalone-node"><b>Installing Monitor on Standalone
                Node</b>
            <p>This section provides the procedure for installing monitor on a standalone node
                instead of installing on a controller node as mentioned in
                    <codeph>entry-scale-kvm-ceph</codeph>.</p><p>
                <!--In the <keyword keyref="kw-hos-phrase"/> example configurations, the Ceph monitor service is installed on the controller nodes by default. If you wish to break these out into their own cluster then you can do so by modifying the input model to form a separate cluster.-->
                <note type="attention">If you want to install the monitor service as a dedicated
                    resource node, then you must decide prior to the deployment of Ceph. <keyword
                        keyref="kw-hos-phrase"/> does not support deployment transition. Once Ceph
                    is deployed, you cannot migrate the monitor service from a controller to
                    dedicated resource nodes. </note>
            </p><!--<p><b>Architecture Diagram</b></p><p>The following architecture diagram illustrates the deployment scenarios of monitor standalone node.</p><p>&lt;need diagram></p>--><p><b>Prerequisite</b></p><p><!--The lifecycle manager must be set up before starting Ceph deployment. For more details on the installation of the lifecycle manager, see <xref href="../install_entryscale_kvm.dita#install_kvm"/>.--></p><p>Perform
                the following procedure to install the Ceph monitor on dedicated nodes. Note that
                Ceph requires at least 3 monitoring servers to form a cluster in case of node
                    failure.<ol id="ol_sys_n3l_lw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to
                            <codeph>~/helion/my_cloud/definition</codeph> directory before beginning
                        the edit
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Edit the <codeph>control_plane.yml</codeph> file to create a new cluster,
                        e.g. <codeph>ceph-mon</codeph> as shown
                        below.<codeblock>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...
                                    
  <b>- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</b>
    
  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file
                        to define the Ceph monitor node (monitor services). The following example
                        shows three nodes for monitor services. It is <b>recommended</b> to use an
                        odd number of monitor
                        nodes.<codeblock># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</codeblock></li>
                    <li>Edit the
                            <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                        file to define a new network interface set for your Ceph monitors as shown below.<p>
                            <codeblock>## This defines the interface used for management
## traffic like logging, monitoring, etc.
- name: CEP-MON-INTERFACES
  network-interfaces:
    - name: BOND0
      device:
          name: bond0
      bond-data:
          options:
              mode: active-backup
              miimon: 200
              primary: hed1
          provider: linux
          devices:
            - name: hed1
            - name: hed2
      network-groups:
        - MANAGEMENT

- name: RGW-INTERFACES
  network-interfaces:
  ...</codeblock>
                        </p><!--<p><b>Two-network Ceph example:</b></p><codeblock>    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This defines the interface used for internal
      ## cluster communication among OSD nodes.
        - name: HETH4
          device:
              name: hed4
          network-groups:
            - OSD</codeblock> <b>Single-network Ceph example:</b> <codeblock>    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This interface is used to connect the client
      ## node to the Ceph nodes so that any Ceph client
      ## like cinder-volume can route data directly to
      ## Ceph over thisinterface.
        - name: ETH2
          device:
              name: hed3
          network-groups:
            - OSD-CLIENT</codeblock>--></li>
                    <li>Edit
                            <codeph>~/helion/my_cloud/definition/data/disks_ceph_monitor.yml</codeph>
                        to define a disk model for monitor nodes.
                        <!--Create a new file named <codeph>disks_ceph_monitor.yml</codeph> in the <codeph>~/helion/my_cloud/definition/data/</codeph> directory which will define the disk model for your Ceph monitors. You can use the <codeph>disks_rgw.yml</codeph> file as a base and then edit to match your environment: --><codeblock>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5
                            
  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root
                            
      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/server_roles.yml</codeph>
                        file to define a new server role for your Ceph monitors:
                        <codeblock>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</codeblock></li>
                    <li>Commit your
                        configuration:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
                    <li>Run the following playbook to add your nodes into Cobbler:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
                    <li>To reimage all the nodes using PXE, run the following playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
                    <li>Run the configuration processor:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory by running the ready-deployment playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Deploy these changes:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
                </ol></p></section>
        <section id="single-vlan-for-all-ceph-traffic">
            <b>Using a Single VLAN for all Ceph Traffic (management, client, and internal)</b>
            <p>You can use a single VLAN to transmit all Ceph traffic. This configuration is
                recommended for a small cluster deployment.</p>
            <!--<p><b>Architecture diagram</b></p> <p>&lt;need diagram></p>-->
            <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-single-network</b>--></p><p>Perform
                the following steps to to configure Entry-scale-kvm-ceph-single-network. </p><p>
                <ol id="ol_d5k_4ss_lw">
                    <li> Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to
                            <codeph>~/helion/my_cloud/definition</codeph> directory before beginning
                        the edit
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Validate that NIC interfaces are correctly specified in
                            <codeph>nic_mapping.yml</codeph> for servers that are used in the
                        cloud.</li>
                    <li>Ensure that there are at least two NICs per Ceph node in order to create
                        bonded interfaces on each node.</li>
                    <li>Validate that your servers are mapped to a correct NIC interface
                        specification in <codeph>servers.yml</codeph>.
                        <!--A server meeting the criteria of at least two NIC is good enough for this input model-->.
                        An example of a server node used for OSD deployment is shown
                        below:<codeblock># Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                    <li>The management network is used for all Ceph traffic, therefore the
                        OSD-INTERNAL and OSD-CLIENT network groups are not required. Delete these
                        network groups from <codeph>network_groups.yml</codeph>. </li>
                    <li>Define <codeph>net_interfaces.yml</codeph> to use only the management
                        network group as shown
                        below.<codeblock>  - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT 
                                    
    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT

    - name: RGW-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT</codeblock></li>
                    <li>Delete the reference to OSD-INTERNAL and OSD-CLIENT from
                            <codeph>server_groups.yml</codeph>. </li>
                    <li>Delete VLAN information for OSD-INTERNAL-NET and OSD-CLIENT-NET from
                            <codeph>networks.yml</codeph>. Only the Management VLAN is used.</li>
                    <li>After your configuration files are set up, perform the steps from
                            <b>8-13</b> mentioned in <xref
                            href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                            >Deploying monitor on standalone node</xref>.</li>
                </ol>
            </p>
            <p id="using-two-vlan"><b>Using two VLANs: one for (management, client) and another for
                    (internal OSD) traffic</b></p>
            <p>Your environment can be configured to use two VLANs to transmit Ceph traffic. In this
                configuration one VLAN is used to transmit management and client traffic and the
                other VLAN is used to transmit internal OSD traffic. Two bonded interfaces are used
                in this configuration, comprised of four total NICs. Each bonded interface is a
                member of a unique VLAN. This configuration has the following characteristics:<ul
                    id="ul_ypz_qgy_lw">
                    <li>Use of two networks i.e., VLANs </li>
                    <li>Use of separate bonded interfaces for each VLAN (this is different than what
                        is provided in <codeph>entry-scale-kvm-ceph</codeph>) </li>
                </ul></p>
        </section>
        <p>The use of separate NICs for each VLAN provides segregation of traffic at the interface
            level and requires your server to have at least 4 physical network ports (2 ports per
            bonded interface). However, this configuration is not required. It's possible to use a
            single bonded interface (a server with only two physical network ports) for Ceph
            deployment.</p>
        <!--<p><b>Architecture diagram </b></p><p>&lt;need diagram></p>-->
        <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-dual-network</b>--></p>
        <p>Perform the following steps to to configure Entry-scale-kvm-ceph-dual-network. </p>
        <p>
            <ol id="ol_cxs_5ts_lw">
                <li>Log in to the lifecycle manager.</li>
                <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to
                        <codeph>~/helion/my_cloud/definition</codeph> directory before beginning the
                    edit
                    process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                <li>Validate that NIC interfaces are correctly specified in
                        <codeph>nic_mapping.yml</codeph> for servers that are used in the cloud. For
                    Ceph OSD nodes, four port servers are required for this configuration. You can
                    use <b>HP-DL360-4PORT</b> as it is defined in <codeph>nic_mapping.yml</codeph>
                    of <codeph>entry-scale-kvm-ceph</codeph> or define a new NIC mapping (as shown
                    below) for new sets of servers having four
                    ports.<codeblock> - name: HP-4PORT-SERVER
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"

        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"

        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"</codeblock></li>
                <li>Modify OSD nodes to use four port servers as shown below. Change the nic-mapping
                    attribute from <b>HP-DL360-4PORT</b> to use any other name defined in
                        <codeph>nic_mapping.yml</codeph>.<codeblock> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>network_groups.yml</codeph>. Please note that
                    there is no dedicated network group for client traffic. In this configuration,
                    the Management network group is used for client traffic.</li>
                <li>Edit <codeph>net_interfaces.yml</codeph> with bonded NIC as shown
                    below.<codeblock> - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
        - name: BOND1
          device:
              name: bond1
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - OSD-INTERNAL

    - name: RGW-INTERFACES
     network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>server_groups.yml</codeph>. </li>
                <li>Delete VLAN information for OSD-CLIENT-NET from <codeph>networks.yml</codeph>.
                    In this configuration, the Management VLAN is used for client traffic. </li>
                <li>After your configuration files are set up, perform the steps from <b>8-13</b>
                    mentioned in <xref href="#config_ceph/deploying-monitor-on-standalone-node"
                        format="dita">Installing Monitor on Standalone Node</xref>.</li>
            </ol>
        </p>
        <section>
            <title>RADOS Gateway</title>
        </section>
        <section id="install-rados-gateway-on-cluster-node-that-host-ceph-monitor">
            <b>Installing RADOS Gateway on Dedicated Cluster Node(s) that Host Ceph Monitor
                Service</b>
            <p>The RADOS Gateway can be installed on dedicated cluster node(s) hosting the Ceph
                Monitor service as follows:</p>
            <ol id="ol_hfp_q22_sv">
                <li>Remove the sections for servers in
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file having
                        <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following lines to the <codeph>service-components</codeph>
                    section for the cluster node(s) having the <codeph>server-role:
                        MON-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw
- apache2</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines the RADOS
                    Gateway network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>After your configuration files are set up, perform the steps from <b>8-13</b>
                    mentioned in <xref href="#config_ceph/deploying-monitor-on-standalone-node"
                        format="dita">Deploying monitor on standalone node</xref>. </li>
            </ol></section>
        <section id="install-rados-gateway-on-controller-nodes">
            <b>Installing RADOS Gateway on Controller Nodes </b>
            <p>The RADOS Gateway can be installed on the controller nodes. Perform the steps
                outlined below to achieve this:</p>
            <ol id="ol_kfp_q22_sv">
                <li>Remove the sections for servers in
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file having
                        <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines the RADOS
                    Gateway network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following line to the <codeph>service-components</codeph> for
                    the cluster with the <codeph>server-role: CONTROLLER-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw</codeblock></li>
                <li>After your configuration files are setup, perform the steps from <b>8-13</b>
                    mentioned in <xref href="#config_ceph/deploying-monitor-on-standalone-node"
                        format="dita">Deploying monitor on standalone node</xref>.</li>
                <!--<li>Commit your configuration.<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit-message>"</codeblock></li><li>Run the following playbook to add your nodes into Cobbler: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li><li>To reimage all the nodes using PXE, run the following playbook: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li><li>Run the configuration processor: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li><li>Update your deployment directory with this playbook: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li><li>Deploy these changes: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>-->
            </ol>
        </section>
        <section id="install-more-two-rados-gateway-servers">
            <b>Installing More Than Two RADOS Gateway Servers </b>
            <p>To deploy more than two RADOS Gateway servers, you need to add a section to the
                    <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file for each
                additional RADOS Gateway node.</p>
            <note>Installing additional RADOS Gateway servers is possible only if the RADOS Gateway
                is installed on dedicated cluster nodes or on dedicated cluster nodes that host the
                Ceph Monitor service. Additional RADOS Gateway servers cannot be added if the RADOS
                Gateway is installed on a controller node.</note>
        </section>
        <section>
            <title id="managing-ceph-cluster-post-deployment">Managing Ceph Cluster Post
                Deployment</title>
            <p>Once your Ceph cluster is successfully deployed and its is functioning as expected,
                you can perform various maintenance operations on the cluster when required. For
                example, supported operations include: changing the configuration of Ceph service
                components and scaling up storage nodes. The following section provides details
                about the various aspects of maintenance that you might encounter during cluster
                management.</p>
            <ul>
                <li>Modify service configuration </li>
                <li>Scale out cluster <ul>
                        <li>Add a Ceph monitor node. See <xref
                                href="../../operations/maintenance/ceph/add_monitor_node.dita#add_monitor_node"
                                >here</xref></li>
                        <li>Add a data disk to an existing storage node. See <xref
                                href="../../operations/maintenance/ceph/add_osd_datadisk.dita#add_datadisk_osd"
                                >here</xref></li>
                        <li>Add new OSD node. See <xref
                                href="../../operations/maintenance/ceph/add_osd_node.dita#add_osdnode"
                                >here</xref></li>
                    </ul></li>
                <li>Scale down cluster <ul>
                        <li>Remove a Ceph monitor node. See <xref
                                href="../../operations/maintenance/ceph/remove_monitor_node.dita#remove_monitor_node"
                                >here</xref></li>
                        <li>Remove a RADOS gateway node. See <xref
                                href="../../operations/maintenance/ceph/remove_ceph_rados_node.dita#remove_rgw_node"
                                >here</xref></li>
                        <li>Remove a data disk from an OSD node. See <xref
                                href="../../operations/maintenance/ceph/remove_osd_datadisk.dita#remove_datadisk_osd"
                                >here</xref></li>
                    </ul></li>
                <li> Repair scenarios <ul>
                        <li>Replace a failed OSD node. See <xref
                                href="../../operations/maintenance/ceph/replace_ceph_osd_node.dita#replace_osd_node"
                                >here</xref></li>
                        <li>Replace a failed monitor node. See <xref
                                href="../../operations/maintenance/ceph/replace_ceph_monitor_node.dita#replace_monitor_node"
                                >here</xref></li>
                        <li>Replace a failed OS disk in an OSD node. See <xref
                                href="../../operations/maintenance/ceph/replace_osd_osdisk.dita#replacing_os_disks"
                                >here</xref></li>
                        <li> Replace a failed data disk in an OSD node. See <xref
                                href="../../operations/maintenance/ceph/replace_osd_datadisk.dita#topic_ryt_gt4_zv"
                                >here</xref></li>
                        <li> Replace a failed journal disk in an OSD node. See <xref
                                href="../../operations/maintenance/ceph/replace_osd_journaldisk.dita#replacing_osd_journal_disks"
                                >here</xref></li>
                    </ul>
                </li>
                <li>Recovering ceph nodes after reboot. See <xref
                        href="../../operations/maintenance/ceph/restart_ceph.dita#restart_ceph"
                        >here</xref></li>
            </ul>
        </section>
        <p>Please note that the above maintenance procedures are based on the
                <codeph>entry-scale-kvm-ceph</codeph> input model. If you have implemented an
            alternate configuration, such as deploying monitor on standalone node, you may have to
            adjust the above procedure accordingly.</p>
    </body>
</topic>
