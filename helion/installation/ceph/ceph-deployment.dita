<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Deployment and Configurations </title>
  <abstract>
    <shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
      backend.</shortdesc>
  </abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <!--<section id="expandCollapse"> <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv> <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv> </section>-->
    <p><keyword keyref="kw-hos-tm"/> Ceph deployment leverage the cloud lifecycle operations
      supported by Helion lifecycle management and provides the simplified lifecycle management of
      critical cluster operations like service check, upgrade, reconfiguring service components.
      This section assumes that the reader has an understanding of cloud input model and highlights
      only the important aspects of cloud input model pertaining to Ceph. Here we focus on the
      deployment aspects of <codeph>entry-scale-kvm-ceph</codeph> input model, which is most widely
      used configuration. For alternative supported choices, refer <xref
        href="alternative-supported-choice.dita#config_ceph">here</xref>. To ensure a proper
      deployment and verification of the Ceph, it is important to read the topics and perform the
      described steps in order. <ol id="ol_axm_tdq_kw">
        <li><xref href="#config_ceph/pre-deployment" format="dita">Pre-Deployment</xref><ol
            id="ol_ycb_13z_jw">
            <li>Define OSD disk model for OSD disk</li>
            <li>Customize your service configuration</li>
          </ol></li>
        <li><xref href="#config_ceph/deploying-ceph" format="dita">Deploying Ceph</xref></li>
        <li><xref href="#config_ceph/verify-ceph-cluster" format="dita">Verifying Ceph Cluster
            Status</xref></li>
      </ol></p>
    <p/>
    <section>
      <title id="pre-deployment">Pre-Deployment</title>
      <p>Before starting the deployment of <keyword keyref="kw-hos-tm"/> cloud with Ceph, you must
        understand the following aspects of Ceph cluster:<ul id="ul_hn5_3vh_gw">
          <li id="define-osd"><b>Define OSD disk model for OSD disk</b><p>This section focus on
              expressing the storage requirements of an OSD node. OSD nodes have following type of
                disks:<ul id="ul_hth_g2q_kw">
                <li>system disks </li>
                <li>data disk </li>
                <li> journal disks</li>
              </ul></p><p>System disks is used for OSD components, logging etc. The configuration of
              data and journal disks in important for Ceph deployment. The sample file of disk model
              of <codeph>entry-scale-kvm-ceph</codeph> cloud is as follows.</p><p>
              <codeblock>---
  product:
    version: 2

  disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    volume-groups:
      - name: hlm-vg
        physical-volumes:
          - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 30%
            fstype: ext4
            mount: /
          - name: log
            size: 45%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
           name: os

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            </p><p>The disk model has the following parameters:</p><p>
              <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry><b>device-groups</b></entry>
                      <entry>The name of the device group. There can be several device groups. This
                        allows different sets of disks to be used for different purposes.</entry>
                    </row>
                    <row>
                      <entry><b>name</b></entry>
                      <entry>An arbitrary name for the device group. The name must be
                        unique.</entry>
                    </row>
                    <row>
                      <entry><b>devices</b></entry>
                      <entry>A list of devices allocated to the device group. A
                          <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                          <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                          <codeph>/dev/sdf</codeph> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>consumer</b></entry>
                      <entry>The service that uses the device group. A <codeph>name</codeph> field
                        containing <b>ceph</b> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>attrs</b></entry>
                      <entry>The attributes associated with the consumer.</entry>
                    </row>
                    <row>
                      <entry><b>usage</b></entry>
                      <entry>There can be several uses of devices for a particular service. In the
                        above sample, <codeph>usage</codeph> field contains <b>data</b> which
                        indicates that the device is used for data storage.</entry>
                    </row>
                    <row>
                      <entry><b>journal_disk</b> [OPTIONAL]</entry>
                      <entry>The disk to be used for storing the journal data. When running multiple
                        Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                        of the node.<p>If you do not specify this value, Ceph stores the journal on
                          the OSD's data disk (in a separate partition).</p></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p><p>The above sample file represents the following:<ul id="ul_g4v_m2q_kw">
                <li>The first disk is used for OS and system purpose.</li>
                <li>There are three OSD data disks (sdc, sde, sdf) and two journal disks (sdd and
                  sdg). It illustrates that we can share journal disks for multiple OSDs. It is
                  recommended to use OSD journal disk for four OSD data disks. Please see section
                    <i>Usage of journal disk</i> for more details.</li>
                <li>The drive type is not mentioned for journal or data disk. You can consume any
                  drive type but it is <b>recommended</b> to use SSD for journal disk.</li>
              </ul></p><p>Although above model illustrates mixed usage of journal disk, it is
              strongly advised to keep journal data separate from OSD data which means that your
              disk model <b>should not</b> have journal disks shared on same data disks. For more
              information, see <i>Usage of journal disk below</i>.</p><p><b>Usage of journal
                disk</b></p><p><keyword keyref="kw-hos-tm"/> recommends storing the Ceph OSD
              (object-storage daemon) journal on an SSD (solid-state drive ) and the OSD object data
              on a separate hard disk drive. Considering that SSD drives are costly, you can use
              multiple partitions in a single SSD drive for multiple OSD journals. We recommend not
              more than four or five OSD journals on each SSD disk as a reasonable balance between
              cost and optimal performance. If you have too many OSD journals on a single SSD, and
              the journal disk crashes, you might lose your data on those disks. Also, too many
              journals in a single SSD can negatively affect the performance.</p><p>Using an OSD
              journal as a partition on the data disk itself is supported. However, you might see a
              significant decline in Ceph performance due the fact that each client request to store
              an object is first written to the journal disk before sending an acknowledgement to
              client.</p><p>Ceph OSD journal size defaults to 5120MB (i.e. 5GB) in <keyword
                keyref="kw-hos-tm"/>. This value can be changed, however, it does not apply to any
              existing journal partitions. It will be effective on any new OSDs created after the
              journal size is changed (whether the journal is on same disk or separate disk than the
              data disk). To change the journal size, edit the <codeph>osd_journal_size</codeph>
              parameter in the <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
              file.</p><p>To summarize: <ol id="ol_sbp_v3z_jw">
                <li>Use SSD for journal disk </li>
                <li> Ratio of OSD data disks to journal disk is recommended to 4:100% </li>
                <li> Default journal partition size is 5 GB which you can change. Actual journal
                  size depends upon your disk drive <codeph>rpm</codeph> and expected throughput.
                  The formula is: osd journal size = {2 * (expected throughput * filestore max sync
                  interval)} </li>
                <li> Journal size for ALREADY configured OSD disks<!-- (post day zero case) --> does
                  not get changed even if you change the <codeph>osd_journal_size</codeph> parameter
                  in the <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file. If you
                  want to re-size journal partition of ALREADY configured OSD disks, flush journal
                  data, remove OSD from cluster, and re-add it. </li>
              </ol></p></li>
        </ul></p>
      <p/>
      <p>
        <ul id="ul_ldl_p5q_kw">
          <li><b>Customize your service configuration</b><p>You must customize the paramters in
              following files:<ul id="ul_crx_hfq_kw">
                <li>Customize parameters at
                    <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file<p><keyword
                      keyref="kw-hos-tm"/> provides easy to configure service parameter. All the
                    common parameters are available in
                      <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file. You can
                    deploy your cluster without altering any of the parameter but it is advised to
                    review and understand the parameters before deploying your cluster. Below table
                    provide details about the parameter that can be changed and its
                      description.</p><p><b>Core service parameter</b></p><p>
                    <simpletable frame="all" relcolwidth="1.0* 1.0* 1.0* 1.0*"
                      id="simpletable_ztr_22v_kw">
                      <sthead>
                        <stentry>Parameter</stentry>
                        <stentry>Description</stentry>
                        <stentry>Default Value</stentry>
                        <stentry>Recommendation</stentry>
                      </sthead>
                      <strow>
                        <stentry>ceph_cluster</stentry>
                        <stentry>The name of the Ceph clusters. The default value is Ceph.</stentry>
                        <stentry>Ceph</stentry>
                        <stentry>Customize to suit your requirements.</stentry>
                      </strow>
                      <strow>
                        <stentry>ceph_release</stentry>
                        <stentry>The name of the Ceph release.</stentry>
                        <stentry>firefly</stentry>
                        <stentry>Do not change the default value.</stentry>
                      </strow>
                      <strow>
                        <stentry>osd_pool_default_size</stentry>
                        <stentry>The number of replicas for objects in the pool.</stentry>
                        <stentry>3</stentry>
                        <stentry>Do not lower the default value. The value can be increased to
                          maximum number of OSD nodes in the environment (increasing it beyond this
                          limit will cause the cluster to never reach an active+clean
                          state).</stentry>
                      </strow>
                      <strow>
                        <stentry>osd_pool_default_pg_num</stentry>
                        <stentry>The default number of placement groups for a pool. This value will
                          change based on the number of OSDs available. Please refer Ceph PG
                          calculator at <xref href="http://ceph.com/pgcalc/" format="html"
                            scope="external">http://ceph.com/pgcalc/</xref></stentry>
                        <stentry>128</stentry>
                        <stentry>The value can be changed based on the number of OSD servers/nodes
                          in the deployment. Please refer Ceph pg calculator at <xref
                            href="http://ceph.com/pgcalc/" format="html" scope="external"
                            >http://ceph.com/pgcalc/</xref> to customize it.</stentry>
                      </strow>
                      <strow>
                        <stentry>fstype</stentry>
                        <stentry>Storage filesystem type for OSDs.</stentry>
                        <stentry>xfs</stentry>
                        <stentry>Only xfs file system is certified.</stentry>
                      </strow>
                      <strow>
                        <stentry>zap_data_disk</stentry>
                        <stentry>Zap partition table and contents of the disk.</stentry>
                        <stentry>True</stentry>
                        <stentry>Not recommended to change the default value.</stentry>
                      </strow>
                      <strow>
                        <stentry>persist_mountpoint</stentry>
                        <stentry>Place to persist OSD data disk mount point.</stentry>
                        <stentry>fstab</stentry>
                        <stentry>Not recommended to change the default value (as it ensures the OSD
                          data disks are mounted automatically on a system reboot).</stentry>
                      </strow>
                      <strow>
                        <stentry>osd_settle_time</stentry>
                        <stentry>The time in seconds to wait for after starting/restarting the Ceph
                          OSD services.</stentry>
                        <stentry>10 seconds</stentry>
                        <stentry>It is recommended to increase this value if the number of OSD
                          servers is more than 3 or the servers have a slow network.</stentry>
                      </strow>
                      <strow>
                        <stentry>osd_journal_size</stentry>
                        <stentry>The size of the journal in megabytes.</stentry>
                        <stentry>5120</stentry>
                        <stentry>The value can be increased to achieve optimal utilization of the
                          journal disk (if it is shared between multiple OSDs).</stentry>
                      </strow>
                      <strow>
                        <stentry>data_disk_poll_attempts</stentry>
                        <stentry>The maximum number of attempts before attempting to activate an OSD
                          for a new disk (default value 5). </stentry>
                        <stentry>5</stentry>
                        <stentry>It is recommended to increase this value if the OSD data disk
                          drives are under performing/slower. Since this parameter and the
                          data_disk_poll_interval (below) have a combined effect, it is recommended
                          to consider both while tweaking either of them.</stentry>
                      </strow>
                      <strow>
                        <stentry>data_disk_poll_interval</stentry>
                        <stentry>The time interval in seconds to wait between
                            <codeph>data_disk_poll_attempts</codeph></stentry>
                        <stentry>12</stentry>
                        <stentry>The value can be customized to suit your requirements. However,
                          since this parameter and data_disk_poll_attempts (above) have a combined
                          effect, it is recommended to consider both while tweaking either of
                          them</stentry>
                      </strow>
                      <strow>
                        <stentry>osd_max_open_files</stentry>
                        <stentry>Maximum number of file descriptors for OSD.</stentry>
                        <stentry>32768</stentry>
                        <stentry>Do not change the default value.</stentry>
                      </strow>
                      <strow>
                        <stentry>mon_default_dir</stentry>
                        <stentry>Directory to store monitor data.</stentry>
                        <stentry><codeph>/var/lib/ceph/mon/&lt;ceph_cluster></codeph></stentry>
                        <stentry>Do not change the default value.</stentry>
                      </strow>
                      <strow>
                        <stentry>mon_max_open_files</stentry>
                        <stentry>Maximum number of file descriptors for monitor.</stentry>
                        <stentry>16384</stentry>
                        <stentry>Do not change the default value.</stentry>
                      </strow>
                    </simpletable>
                  </p><p><b>RADOS Gateway parameter</b><table frame="all" rowsep="1" colsep="1"
                      id="table_gc4_c5t_5t">
                      <tgroup cols="4">
                        <colspec colname="c1" colnum="1"/>
                        <colspec colname="c2" colnum="2"/>
                        <colspec colname="newCol3" colnum="3" colwidth="1*"/>
                        <colspec colname="newCol4" colnum="4" colwidth="1*"/>
                        <thead>
                          <row>
                            <entry>Parameter</entry>
                            <entry>Description</entry>
                            <entry>Default Value</entry>
                            <entry>Recommendation</entry>
                          </row>
                        </thead>
                        <tbody>
                          <row>
                            <entry>radosgw_user</entry>
                            <entry>The name of the Ceph client user for
                              <codeph>radosgw</codeph>.</entry>
                            <entry>gateway</entry>
                            <entry>Customize to suit your requirements.</entry>
                          </row>
                          <row>
                            <entry>radosgw_admin_email </entry>
                            <entry>The email address of the server administrator. </entry>
                            <entry>
                              <codeph>admin@hpe.com</codeph></entry>
                            <entry>Update appropriately the email address of the server
                              administrator.</entry>
                          </row>
                          <row>
                            <entry>rgw_keystone_service_type</entry>
                            <entry>The <codeph>service_type</codeph> with which
                                <codeph>radosgw</codeph> gets registered as a service in OpenStack
                              Keystone inventory. If you want to replace Swift with Ceph
                                <codeph>radosgw</codeph> for all your object storage requirements,
                              the value should be <b>object-store</b>.</entry>
                            <entry><codeph>ceph-object-store</codeph>
                            </entry>
                            <entry>
                              <p>Recommended to default value if OpenStack Swift and Ceph
                                  <codeph>radosgw</codeph> will be used for your cloud’s object
                                storage requirements.</p>
                              <p>If only Ceph <codeph>radosgw</codeph> will be used for your cloud’s
                                object storage requirements, the value should be updated to
                                  <b>object-store</b>.</p>
                            </entry>
                          </row>
                          <row>
                            <entry>rgw_keystone_accepted_roles </entry>
                            <entry>Only users having either of the roles listed here will be able to
                              access the Swift APIs of <codeph>radosgw</codeph>.</entry>
                            <entry><codeph>admin</codeph>, <i>_member_</i></entry>
                            <entry>Do not change the default value.</entry>
                          </row>
                        </tbody>
                      </tgroup>
                    </table></p><b>Ceph client parameter</b><table frame="all" rowsep="1" colsep="1"
                    id="table_fgv_c3m_hw">
                    <tgroup cols="4">
                      <colspec colname="c1" colnum="1"/>
                      <colspec colname="c2" colnum="2"/>
                      <colspec colname="newCol3" colnum="3" colwidth="1*"/>
                      <colspec colname="newCol4" colnum="4" colwidth="1*"/>
                      <thead>
                        <row>
                          <entry>Value</entry>
                          <entry>Description</entry>
                          <entry>Default Value</entry>
                          <entry>Recommendation</entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry>pg_active_delay_time</entry>
                          <entry>The delay time for Ceph PG’s to come into active state. </entry>
                          <entry>10</entry>
                          <entry>The value can be increased if the number of OSD servers/nodes in
                            the deployment are more than 3. Since this parameter and the
                              <b>pg_active_retries</b> (below) have a combined effect, it is
                            recommended to consider both while tweaking either of them.</entry>
                        </row>
                        <row>
                          <entry>pg_active_retries</entry>
                          <entry>The number of retries for Ceph placement groups to come into active
                            state with a duration of <codeph>pg_active_delay_time</codeph> seconds
                            between entries.</entry>
                          <entry>5</entry>
                          <entry>The value can be customized to suit your requirements. However,
                            since this parameter and <b>pg_active_delay_time</b> (above) have a
                            combined effect, it is recommended to consider both while tweaking
                            either of them.</entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </table></li>
                <li>Customize parameter at
                    <codeph>~/helion/hos/ansible/roles/_CEP-CMN/defaults/main.yml</codeph><p>The
                    following table provides the description of the parameter. You can edit the
                    parameter in <codeph>main.yml</codeph> file.</p><p>
                    <table frame="all" rowsep="1" colsep="1" id="table_n3k_lrc_5v">
                      <tgroup cols="2">
                        <colspec colname="c1" colnum="1"/>
                        <colspec colname="c2" colnum="2"/>
                        <thead>
                          <row>
                            <entry>Value</entry>
                            <entry>Description</entry>
                          </row>
                        </thead>
                        <tbody>
                          <row>
                            <entry>fsid</entry>
                            <entry>A unique identifier, File System ID, for the Ceph cluster that
                              you should generate prior to deploying a cluster (use the
                                <codeph>uuidgen</codeph> command to generate a new FSID). Once set,
                              this value cannot be changed.</entry>
                          </row>
                        </tbody>
                      </tgroup>
                    </table>
                  </p></li>
              </ul></p></li>
        </ul>
      </p>
    </section>
    <p/>
    <section>
      <title id="deploying-ceph">Deploying Ceph</title>
      <p>To deploy a new <keyword keyref="kw-hos-tm"/> Ceph cloud using the default
          <codeph>entry-scale-kvm-ceph</codeph> model, follow the steps
        below<!--, starting with <b>Edit Your Ceph Environment Input Files</b>-->. </p>
      <p><b>Edit Your Ceph Environment Input Files</b></p>
      <p>Perform the following steps:<ol id="ol_ipw_lfc_2w">
          <li>Log in to the lifecycle manager.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment:
              <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock><p>Begin
              inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p><p>Full details of how
              to do this can be found <xref
                href="../../architecture/input_model/input_model.dita#input_model"
            >here</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file and enter
            details. If you are using alternative RADOS Gateway deployments, see <xref
              href="#config_ceph/install-RGW" format="dita">Alternate RADOS Gateway deployment
              architecture choice</xref> before editing the
              <codeph>servers.yml</codeph>.<codeblock> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin 
    
    - id: osd2
      ip-addr: 192.168.10.10
      role: OSD-ROLE
      server-group: RACK2
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:79"
      ilo-ip: 192.168.9.10
      ilo-password: password
      ilo-user: admin 
 
    - id: osd3
      ip-addr: 192.168.10.11
      role: OSD-ROLE
      server-group: RACK3
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:7a"
      ilo-ip: 192.168.9.11
      ilo-password: password
      ilo-user: admin 

# Ceph RGW Nodes 
   - id: rgw1 
     ip-addr: 192.168.10.12 
     role: RGW-ROLE 
     server-group: RACK1 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:62" 
     ilo-ip: 192.168.9.12 
     ilo-password: password 
     ilo-user: admin 

   - id: rgw2 
     ip-addr: 192.168.10.13 
     role: RGW-ROLE 
     server-group: RACK2 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:63" 
     ilo-ip: 192.168.9.13 
     ilo-password: password 
     ilo-user: admin </codeblock><p>The
              above sample files contains three OSD nodes and two RADOS Gateway nodes.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file to
            align disk model a per server specification in your environment. For details on disk
            model refer to <xref href="#config_ceph/define-osd" format="dita">disk
              model.</xref><p>The Ceph service configuration parameters can be modified as described
              in the <b>Pre-Deployment </b>section above.</p></li>
          <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
          <li> After your configuration files are setup, continue with the <xref
              href="../installing_kvm.dita#install_kvm/config_processor">Entry-scale KVM Cloud
              installation steps</xref>. <p>
              <note>For any troubleshooting information regarding the OSD node failure, see <xref
                  href="../operations/troubleshooting/ts_ceph.dita#troubleshooting_ceph"/></note>
            </p></li>
        </ol></p>
    </section>
    <p/>
    <section>
      <title id="verify-ceph-cluster">Verifying Ceph Cluster Status</title>
      <p>If you have deployed RADOS Gateway with core Ceph then ensure that all service components
        including RADOS Gateway are functioning as expected.</p>
      <p><b>Verify Core Ceph</b></p>
      <p>Perform the following steps to check the status of Ceph cluster:<ol id="ol_knp_2z2_2w">
          <li>Login to monitor
            node.<!-- (which is controller node in case of entry-scale-kvm-ceph input model)--></li>
          <li>Execute the following command and ensure that the result is HEALTH_OK or
              HEALTH_WARN:<codeblock>$ ceph health</codeblock><p>Optionally the life cycle manager
              can also be setup as a Ceph client node (refer to Set Up the Lifecycle Manager as a
              Ceph Client ) and the above command can be executed from lifecycle manager.</p></li>
        </ol></p>
    </section>
    <p><b>Verify RADOS Gateway</b></p>
    <p>To make sure that a Keystone user can access the RADOS Gateway using Swift, perform the
      following steps:<ol id="ol_ldl_nhl_lw">
        <li>Login to a controller node.</li>
        <li>Source the service.osrc file:<codeblock>source ~/service.osrc</codeblock></li>
        <li>Execute the following command to generate a list of the containers associated with the
            user:<codeblock>swift --os-service-type ceph-object-store list</codeblock><p>If the
            containers are listed, indicates the RADOS Gateway is accessible. </p></li>
      </ol></p>
  </body>
</topic>
