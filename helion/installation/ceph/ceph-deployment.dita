<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Deployment and Configurations </title>
  <abstract>
    <shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
      backend.</shortdesc>
  </abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p><keyword keyref="kw-hos-tm"/> Ceph deployment leverages the cloud lifecycle operations
      supported by Helion lifecycle management and provides simplified lifecycle management of
      critical cluster operations like service check, upgrade, reconfiguring service components.
      This section assumes that the reader has an understanding of the cloud input model, and as
      such, only highlights important aspects of the cloud input model that pertain to Ceph. Here we
      focus on the deployment aspects of <codeph>entry-scale-kvm-ceph</codeph> input model, which is
      the most widely used configuration. For alternative supported choices, refer <xref
        href="alternative-supported-choice.dita#config_ceph">here</xref>. To ensure a proper
      deployment and verification of Ceph, it is important to read the topics and perform the
      described steps in order. <ol id="ol_axm_tdq_kw">
        <li><xref href="#config_ceph/pre-deployment" format="dita">Pre-Deployment</xref><ol
            id="ol_ycb_13z_jw">
            <li>Define OSD disk model for OSD disk</li>
            <li>Customize your service configuration</li>
          </ol></li>
        <li><xref href="#config_ceph/deploying-ceph" format="dita">Deploying Ceph</xref></li>
        <li><xref href="#config_ceph/verify-ceph-cluster" format="dita">Verifying Ceph Cluster
            Status</xref></li>
      </ol></p>
    <p/>

    <section>
      <title id="pre-deployment">Pre-Deployment</title>
      <p>Before starting the deployment of <keyword keyref="kw-hos-tm"/> cloud with Ceph, you must
        understand the following aspects of a Ceph cluster:</p>
      <p id="define-osd"><b>Define OSD disk model for OSD disk</b></p>
      <p>This section focuses on expressing the storage requirements of an OSD node. OSD nodes have
        the following disk types:</p>
      <ul>
        <li>system disks</li>
        <li>data disk</li>
        <li>journal disks</li>
      </ul>
      <p>System disks are used for OSD components, logging, etc. The configuration of data and
        journal disks is important for Ceph deployment. The sample file of disk model of
          <codeph>entry-scale-kvm-ceph</codeph> cloud is as follows.</p>
      <codeblock>---
  product:
    version: 2

  disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    volume-groups:
      - name: hlm-vg
        physical-volumes:
          - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 30%
            fstype: ext4
            mount: /
          - name: log
            size: 45%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
           name: os

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
      <p>The disk model has the following parameters:</p>
      <table frame="all" rowsep="1" colsep="1" id="ceph1">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2.29*"/>
          <thead>
            <row>
              <entry>Value</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><b>device-groups</b></entry>
              <entry>The name of the device group. There can be several device groups. This allows
                different sets of disks to be used for different purposes.</entry>
            </row>
            <row>
              <entry><b>name</b></entry>
              <entry>An arbitrary name for the device group. The name must be unique.</entry>
            </row>
            <row>
              <entry><b>devices</b></entry>
              <entry>A list of devices allocated to the device group. A <codeph>name</codeph> field
                containing <codeph>/dev/sdb</codeph>, <codeph>/dev/sdc</codeph>,
                  <codeph>/dev/sde</codeph> and <codeph>/dev/sdf</codeph> indicates that the device
                group is used by Ceph.</entry>
            </row>
            <row>
              <entry><b>consumer</b></entry>
              <entry>The service that uses the device group. A <codeph>name</codeph> field
                containing <b>ceph</b> indicates that the device group is used by Ceph.</entry>
            </row>
            <row>
              <entry><b>attrs</b></entry>
              <entry>The attributes associated with the consumer.</entry>
            </row>
            <row>
              <entry><b>usage</b></entry>
              <entry>There can be several uses of devices for a particular service. In the above
                sample, <codeph>usage</codeph> field contains <b>data</b> which indicates that the
                device is used for data storage.</entry>
            </row>
            <row>
              <entry><b>journal_disk</b> [OPTIONAL]</entry>
              <entry>The disk to be used for storing the journal data. When running multiple Ceph
                OSD daemons on a single node, a journal disk can be shared between OSDs of the
                  node.<p>If you do not specify this value, Ceph stores the journal on the OSD's
                  data disk (in a separate partition).</p></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>The above sample file represents the following:</p>
      <ul>
        <li>The first disk is used for OS and system purpose.</li>
        <li>There are three OSD data disks (sdc, sde, sdf) and two journal disks (sdd and sdg). It
          illustrates that we can share journal disks for multiple OSDs. The recommended
          configuration is to use one OSD journal disk for every four OSD data disks. Please see
          section <i>Usage of journal disk</i> for more details.</li>
        <li>The drive type is not mentioned for journal or data disk. You can consume any drive type
          but it is <b>recommended</b> to use an SSD for the journal disk.</li>
      </ul>
      <p>Although the above model illustrates mixed usage of a journal disk, it is strongly advised
        to keep journal data separate from OSD data. In other words, we recommend that your disk
        model <b>should not</b> have journal data and OSD data on the same disks. For more
        information, see <i>Usage of journal disk below</i>.</p>
      <p><b>Usage of journal disk</b></p>
      <p><keyword keyref="kw-hos-tm"/> recommends storing the Ceph OSD (object-storage daemon)
        journal on a SSD (solid-state drive ) and the OSD object data on a separate hard disk drive.
        Considering that SSD drives are costly, you can use multiple partitions on a single SSD for
        multiple OSD journals. We recommend not more than four or five OSD journals on each SSD as a
        reasonable balance between cost and optimal performance. If you have too many OSD journals
        on a single SSD, and the journal disk crashes, you risk losing the data on those disks.
        Also, too many journals in a single SSD can negatively affect the performance.</p>
      <p>Creating a separate OSD journal partition on an OSD data disk is supported. However, you
        might see a significant decline in Ceph performance with this configuration, due the fact
        that each client request to store an object is first written to the journal disk before
        sending an acknowledgement to the client.</p>
      <p>Ceph OSD journal size defaults to 5120MB (i.e. 5GB) in <keyword keyref="kw-hos-tm"/>. This
        value can be changed, however, it does not apply to any existing journal partitions. It will
        be effective on any new OSDs created after the journal size is changed (whether the journal
        is on same disk or separate disk than the data disk). To change the journal size, edit the
          <codeph>osd_journal_size</codeph> parameter in the
          <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file.</p>
      <p>To summarize:</p>
      <ul>
        <li>Use a SSD for journal disk.</li>
        <li>The recommended ratio of OSD data disks to journal disks is 4:1.</li>
        <li>Default journal partition size is 5 GB, though this can be changed. Actual journal size
          depends upon your disk drive <codeph>rpm</codeph> and expected throughput. The formula is:
          osd journal size = {2 * (expected throughput * filestore max sync interval)}.</li>
        <li>Journal size for ALREADY configured OSD disks<!-- (post day zero case) --> does not get
          changed even if you change the <codeph>osd_journal_size</codeph> parameter in the
            <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file. If you want to re-size
          the journal partition of ALREADY configured OSD disks, flush the journal data, remove the
          OSD disk from the cluster, and re-add it.</li>
      </ul>
      <p><b>Customize your service configuration</b></p>
      <p>You must customize parameters in the
          <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file.</p>
      <p><keyword keyref="kw-hos-tm"/> provides easy to configure service parameters. All the common
        parameters are available in <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
        file. You can deploy your cluster without altering any of the parameters but it is
        recommended that you review and understand the parameters before deploying your cluster. The
        table below provides details and descriptions of the parameters that can be changed.</p>
      <p><b>Core service parameter</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_hlb_cz3_nw">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="newCol3" colnum="3" colwidth="1*"/>
          <colspec colname="newCol4" colnum="4" colwidth="1*"/>
          <thead>
            <row>
              <entry>Parameter</entry>
              <entry>Description</entry>
              <entry>Default Value</entry>
              <entry>Recommendation</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>ceph_cluster</entry>
              <entry>The name of the Ceph clusters. The default value is Ceph.</entry>
              <entry>Ceph</entry>
              <entry>Customize to suit your requirements.</entry>
            </row>
            <row>
              <entry>ceph_release</entry>
              <entry>The name of the Ceph release</entry>
              <entry>firefly</entry>
              <entry>Do not change the default value.</entry>
            </row>
            <row>
              <entry>osd_pool_default_size</entry>
              <entry>The number of replicas for objects in the pool.</entry>
              <entry>3</entry>
              <entry>Do not lower the default value. The value can be increased to maximum number of
                OSD nodes in the environment (increasing it beyond this limit will cause the cluster
                to never reach an active+clean state).</entry>
            </row>
            <row>
              <entry>osd_pool_default_pg_num</entry>
              <entry>The default number of placement groups for a pool. This value will change based
                on the number of OSDs available. Please refer Ceph PG calculator at <xref
                  href="http://ceph.com/pgcalc/" format="html" scope="external"
                  >http://ceph.com/pgcalc/</xref></entry>
              <entry>128</entry>
              <entry>The value can be changed based on the number of OSD servers/nodes in the
                deployment. Please refer Ceph pg calculator at <xref href="http://ceph.com/pgcalc/"
                  format="html" scope="external">http://ceph.com/pgcalc/</xref> to customize
                it</entry>
            </row>
            <row>
              <entry>fstype</entry>
              <entry>Storage filesystem type for OSDs.</entry>
              <entry>xfs</entry>
              <entry>Only xfs file system is certified.</entry>
            </row>
            <row>
              <entry>zap_data_disk</entry>
              <entry>Zap partition table and contents of the disk.</entry>
              <entry>True</entry>
              <entry>Not recommended to change the default value.</entry>
            </row>
            <row>
              <entry>persist_mountpoint</entry>
              <entry>Place to persist OSD data disk mount point</entry>
              <entry>fstab</entry>
              <entry>Not recommended to change the default value (as it ensures the OSD data disks
                are mounted automatically on a system reboot)</entry>
            </row>
            <row>
              <entry>osd_settle_time</entry>
              <entry>The time in seconds to wait for after starting/restarting the Ceph OSD
                services.</entry>
              <entry>10 seconds</entry>
              <entry>It is recommended to increase this value if the number of OSD servers is more
                than 3 or the servers have a slow network.</entry>
            </row>
            <row>
              <entry>osd_journal_size</entry>
              <entry>The size of the journal in megabytes.</entry>
              <entry>5120</entry>
              <entry>The value can be increased to achieve optimal utilization of the journal disk
                (if it is shared between multiple OSDs).</entry>
            </row>
            <row>
              <entry>data_disk_poll_attempts</entry>
              <entry>The maximum number of attempts before attempting to activate an OSD for a new
                disk (default value 5). </entry>
              <entry>5</entry>
              <entry>It is recommended to increase this value if the OSD data disk drives are under
                performing/slower. Since this parameter and the data_disk_poll_interval (below) have
                a combined effect, it is recommended to consider both while tweaking either of
                them.</entry>
            </row>
            <row>
              <entry>data_disk_poll_interval</entry>
              <entry>The time interval in seconds to wait between
                  <codeph>data_disk_poll_attempts</codeph></entry>
              <entry>12</entry>
              <entry>The value can be customized to suit your requirements. However, since this
                parameter and data_disk_poll_attempts (above) have a combined effect, it is
                recommended to consider both while changing either of them</entry>
            </row>
            <row>
              <entry>osd_max_open_files</entry>
              <entry>Maximum number of file descriptors for OSD.</entry>
              <entry>32768</entry>
              <entry>Do not change the default value.</entry>
            </row>
            <row>
              <entry>mon_default_dir</entry>
              <entry>Directory to store monitor data.</entry>
              <entry><codeph>/var/lib/ceph/mon/&lt;ceph_cluster></codeph></entry>
              <entry>Do not change the default value.</entry>
            </row>
            <row>
              <entry>mon_max_open_files</entry>
              <entry>Maximum number of file descriptors for monitor.</entry>
              <entry>16384</entry>
              <entry>Do not change the default value.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>RADOS Gateway parameter</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.09*"/>
          <colspec colname="c2" colnum="2" colwidth="1.09*"/>
          <colspec colname="newCol3" colnum="3" colwidth="1*"/>
          <colspec colname="newCol4" colnum="4" colwidth="1.17*"/>
          <thead>
            <row>
              <entry>Parameter</entry>
              <entry>Description</entry>
              <entry>Default Value</entry>
              <entry>Recommendation</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>radosgw_user</entry>
              <entry>The name of the Ceph client user for <codeph>radosgw</codeph>.</entry>
              <entry>gateway</entry>
              <entry>Customize to suit your requirements.</entry>
            </row>
            <row>
              <entry>radosgw_admin_email </entry>
              <entry>The email address of the server administrator. </entry>
              <entry>
                <codeph>admin@hpe.com</codeph></entry>
              <entry>Update appropriately the email address of the server administrator.</entry>
            </row>
            <row>
              <entry>rgw_keystone_service_type</entry>
              <entry>The <codeph>service_type</codeph> with which <codeph>radosgw</codeph> gets
                registered as a service in OpenStack Keystone inventory. If you want to replace
                Swift with Ceph <codeph>radosgw</codeph> for all your object storage requirements,
                the value should be <b>object-store</b>.</entry>
              <entry><codeph>ceph-object-store</codeph>
              </entry>
              <entry>
                <p>Recommended to default value if OpenStack Swift and Ceph <codeph>radosgw</codeph>
                  will be used for your cloud’s object storage requirements.</p>
                <p>If only Ceph <codeph>radosgw</codeph> will be used for your cloud’s object
                  storage requirements, the value should be updated to <b>object-store</b>.</p>
              </entry>
            </row>
            <row>
              <entry>rgw_keystone_accepted_roles </entry>
              <entry>Only users having either of the roles listed here will be able to access the
                Swift APIs of <codeph>radosgw</codeph>.</entry>
              <entry><codeph>admin</codeph>, <i>_member_</i></entry>
              <entry>Do not change the default value.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>Ceph client parameter</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_fgv_c3m_hw">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.34*"/>
          <colspec colname="c2" colnum="2" colwidth="1.59*"/>
          <colspec colname="newCol3" colnum="3" colwidth="1*"/>
          <colspec colname="newCol4" colnum="4" colwidth="1.96*"/>
          <thead>
            <row>
              <entry>Value</entry>
              <entry>Description</entry>
              <entry>Default Value</entry>
              <entry>Recommendation</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>pg_active_delay_time</entry>
              <entry>The delay time for Ceph PG’s to come into active state. </entry>
              <entry>10</entry>
              <entry>The value can be increased if the number of OSD servers/nodes in the deployment
                are more than 3. Since this parameter and the <b>pg_active_retries</b> (below) have
                a combined effect, it is recommended to consider both while tweaking either of
                them.</entry>
            </row>
            <row>
              <entry>pg_active_retries</entry>
              <entry>The number of retries for Ceph placement groups to come into active state with
                a duration of <codeph>pg_active_delay_time</codeph> seconds between entries.</entry>
              <entry>5</entry>
              <entry>The value can be customized to suit your requirements. However, since this
                parameter and <b>pg_active_delay_time</b> (above) have a combined effect, it is
                recommended to consider both while tweaking either of them.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>Customize parameter at
          <codeph>~/helion/hos/ansible/roles/_CEP-CMN/defaults/main.yml</codeph> file.</p>
      <p>The following table provides the description of the parameter. You can edit the parameter
        in <codeph>main.yml</codeph> file.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_n3k_lrc_5v">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="3.38*"/>
          <thead>
            <row>
              <entry>Value</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>fsid</entry>
              <entry>A unique identifier, File System ID, for the Ceph cluster that you should
                generate prior to deploying a cluster (use the <codeph>uuidgen</codeph> command to
                generate a new FSID). Once set, this value cannot be changed.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title id="deploying-ceph">Deploying Ceph</title>
      <p>To deploy a new <keyword keyref="kw-hos-tm"/> Ceph cloud using the default
          <codeph>entry-scale-kvm-ceph</codeph> model, follow the steps
        below<!--, starting with <b>Edit Your Ceph Environment Input Files</b>-->. </p>
      <p><b>Edit Your Ceph Environment Input Files</b></p>
      <p>Perform the following steps:<ol id="ol_ipw_lfc_2w">
          <li>Log in to the lifecycle manager.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment:
              <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock><p>Begin
              inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p><p>Full details of how
              to do this can be found <xref
                href="../../architecture/input_model/input_model.dita#input_model"
            >here</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file and enter
            details. If you are using alternative RADOS Gateway deployments, see <xref
              href="alternative-supported-choice.dita#config_ceph">Alternate RADOS Gateway
              deployment architecture choice</xref> before editing the
              <codeph>servers.yml</codeph>.<codeblock> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin 
    
    - id: osd2
      ip-addr: 192.168.10.10
      role: OSD-ROLE
      server-group: RACK2
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:79"
      ilo-ip: 192.168.9.10
      ilo-password: password
      ilo-user: admin 
 
    - id: osd3
      ip-addr: 192.168.10.11
      role: OSD-ROLE
      server-group: RACK3
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:7a"
      ilo-ip: 192.168.9.11
      ilo-password: password
      ilo-user: admin 

# Ceph RGW Nodes 
   - id: rgw1 
     ip-addr: 192.168.10.12 
     role: RGW-ROLE 
     server-group: RACK1 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:62" 
     ilo-ip: 192.168.9.12 
     ilo-password: password 
     ilo-user: admin 

   - id: rgw2 
     ip-addr: 192.168.10.13 
     role: RGW-ROLE 
     server-group: RACK2 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:63" 
     ilo-ip: 192.168.9.13 
     ilo-password: password 
     ilo-user: admin </codeblock><p>The
              above sample files contains three OSD nodes and two RADOS Gateway nodes.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file to
            align disk model a per server specification in your environment. For details on disk
            model refer to <xref href="#config_ceph/define-osd" format="dita">disk
              model.</xref><p>The Ceph service configuration parameters can be modified as described
              in the <b>Pre-Deployment </b>section above.</p></li>
          <li>Commit your configuration to the<xref href="../using_git.dita#using_git"> local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
          <li> After your configuration files are setup, continue with the <xref
              href="../installing_kvm.dita#install_kvm/config_processor">Entry-scale KVM Cloud
              installation steps</xref>. <p>
              <note>For any troubleshooting information regarding the OSD node failure, see <xref
                  href="../../operations/troubleshooting/ts_ceph.dita#troubleshooting_ceph"/></note>
            </p></li>
        </ol></p>
    </section>
    <p/>
    <section>
      <title id="verify-ceph-cluster">Verifying Ceph Cluster Status</title>
      <p>If you have deployed RADOS Gateway with core Ceph then ensure that all service components
        including RADOS Gateway are functioning as expected.</p>
      <p><b>Verify Core Ceph</b></p>
      <p>Perform the following steps to check the status of Ceph cluster:<ol id="ol_knp_2z2_2w">
          <li>Login to monitor
            node.<!-- (which is controller node in case of entry-scale-kvm-ceph input model)--></li>
          <li>Execute the following command and ensure that the result is HEALTH_OK or
              HEALTH_WARN:<codeblock>$ ceph health</codeblock><p>Optionally the life cycle manager
              can also be setup as a Ceph client node (refer to Set Up the Lifecycle Manager as a
              Ceph Client ) and the above command can be executed from lifecycle manager.</p></li>
        </ol></p>
    </section>
    <p><b>Verify RADOS Gateway</b></p>
    <p>To verify that the RADOS Gateway is accessible to Keystone users, we will utilize calls to
      the Swift API to create, then list containers associated with a user. Note that if the the
      RADOS Gateway is accessible to Keystone users, but no containers exist, the command in step 4
      below will execute with no output. </p>
    <p>To make sure that a Keystone user can access the RADOS Gateway using Swift, perform the
      following steps:<ol id="ol_ldl_nhl_lw">
        <li>Login to a controller node.</li>
        <li>Source the service.osrc file:<codeblock>source ~/service.osrc</codeblock></li>
        <li>Create a test
          container:<codeblock>touch ~/test_container.txt
swift --os-service-type ceph-object-store upload test_container ~/test_container.txt</codeblock></li>
        <li>Execute the following command to generate a list of the containers associated with the
            user:<codeblock>swift --os-service-type ceph-object-store list</codeblock><p>If the
            container created in step 3 is listed in the output of the above command, it indicates
            that the RADOS Gateway is accessible to Keystone users. Please note that if the RADOS
            Gateway is accessible to Keystone users but no containers have been created, the above
            command will execute with no output.</p></li>
        <li>OPTIONAL. Remove the container "test_container" created in step 3, as well as the empty
          txt file used to create
          it:<codeblock>swift --os-service-type ceph-object-store delete test_container
rm ~/test_container.txt</codeblock></li>
      </ol></p>
  </body>
</topic>
