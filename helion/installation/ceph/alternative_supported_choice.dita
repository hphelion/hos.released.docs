<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Alternative Supported Choices</title>
  <abstract>
    <shortdesc outputclass="hdphidden">This section provides insight on how to alter the
                <codeph>entry-scale-kvm-ceph</codeph> input model to deploy Ceph with various
            supported options. We recommend that you deploye your supported choice only after
            evaluating all pros and cons.</shortdesc>
  </abstract>
  <body>
        <!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <p>This section provides insight on how to alter the <codeph>entry-scale-kvm-ceph</codeph>
            input model to deploy Ceph with various supported options. We recommend that you deploye
            your supported choice only after evaluating all pros and cons. For technical details,
            please consult with the technical support team. The choices available can impact the
            performance and scaling of clusters. Choices are illustrated for reference purposes and
            you can combine one or more of them as needed. The content is categorized as
            follows:</p>
        <p> </p>
        <p>
            <ol id="ol_sqb_mrg_kw">
                <li>Core Ceph<ul id="ul_ejq_yrg_kw">
                        <li><xref href="#config_ceph/deploying-monitor-on-standalone-node"
                                format="dita">Installing the Monitor Service on Standalone
                                Nodes</xref>
                        </li>
                        <li><xref href="#config_ceph/single-vlan-for-all-ceph-traffic" format="dita"
                                >Using a Single VLAN for All Ceph Traffic (Management, Client, and
                                Internal OSD)</xref></li>
                        <li><xref href="#config_ceph/using-two-vlan" format="dita">Using Two VLANs:
                                For Management and Client Traffic and for Internal OSD`
                                Traffic</xref>
                        </li>
                    </ul>
                </li>
                <li>RADOS Gateway<ul id="ul_qpj_zs1_nw">
                        <li><xref
                                href="#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                                format="dita">Installing RADOS Gateway on Dedicated Cluster Nodes
                                that Host the Ceph Monitor Service</xref>
                        </li>
                        <li><xref href="#config_ceph/install-rados-gateway-on-controller-nodes"
                                format="dita">Installing RADOS Gateway on Controller
                            Nodes</xref></li>
                        <li>
                            <p><xref href="#config_ceph/install-more-two-rados-gateway-servers"
                                    format="dita">Installing More than Two RADOS Gateway
                                    Servers</xref></p>
                        </li>
                    </ul></li>
                <li><xref href="#config_ceph/managing-ceph-cluster-post-deployment" format="dita"
                        >Managing Ceph Clusters After Deployment</xref></li>
            </ol>
        </p>
        <p/>
        <section>
            <title>Core Ceph</title>
        </section>
        <section id="deploying-monitor-on-standalone-node"><b>Installing the Monitor Service on
                Standalone Nodes</b>
            <p>This section provides the procedure for installing the monitor service on standalone
                nodes instead of installing on controller nodes, as mentioned in
                    <codeph>entry-scale-kvm-ceph</codeph>.</p><p>
                <!--In the <keyword keyref="kw-hos-phrase"/> example configurations, the Ceph monitor service is installed on the controller nodes by default. If you want to break these out into their own cluster, then you can do so by modifying the input model to form a separate cluster.-->
                <note type="attention">If you want to install the monitor service as a dedicated
                    resource node, then you must decide before deploying Ceph. <keyword
                        keyref="kw-hos-phrase"/> does not support deployment transition. After Ceph
                    is deployed, you cannot migrate the monitor service from controller nodes to
                    dedicated resource nodes. </note>
            </p><!--<p><b>Architecture Diagram</b></p><p>The following architecture diagram illustrates deployment scenarios of the monitor service on a standalone node.</p><p>&lt;need diagram></p>--><p><b>Prerequisite</b></p><p><!--The lifecycle manager must be set up before starting Ceph deployment. For more details on the installation of the lifecycle manager, see <xref href="../install_entryscale_kvm.dita#install_kvm"/>.--></p><p>Perform
                the following steps to install the Ceph monitor on dedicated nodes. Note that Ceph
                requires at least three monitoring servers to form a cluster in case of node
                    failure.<ol id="ol_sys_n3l_lw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                            <codeph>~/helion/my_cloud/definition</codeph> directory before you begin
                        the editing
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Edit the <codeph>control_plane.yml</codeph> to create a new cluster, such as
                        with <codeph>ceph-mon</codeph>, as shown
                        here.<codeblock>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...
                                    
  <b>- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</b>
    
  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file
                        to define the Ceph monitor node (monitor services). The following example
                        shows three nodes for monitor services. We recommend using an odd number of
                        monitor
                        nodes.<codeblock># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</codeblock></li>
                    <li>Edit the
                            <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                        file to define a new network interface set for your Ceph monitors, as shown here.<p>
                            <codeblock>## This defines the interface used for management
## traffic such as logging, monitoring, etc.
- name: CEP-MON-INTERFACES
  network-interfaces:
    - name: BOND0
      device:
          name: bond0
      bond-data:
          options:
              mode: active-backup
              miimon: 200
              primary: hed1
          provider: linux
          devices:
            - name: hed1
            - name: hed2
      network-groups:
        - MANAGEMENT

- name: RGW-INTERFACES
  network-interfaces:
  ...</codeblock>
                        </p><!--<p><b>Two-network Ceph example:</b></p><codeblock>    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic such as logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic such as logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This defines the interface used for internal
      ## cluster communication among OSD nodes.
        - name: HETH4
          device:
              name: hed4
          network-groups:
            - OSD</codeblock> <b>Single-network Ceph example:</b> <codeblock>    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic such as logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This interface is used to connect the client
      ## node to the Ceph nodes so that any Ceph client
      ## such as cinder-volume can route data directly to
      ## Ceph over this interface.
        - name: ETH2
          device:
              name: hed3
          network-groups:
            - OSD-CLIENT</codeblock>--></li>
                    <li>Edit
                            <codeph>~/helion/my_cloud/definition/data/disks_ceph_monitor.yml</codeph>
                        to define the disk model for monitor nodes.
                        <!--Create a new file named <codeph>disks_ceph_monitor.yml</codeph> in the <codeph>~/helion/my_cloud/definition/data/</codeph> directory. This will define the disk model for your Ceph monitors. You can use the <codeph>disks_rgw.yml</codeph> file as a base and then edit it to match your environment:--><?oxy_delete_marker author="mwelch" timestamp="20160809T090937-0700" start="295" content=" "?><codeblock>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5
                            
  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root
                            
      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/server_roles.yml</codeph>
                        file to define a new server role for your Ceph monitors:
                        <codeblock>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</codeblock></li>
                    <li>Commit your
                        configuration:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
                    <li>Run the following playbook to add your nodes into Cobbler:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
                    <li>To reimage all the nodes using PXE, run the following playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
                    <li>Run the configuration processor:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory with this playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Deploy these changes:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
                </ol></p></section>
        <section id="single-vlan-for-all-ceph-traffic">
            <b>Using a Single VLAN for All Ceph Traffic (Management, Client, and Internal OSD)</b>
            <p>You can use a single VLAN to transmit all Ceph traffic. This configuration is
                recommended for a small cluster deployment.</p>
            <!--<p><b>Architecture diagram</b></p> <p>&lt;need diagram></p>-->
            <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-single-network</b>--></p><p>Perform
                the following steps to to configure
                    <codeph>Entry-scale-kvm-ceph-single-network</codeph>. </p><p>
                <ol id="ol_d5k_4ss_lw">
                    <li> Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                            <codeph>~/helion/my_cloud/definition</codeph> directory before you begin
                        the editing
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Validate that NIC interfaces are correctly specified in
                            <codeph>nic_mapping.yml</codeph> for servers that are used in the
                        cloud.</li>
                    <li>Ensure that you have at least two NICs for Ceph nodes to create a bonded
                        interface for it.</li>
                    <li>Validate that your servers are mapped to a correct NIC interface
                        specification in <codeph>servers.yml</codeph>.
                        <!--A server meeting the criteria of at least two NICs is good enough for this input model.-->
                        The following is an example of a server node used for OSD
                        deployment:<codeblock># Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                    <li>Delete the OSD-INTERNAL and OSD-CLIENT network groups from
                            <codeph>network_groups.yml</codeph>. This is necessary because only the
                        management network is used for Ceph traffic, thus OSD-INTERNAL and
                        OSD-CLIENT network groups are not required. </li>
                    <li>Define <codeph>net_interfaces.yml</codeph> to use only management network
                        groups, as shown
                        here.<codeblock>  - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT 
                                    
    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT

    - name: RGW-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT</codeblock></li>
                    <li>Delete VLAN information for OSD-INTERNAL-NET and OSD-CLIENT-NET from
                            <codeph>networks.yml</codeph>. Only Management VLANs are used.</li>
                    <li>After you set up your configuration files, perform steps <b>8 to 13</b> in
                            <xref href="#config_ceph/deploying-monitor-on-standalone-node"
                            format="dita">Deploying the Monitor Service on Standalone
                        Nodes</xref>.</li>
                </ol>
            </p>
            <p id="using-two-vlan"><b>Using Two VLANs: For Management and Client Traffic and for
                    Internal OSD Traffic</b></p>
            <p>You can use dual VLANs to transmit Ceph traffic. In this configuration one VLAN
                transmits management and client traffic and the other VLAN transmits internal OSD
                traffic. A separate bonded interface for two VLANs is used with four NICs. This
                configuration provides two aspects:<ul id="ul_ypz_qgy_lw">
                    <li>Use of two networks, such as VLANs. </li>
                    <li>Use of separate bonded interfaces for each VLAN (different from what is
                        provided in <codeph>entry-scale-kvm-ceph</codeph>). </li>
                </ul></p>
        </section>
        <p>The use of separate NICs segregates traffic at the interface level and requires your
            server to have at least four NICs. But using a separate bonded interface for each VLAN
            is not mandatory, and thus you can use a single bonded interface (or server with only
            two NICs) for Ceph deployment. </p>
        <!--<p><b>Architecture diagram </b></p><p>&lt;need diagram></p>-->
        <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-dual-network</b>--></p>
        <p>Perform the following steps to to configure
                <codeph>Entry-scale-kvm-ceph-dual-network</codeph>. </p>
        <p>
            <ol id="ol_cxs_5ts_lw">
                <li>Log in to the lifecycle manager.</li>
                <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                        <codeph>~/helion/my_cloud/definition</codeph> directory before you begin the
                    editing
                    process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                <li>Validate that NIC interfaces are correctly specified in
                        <codeph>nic_mapping.yml</codeph> for servers that are used in the cloud. For
                    Ceph OSD nodes, four port servers are required. You can use
                        <b>HP-DL360-4PORT</b> as it is defined in <codeph>nic_mapping.yml</codeph>
                    of <codeph>entry-scale-kvm-ceph</codeph> or define a new NIC mapping (as shown
                    here) for new sets of servers having four port
                    servers.<codeblock> - name: HP-4PORT-SERVER
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"

        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"

        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"</codeblock></li>
                <li>Modify OSD nodes to use four port servers, as shown here. Change the NIC mapping
                    attribute from <b>HP-DL360-4PORT</b> to use any other name defined in
                        <codeph>nic_mapping.yml</codeph>.<codeblock> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>network_groups.yml</codeph>. Note that no
                    dedicated network group exists for client traffic. Only the management network
                    group is used for client traffic.</li>
                <li>Edit <codeph>net_interfaces.yml</codeph> with a bonded NIC as shown
                    here.<codeblock> - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
        - name: BOND1
          device:
              name: bond1
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - OSD-INTERNAL

    - name: RGW-INTERFACES
     network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>server_groups.yml</codeph>. </li>
                <li>Delete VLAN information for OSD-CLIENT-NET from <codeph>networks.yml</codeph>.
                    Only management VLANs are used for client traffic. </li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Installing Monitor on Standalone Node</xref>.</li>
            </ol>
        </p>
        <section>
            <title>RADOS Gateway</title>
        </section>
        <section id="install-rados-gateway-on-cluster-node-that-host-ceph-monitor">
            <b>Installing RADOS Gateway on Dedicated Cluster Nodes that Host the Ceph Monitor
                Service</b>
            <p>You can configure RADOS Gateway to install on one or more dedicated cluster nodes
                hosting the Ceph monitor service as follows:</p><ol id="ol_hfp_q22_sv">
                <li>Remove the sections for servers in the
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file that
                    have the <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following lines to the <codeph>service-components</codeph>
                    section for the cluster nodes that have the <codeph>server-role:
                        MON-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw
- apache2</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Deploying the Monitor on Standalone Nodes</xref>. </li>
            </ol></section>
        <section id="install-rados-gateway-on-controller-nodes">
            <b>Installing RADOS Gateway on Controller Nodes</b>
            <p>You can configure RADOS Gateway to install on controller nodes. To do this, perform
                the following steps:</p><ol id="ol_kfp_q22_sv">
                <li>Remove the sections for servers in the
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file that
                    have the <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following line to <codeph>service-components</codeph> for the
                    cluster with the <codeph>server-role: CONTROLLER-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw</codeblock></li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Deploying the Monitor Service on Standalone Nodes</xref>.</li>
                <!--<li>Commit your configuration.<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit-message>"</codeblock></li><li>Run the following playbook to add your nodes into Cobbler: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li><li>To reimage all the nodes using PXE, run the following playbook: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li><li>Run the configuration processor: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li><li>Update your deployment directory with this playbook: <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li><li>Deploy these changes: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>-->
            </ol>
        </section>
        <section id="install-more-two-rados-gateway-servers">
            <b>Installing More than Two RADOS Gateway Servers</b>
            <p>To deploy more than two RADOS Gateway servers, you need to add a section to the
                    <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file for each
                additional RADOS Gateway node.</p>
            <note>Installing additional RADOS Gateway servers is possible only if RADOS Gateway is
                installed on dedicated cluster nodes or on dedicated cluster nodes that host the
                Ceph monitor service. Additional RADOS Gateway servers cannot be added if RADOS
                Gateway is installed on a controller node.</note></section>
       
    </body>
</topic>
<?oxy_options track_changes="on"?>