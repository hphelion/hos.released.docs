<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for Ceph Block Storage
    Backend</title>
  <abstract><shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
      backend.</shortdesc>This page describes how to configure your Ceph backend for the Helion
    Entry-scale with Ceph cloud model. It consists of the following steps:</abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
    </section>
    <section id="notes"><title outputclass="headerH">Notes</title>Ceph implements distributed object
      storage. Ceph’s software libraries provide client applications with direct access to the
      reliable autonomic distributed object store (RADOS) object-based storage system, and also
      provide a foundation for some of Ceph’s features, including <i>RADOS Block Device</i> (RBD),
        <i>RADOS Gateway</i>, and the <i>Ceph File System</i>.<sectiondiv
        outputclass="insideSection">
        <p>The Ceph cluster expects the network group that will be used for management network
          traffic within the cloud to be left to its default value i.e. MANAGEMENT. Altering the
          name will lead to failures in the cloud deployment, therefore it should be avoided.</p>
        <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
          environment after installing the lifecycle manager. For more details, refer to <xref
            href="../operations/blockstorage/ceph/deploy_monitor_standalone_node.dita">Install a
            monitor service on a dedicated resource node</xref>.</p>
        <p>The following steps describe how to configure Ceph as your Glance backend. This process
          replaces Swift as is the default backend option in HPE Helion OpenStack. If you have any
          pre-existing images in your Glance repository you should download them locally prior to
          configuring Ceph as your backend. Once the Ceph configuration is complete you will then
          need to re-upload the images to your Glance repository in order for those images to
          work.</p>
        <p><b>Ceph and RADOS:</b> The default Helion Entry-scale KVM Ceph input model deploys the
          Ceph RADOS (Reliable Autonomic Distributed Object Store) service on two dedicated cluster
          nodes. The deployment includes the RADOS Gateway (radosgw) service, which is a FastCGI
          module for interacting with librgw and librados APIs. </p>
        <p>The RADOS Gateway service is an object storage interface that allows end user to perform
          HTTP based CRUD operations on object. It supports both the OpenStack Swift and Amazon S3
          REST APIs.</p>
        <p>Because the Ceph RADOS Gateway provides interfaces compatible with Swift and S3, the
          gateway has its own user management. Ceph RADOS Gateway can store data in the same Ceph
          Storage Cluster used to store data from Ceph Filesystem clients or Ceph Block Device
          clients. The S3 and Swift APIs share a common namespace, so you may write data with one
          API and retrieve it with the other.</p>
      </sectiondiv><sectiondiv outputclass="insideSection">
        <p>The HP Helion OpenStack deployment of the RADOS Gateway features include:</p>
        <p>
          <ul id="ul_cl2_gqd_sv">
            <li>RADOS Gateway can be configured to run in a simple (non-federated or single region)
              mode.</li>
            <li>OpenStack Keystone integration (users having configured set of roles can access
              Swift APIs served by radosgw).</li>
            <li>The RADOS Gateway external and internal endpoints are SSL/TLS-enabled by default,
              and the traffic from Ceph clients to HAProxy is SSL/TLS enabled.</li>
            <li>The HAProxy on the controller node acts as a load balancer (in leastconn mode, where
              the load balancer selects the server with the least number of connections) for RADOS
              Gateway servers.</li>
            <li>The default configuration installs RADOS Gateway on standalone nodes (however is an
              optional component)</li>
          </ul>
        </p>
        <p id="multibackend"><b>Concerning using multiple backends:</b> If you are using multiple
          backend options, ensure that you specify each of the backends you are using when
          configuring your <codeph>cinder.conf.j2</codeph> file using a comma-delimited list. An
          example would be <codeph>enabled_backends=vsa-1,3par_iSCSI,ceph1</codeph> and is included
          in the steps below. Also create multiple volume types so you can specify which backend you
          want to utilize when creating volumes. These instructions are also included. In addition
          to our documentation, you can also read the OpenStack documentation at <xref
            href="http://docs.openstack.org/admin-guide-cloud/blockstorage_multi_backend.html"
            scope="external" format="html">Configure multiple storage backends</xref> as well.</p>
      </sectiondiv></section>
    <section id="prereqs">
      <title outputclass="headerH">Prerequisites</title>
      <sectiondiv outputclass="insideSection">
        <p>Ensure that disks designated to be used for the OSD data and journal storage meet the
          following conditions. If these conditions are not met,  the Ceph configuration will
          fail:</p>
        <ul id="ul_xsp_mfx_dv">
          <li>Any existing partitions must be deleted</li>
          <li>The disk should not be mounted</li>
          <li>The disk should not be in use or held by some other device</li>
        </ul>
        <sectiondiv id="osd-journal">
          <p><b>Concerning OSD journals:</b> HPE Helion OpenStack recommends storing the Ceph RADOS
            Gateway OSD (object-storage deamon) journal on an SSD (solid-state drive ) and the OSD
            object data on a separate hard disk drive. Considering that SSD drives are costly, you
            can use multiple partitions in a single SSD drive for multiple OSD journals. We
            recommend no more than four or five OSD journals on each SSD disk as a reasonable
            balance between cost and optimal performance. If you have too many OSD journals on a
            single SSD, and the journal disk crashes, you might lose your data on those disks. Also,
            too many journals in a single SSD might negatively affect performance. </p>
          <p>Using an OSD journal as a partition on the data disk itself is supported. However, you
            might see a significant decline in Ceph performance due the fact that each client
            request to store an object is first written to the journal disk before sending an
            acknowledgement to client.</p>
          <p>Ceph OSD journal size defaults to 5120MB (i.e. 5GB) in HPE Helion OpenStack. This value
            can be changed, however, it does not apply to any existing journal partitions. It will
            be effective on any new OSDs created after the journal size is changed (whether the
            journal is on same disk or separate disk than the data disk).  To change the journal
            size, edit the <codeph>osd_journal_size</codeph> parameter in the
              <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file. <!--DOCS-2833--></p>
        </sectiondiv>
      </sectiondiv>
    </section>
    <section><title outputclass="headerH">Installing Ceph and RADOS</title><sectiondiv
        outputclass="insideSection">
        <p>In HPE Helion OpenStack 3.0, the Ceph is installed on two dedicated cluster nodes
          servers. You can install the RADOS Gateway the node(s) hosting the Ceph Monitor service or
          on the HPE Helion OpenStack controller nodes. For information on these alternate
          installation scenarios, see <xref href="#config_ceph/alternate" format="dita"/> before
          starting the installation.</p>
        <p>
          <note> Both of these alternate configurations would lead to a sub-optimal
            performance.</note>
        </p>
      </sectiondiv><b>Edit Your Ceph Environment Input Files</b><sectiondiv
        outputclass="insideSection">
        <ol id="ol_xpb_svc_5v">
          <li>Log in to the lifecycle manager.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment:
              <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock><p>Begin
              inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p><p>Full details of how
              to do this can be found here: <xref keyref="input_model">Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file and enter
            details If you are using alternative RADOS gateway deployments, see <xref
              href="#config_ceph/alternate" format="dita"/> before editing the
              <codeph>servers.yml</codeph>.<codeblock># Ceph RGW Nodes 
   - id: rgw1 
     ip-addr: 192.168.10.12 
     role: RGW-ROLE 
     server-group: RACK1 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:62" 
     ilo-ip: 192.168.9.12 
     ilo-password: password 
     ilo-user: admin 

   - id: rgw2 
     ip-addr: 192.168.10.13 
     role: RGW-ROLE 
     server-group: RACK2 
     nic-mapping: MY-2PORT-SERVER 
     mac-addr: "8b:f6:9e:ca:3b:63" 
     ilo-ip: 192.168.9.13 
     ilo-password: password 
     ilo-user: admin </codeblock><table
              frame="all" rowsep="1" colsep="1" id="table_mgc_2wd_sv">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1"/>
                <colspec colname="c2" colnum="2"/>
                <thead>
                  <row>
                    <entry>Value</entry>
                    <entry>Description</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>id</entry>
                    <entry>An administrator-defined identifier for the server. IDs must be unique
                      and are used to track server allocations. (see <xref
                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/other_topics/persisteddata.html"
                        format="html" scope="external">Persisted Data</xref>).</entry>
                  </row>
                  <row>
                    <entry>ip-addr</entry>
                    <entry> The IP address is used by the configuration processor to install and
                      configure the service components on this server. </entry>
                  </row>
                  <row>
                    <entry>role</entry>
                    <entry> Identifies the server-role of the server. (see <xref
                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/concepts/concepts.html#concept_serverroles"
                        format="html" scope="external">Server Roles</xref> for a description of
                      server roles) </entry>
                  </row>
                  <row>
                    <entry>server-group</entry>
                    <entry> Identifies the server-groups entry that this server belongs to. (see
                        <xref
                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/concepts/concepts.html#concept_servergroups"
                        format="html" scope="external">Server Groups</xref>) </entry>
                  </row>
                  <row>
                    <entry>nic-mapping</entry>
                    <entry>The nic-mappings entry to apply to this server. (see <xref
                        href="http://docs-staging.hpcloud.com/HOS3.0pre-release/helion/architecture/input_model/configobj/nicmappings.html"
                        format="html" scope="external">NIC Mappings</xref>)</entry>
                  </row>
                  <row>
                    <entry>mac-addr</entry>
                    <entry>The MAC address on the server that will be used to network install the
                      operating system.</entry>
                  </row>
                  <row>
                    <entry>ilo-ip</entry>
                    <entry>The IP address of the power management (e.g. IPMI, iLO)
                      subsystem.</entry>
                  </row>
                  <row>
                    <entry>ilo-password</entry>
                    <entry>The user name of the power management (e.g. ipmi-ip, iLO)
                      subsystem.</entry>
                  </row>
                  <row>
                    <entry>ilo-admin</entry>
                    <entry>The user password of the power management (e.g. ipmi-ip, iLO)
                      subsystem.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal
              filesystems.<p>The following is an example <codeph>disks_osd.yml</codeph> with the OSD
              journal on a separate
              disk:</p><codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg  </codeblock><p>The
              above sample file contains three OSD services, one OSD with dedicated journal disk and
              two OSDs with shared journal disk. </p>The following is an example
              <codeph>disks_osd.yml</codeph> without separate journal
            disk:<codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5 

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      -  name: ceph-osd-data-only
      devices:
         - name: /dev/sdc
      consumer:            
         - name: ceph  
      attrs:              
          usage: data    </codeblock>The
            above sample file contains one OSD service, without a separate journal disk. For OSDs
            without separate journal disk, Ceph stores the journal on the OSD's data disk (in a
            separate partition). <p>The disk model has the following fields:</p><table frame="all"
              rowsep="1" colsep="1" id="ceph1">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1"/>
                <colspec colname="c2" colnum="2"/>
                <thead>
                  <row>
                    <entry>Value</entry>
                    <entry>Description</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry><b>device-groups</b></entry>
                    <entry>The name of the device group. There can be several device groups. This
                      allows different sets of disks to be used for different purposes.</entry>
                  </row>
                  <row>
                    <entry><b>name</b></entry>
                    <entry>An arbitrary name for the device group. The name must be unique.</entry>
                  </row>
                  <row>
                    <entry><b>devices</b></entry>
                    <entry>A list of devices allocated to the device group. A <codeph>name</codeph>
                      field containing <codeph>/dev/sdb</codeph>, <codeph>/dev/sdc</codeph>,
                        <codeph>/dev/sde</codeph> and <codeph>/dev/sdf</codeph> indicates that the
                      device group is used by Ceph.</entry>
                  </row>
                  <row>
                    <entry><b>consumer</b></entry>
                    <entry>The service that uses the device group. A <codeph>name</codeph> field
                      containing <b>ceph</b> indicates that the device group is used by
                      Ceph.</entry>
                  </row>
                  <row>
                    <entry><b>attrs</b></entry>
                    <entry>The attributes associated with the consumer.</entry>
                  </row>
                  <row>
                    <entry><b>usage</b></entry>
                    <entry>There can be several uses of devices for a particular service. In the
                      above sample, <codeph>usage</codeph> field contains <b>data</b> which
                      indicates that the device is used for data storage.</entry>
                  </row>
                  <row>
                    <entry><b>journal_disk</b> [OPTIONAL]</entry>
                    <entry>The disk to be used for storing the journal data. When running multiple
                      Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                      of the node.<p>If you do not specify this value, Ceph stores the journal on
                        the OSD's data disk (in a separate partition).</p></entry>
                  </row>
                </tbody>
              </tgroup>
            </table><p>
              <note>Make sure that disks designated to be used for the OSD data and journal storage
                meet the <xref href="#config_ceph/prereqs" format="dita">requirements outlined in
                  the prequisites</xref> at the beginning of this document. </note>
            </p></li>
          <li>[OPTIONAL] There are parameters for Ceph that can be edited by the admin in the
            locations described below. The default values will work but if you choose to change any
            of these values, also change them where they are referenced in your
              <codeph>cinder.conf.j2</codeph> file, which we cover in the next section of this
            guide. <ul id="ul_ypb_svc_5v">
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p><table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>fsid</entry>
                        <entry>A unique identifier, File System ID, for the Ceph cluster that you
                          should generate prior to deploying a cluster (use the
                            <codeph>uuidgen</codeph> command to generate a new FSID). Once set, this
                          value cannot be changed.</entry>
                      </row>
                      <row>
                        <entry>ceph_cluster</entry>
                        <entry> The name of the Ceph clusters. The default cluster name is
                            <codeph>ceph</codeph>, but you may specify a different cluster name.
                        </entry>
                      </row>
                      <row>
                        <entry>osd_settle_time</entry>
                        <entry> The time in seconds to wait for after starting/restarting the Ceph
                          OSD services. </entry>
                      </row>
                      <row>
                        <entry>osd_journal_size</entry>
                        <entry> The size of the journal in megabytes. </entry>
                      </row>
                      <row>
                        <entry>data_disk_poll_attempts </entry>
                        <entry>The maximum number of attempts before attempting to activate an OSD
                          for a new disk (default value 5) </entry>
                      </row>
                      <row>
                        <entry>data_disk_poll_interval</entry>
                        <entry>The time interval in seconds to wait between data_disk_poll_attempts
                          (default value 12) </entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>Add any additional configuration parameters for Ceph in the same file
                  (<codeph>settings.yml</codeph> file) under the <codeph>extra</codeph>: category as
                follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></li>
              <li><codeph>ceph-ansible/roles/_CEP-CMN/defaults/main.yml</codeph>
                <p>In the <codeph>main.yml</codeph> file, you can edit the following
                  parameters:</p><table frame="all" rowsep="1" colsep="1" id="table_n3k_lrc_5v">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>fsid</entry>
                        <entry>A unique identifier, File System ID, for the Ceph cluster that you
                          should generate prior to deploying a cluster (use the
                            <codeph>uuidgen</codeph> command to generate a new FSID). Once set, this
                          value cannot be changed.</entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table></li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HPE Helion OpenStack.</p>
                <table frame="all" rowsep="1" colsep="1" id="table_qbk_mpq_35">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                    <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>secret_id</entry>
                        <entry>A UUID that should be generated prior to configuring the Ceph client
                          nodes. The <codeph>secret_id</codeph> specified here is used by the
                          libvirt process to access the Ceph cluster while attaching a block device
                          from Cinder.</entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
            </ul></li>
          <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
        </ol>
        <p>After your configuration files are setup, continue with the <xref
            href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
            installation steps.</xref></p>
      </sectiondiv></section>
    <section id="configure_backend">
      <title outputclass="headerH">Configure Ceph as the Backend</title>
      <sectiondiv outputclass="insideSection">
        <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
          will show you how to do this.</p>
        <p>Perform the following procedure on the lifecycle manager to configure Ceph as a volume
          backend:</p>
        <p><b>Prerequisites</b></p>
        <p>In order for Ceph to be used as a backend for volumes, the nodes running these services
          should have the Ceph client installed on them. Use the
            <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on these
          nodes.</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
        <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
        <p>
          <note>The steps below install required packages and configure existing client nodes (i.e.
            Cinder, Glance and Nova Compute nodes) to use the Ceph cluster. However, for any new
            client nodes added later on that need to be configured to use the Ceph cluster, just
            execute the above playbook with the additon of the <codeph>--limit
              &lt;new-client-node&gt;</codeph> switch.</note>
        </p>
        <p><b>Ceph Configuration</b></p>
        <p>Continue with the Ceph configuration with the steps below:</p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section:
                  <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock><note
                  type="important">If you are using multiple backend types, you can use a comma
                  delimited list here. For example, if you are going to use both VSA and Ceph
                  backends, specify <codeph>enabled_backends=vsa-1,ceph1</codeph>.</note></li>
              <li>[OPTIONAL] If you want your volumes to use a default volume type, enter the name
                of the volume type in the <codeph>[DEFAULT]</codeph> section with the syntax below.
                  <b>Make note of this value when you create your volume type later.</b>
                <p>
                  <note type="important">If you do not specify a default type then your volumes will
                    default to a non-redundant RAID configuration. It is recommended that you create
                    a volume type and specify it here that meets your environments needs.</note>
                </p><codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li>
              <li>Uncomment the <codeph>ceph</codeph> section and fill the values as per your
                cluster information. If you have more than one cluster, you will need to add another
                similar section with its respective values. In the following example only one
                cluster is added.
                  <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock><p>where:</p><table
                  frame="all" rowsep="1" colsep="1" id="ceph_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>The secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:: <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>&lt;secret ID will be here></b>
pools:
    - name: volumes</codeblock>
                          <note type="important">You should generate and use your own secret ID. You
                            can use any UUID generation tool.</note></entry>
                      </row>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>The username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: &lt;secret ID will be here>
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_pool</entry>
                        <entry>The pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:
                          <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_ceph_conf</entry>
                        <entry>The Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>volume_driver</entry>
                        <entry>The Cinder volume driver. Leave this as the default value specified
                          for Ceph.</entry>
                      </row>
                      <row>
                        <entry>volume_backend_name</entry>
                        <entry>The name given to the Ceph backend.
                          <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
            </ol></li>
          <li>To enable Cinder to backup to Ceph, make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values:
                  <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock><p>where:</p><table
                  frame="all" rowsep="1" colsep="1" id="ceph_backup">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>backup_driver</entry>
                        <entry>The Cinder volume driver. Leave this as the default value specified
                          for Ceph.</entry>
                      </row>
                      <row>
                        <entry>backup_ceph_conf</entry>
                        <entry>The Ceph configuration file location, usually:
                            <codeph>/etc/ceph/ceph.conf</codeph>.</entry>
                      </row>
                      <row>
                        <entry>backup_ceph_user</entry>
                        <entry>The user name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                      </row>
                      <row>
                        <entry>backup_ceph_pool</entry>
                        <entry>The pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>pools:
    - name: <b>backups</b></codeblock></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table></li>
            </ol>
          </li>
          <li>To enable Ceph as your Glance backend, make the following changes to the
              <codeph>~/helion/my_cloud/config/glance/glance-api.conf.j2</codeph> file: <ol>
              <li>Uncomment and edit the following values:
                <codeblock>default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</codeblock></li>
              <li>In the same file, comment out the following references to Swift:
                <codeblock>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</codeblock></li>
            </ol>
          </li>
          <li><b>IMPORTANT:</b> If you have pre-existing images in your Glance repo and want to use
            exclusively Ceph as a backend, use the Glance CLI or other tool back-up your images
            prior to configuring Ceph as your Glance backend: <ol>
              <li>Snapshot or delete all Nova instances using those images.</li>
              <li>Download the images locally that you want to save.</li>
              <li>Delete all of the images from Glance.</li>
            </ol><p>After you have finished the Ceph configuration you will need to re-add those
              images.</p></li>
          <li>To enable attaching Ceph volumes to Nova provisioned instances, make the following
            changes to the <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph>
            file: <ol>
              <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &lt;ceph-user>
rbd_secret_uuid = &lt;secret-uuid></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>The username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
            name: <b>cinder</b>
            type: openstack
            secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>The secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
           name: <b>cinder</b>
           type: openstack
           secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
                <note>To attach a volume provisioned out of a newly-added Ceph backend to an
                  existing OpenStack instance, the instance must be rebooted after the new backend
                  has been added.</note></li>
            </ol></li>
          <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Update your deployment directory:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Run the Cinder Reconfigure Playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
          <li>If Nova has been configured to attach Ceph backend volumes, run the Nova reconfigure
            playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
          <li>If Ceph has been configured as the Glance backend, run the Glance reconfigure
            playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
        </ol>
      </sectiondiv>
    </section>
    <section id="alternate">
      <title outputclass="headerH">Alternate RADOS Gateway deployment architecture choices</title>
      <sectiondiv outputclass="insideSection"> The RADOS Gateway service can also be co-hosted along
        with other HPE Helion OpenStack services as listed in the following alternatives below. <ul
          id="ul_wmx_w22_sv">
          <li>Configure the Lifecycle Manager as a Ceph Client</li>
          <li>Installing RADOS Gateway on (dedicated) cluster node(s) that host Ceph Monitor
            service</li>
          <li>Installing RADOS Gateway on controller nodes</li>
        </ul><p>
          <note>Because the RADOS Gateway service will be sharing server resources with multiple
            services, these alternate configurations will likely result in sub-optimal performance,
            as compared to the default configuration.</note>
        </p><p>Each of this requires corresponding change in input model es explained
        below.</p></sectiondiv>
    </section>
    <section>
      <title outputclass="headerH">Set Up the Lifecycle Manager as a Ceph Client</title>
      <sectiondiv outputclass="insideSection">
        <p>If you have a separate lifecycle manager node or if you want to set up the lifecycle
          manager as a Ceph client, then the lifecycle manager node should have the Ceph client
          installed on it along with the necessary configuration and keyring files required to
          access to the Ceph cluster. Use the <codeph>ceph-setup-deployer-as-client.yml</codeph>
          playbook to use the lifecycle manager node to access the Ceph cluster:</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</codeblock>
      </sectiondiv>
    </section>
    <section>
      <sectiondiv outputclass="insideSection"><b>Installing RADOS Gateway on (dedicated) cluster
          node(s) that host Ceph Monitor service</b><p>The RADOS Gateway can be configured to
          install on dedicated cluster node(s) hosting the Ceph Monitor service as follows:</p><ol
          id="ol_hfp_q22_sv">
          <li>Remove the sections for servers in
              <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file having
              <codeph>role: RGW-ROLE</codeph> attribute.</li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file and
            add the following lines to the <codeph>service-components</codeph> section for the
            cluster node(s) having  the <codeph>server-role: MON-ROLE</codeph>
            attribute.<codeblock>- ceph-radosgw
- apache2</codeblock></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file to
            remove the RGW-INTERFACES section. This section defines the RADOS gateway network
            interfaces, which are not required in this configuration:
            <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
        </ol><b>Installing RADOS Gateway on controller nodes</b><p>The RADOS Gateway can be
          configured to install on the controller nodes. The steps outlined below need to be
          performed to achieve this:</p><ol id="ol_kfp_q22_sv">
          <li>Remove the sections for servers in
              <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file having
              <codeph>role: RGW-ROLE</codeph> attribute.</li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file to
            remove the RGW-INTERFACES section. This section defines the RADOS gateway network
            interfaces, which are not required in this configuration:
            <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file and
            add the following line to the <codeph>service-components</codeph> for the cluster with
            the <codeph>server-role: CONTROLLER-ROLE</codeph>
            attribute.<codeblock>- ceph-radosgw</codeblock></li>
        </ol><p><b>Installing more than 2 RADOS Gateway servers</b></p><p>To deploy more than RADOS
          Gateway servers, you need to add a section to the
            <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file for each additional
          RADOS Gateway node.</p></sectiondiv>
    </section>
    <section>
      <title outputclass="headerH">Consumption of RADOS gateway using Swift and S3-compatible
        API</title>
      <sectiondiv outputclass="insideSection"> Configuring the RADOS Gateway with OpenStack
        Keystone, sets up the gateway to accept Keystone as the users authority. A user that
        Keystone authorizes to access the gateway will also be automatically created for the Ceph
        RADOS Gateway (if one doesn't already exist). A token that Keystone validates will be
        considered as valid by the gateway. A Ceph Object Gateway user is mapped into a Keystone
        tenant.The RADOS Gateway service offers two types of users:<ul id="ul_iyy_dz2_sv">
          <li>RADOS Gateway users that can access objects using both S3 and Swift API</li>
          <li>Keystone users that can access objects ONLY using Swift API</li>
        </ul><p>When the RADOS Gateway is deployed in the Entry Scale KVM Ceph model, by default,
          the RADOS Gateway exists alongside OpenStack Swift (since the Keystone service type for
          RADOS Gateway defaults to <codeph>ceph-object-store</codeph>). In this mode, the Horizon
          interface works only with Swift for all the Object Store operations.</p><p>To use the
          Horizon interface to use the RADOS Gateway though Horizon, you must create a Keystone user
          that will user S3 to access the RADOS Gateway functionality and also enable the Keystone
          authentication mechanism for RADOS Gateway. The following sections describe how to perform
          these tasks. </p><b>Creating a new user and subuser</b><p>You can create a new user for
          interacting with Ceph Object Gateway services.</p><ul id="ul_qnm_t1f_sv">
          <li><b>User:</b> A <codeph>user</codeph> can access the S3 interface.</li>
          <li><b>Subuser:</b> A <codeph>subuser</codeph> can access the Swift interface and is
            associated to a user . A subuser needs a Swift access key. </li>
        </ul><p><note>Creating a user also creates an access_key and secret_key entry for use with
            any S3 API-compatible client.</note>To create a user and subuser:<ol id="ol_jqf_1bf_sv">
            <li>Log into a RADOS Gateway server.</li>
            <li>Execute the following command to create a user:
                <codeblock>sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}" --email="{email}"</codeblock><p>For
                example:<codeblock>sudo radosgw-admin user create --uid=rgwuser --display-name="RadosGatewayUser"--email="rgwuser@test.com"</codeblock></p></li>
            <li>Execute the following command to create sub-user for Swift and generate secret
                key:<codeblock>sudo radosgw-admin subuser create --uid={username} --subuser={username}:{subusername} --access=full --key-type=swift --gen-secret</codeblock><p>For
                example:<codeblock>sudo radosgw-admin subuser create --uid="rgwuser"--subuser="rgwuser:rgwswiftuser"--access=full --key-type=swift --gen-secret</codeblock></p><note>A
                Ceph Object Gateway <codeph>user:subuser</codeph> tuple maps to the
                  <codeph>tenant:user</codeph> tuple expected by Swift. </note></li>
          </ol></p></sectiondiv>
    </section>
    <section>
      <title outputclass="headerH">Configure a Keystone user to access the RADOS Gateway
        functionality using Swift API </title>
      <sectiondiv outputclass="insideSection">
        <p>The OpenStack Swift command line interface can be tuned to communicate with both
          OpenStack Swift and RADOS Gateway.</p>
        <ol id="ol_jkf_2cf_sv">
          <li>Execute the following command to interact with Swift (default):
            <codeblock>export OS_SERVICE_TYPE=object-store</codeblock></li>
          <li>Execute the following command to interact with RADOS Gateway:
            <codeblock>export OS_SERVICE_TYPE=ceph-object-store</codeblock> You can use Swift CLI to
            perform object operations on Ceph.</li>
        </ol>
        <p><b>Configure a Keystone user to access the RADOS Gateway functionality using S3</b>
        </p>
        <p>By default, a Keystone user can access the RADOS Gateway functionality using the Swift
          API. In order to configure a Keystone user for S3 API access the RADOS Gateway, perform
          the following tasks:</p>
        <p>The details to achieve each of the following is explained in the sections below.</p>
        <ol id="ol_qhv_xls_sv">
          <li>Configure the RADOS Gateway to use Keystone authentication for S3: <ol
              id="ol_lhg_1ms_sv">
              <li>Login to lifecycle manager node. </li>
              <li>Edit the <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file to add
                the following section. The following example assumes the RADOS Gateway user name is
                default: <codeph>gateway</codeph>:
                <codeblock>extra: 
  client.radosgw.gateway: 
    rgw_s3_auth_use_keystone: true</codeblock></li>
              <li>Commit your configuration changes:
                <codeblock>cd ~/helion/hos/ansible 
git add -A 
git commit -m "&lt;commit message>"</codeblock></li>
              <li>Execute the ready-deployment playbook:
                <codeblock>cd ~/helion/hos/ansible 
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
              <li>Execute the ceph-reconfigure playbook:
                <codeblock>cd ~/scratch/ansible/next/hos/ansible 
ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
            </ol></li>
          <li>Update Ceph cluster configuration file on all Rados Gateway nodes. <p>The RADOS
              Gateway uses the Keystone V2 API. You need to update the Keystone URL to use the admin
              port instead of public port using the following steps on each RADOS Gateway node in
              the deployment:</p><ol id="ol_lcj_k4s_sv">
              <li>Login to a RADOS Gateway node. </li>
              <li>Edit the Ceph cluster configuration file:
                <codeblock>cat /etc/ceph/ceph.conf</codeblock></li>
              <li>In the <codeph>radosgw</codeph> section (default:
                  <codeph>client.radosgw.gateway</codeph>) change a the <codeph>rgw keystone url =
                  https://helion-cp1-vip-KEY-API-mgmt:</codeph> port from 5000 to 35357.
                <codeblock>CHANGE: 
rgw keystone url = https://helion-cp1-vip-KEY-API-mgmt:5000 
TO: 
rgw keystone url = https://helion-cp1-vip-KEY-API-mgmt:35357</codeblock></li>
              <li>Restart the <codeph>radosgw</codeph> service so that it reloads the configuration
                changes: <codeblock>sudo service radosgw restart</codeblock><p>
                  <note>The changes made above will be overwritten if you run the
                      <codeph>ceph-deploy.yml</codeph> and <codeph>ceph-reconfigure.yml</codeph>
                    playbooks. Perform these configuration tasks each time either of those playbooks
                    is executed for the RADOS Gateway nodes. </note>
                </p></li>
            </ol></li>
          <li>Create the EC2 credentials. <ol id="ol_qth_qps_sv">
              <li>Login to a controller node and source the admin user credentials:
                <codeblock>source ~/service.osrc</codeblock></li>
              <li>Execute the following commands to generate the EC2 credentials for the OpenStack
                user:
                <codeblock>openstack ec2 credentials create --project &lt;project-name> --user &lt;user-name></codeblock>
                For example, to generate the EC2 credentials for user <b>demo</b> for the project
                  <b>demo</b>:
                <codeblock>openstack ec2 credentials create --project demo --user demo</codeblock></li>
            </ol></li>
        </ol>
      </sectiondiv>
    </section>
    <section>
      <title outputclass="headerH">Uninstall Swift</title>
      <sectiondiv outputclass="insideSection">
        <p>If you do not want the Ceph and Swift object storage solutions in the same cloud, you can
          deploy a cloud with only the RADOS Gateway.</p>
        
            <p><b>Deploying Ceph RADOS Gateway as a replacement
                for OpenStack Swift in new cloud</b></p>
              <p>HPE Helion OpenStack 3.0 cloud can be deployed with Ceph RADOS Gateway as a
                replacement for OpenStack Swift. The input model needs to be tuned in the following
                way to achieve this. </p><ol id="ol_pzz_w4c_5v">
                <li>Login to the deployer node and remove all the <codeph>swift</codeph> components
                  except <codeph>swift-client</codeph> from
                    <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file.
                  Remove the following Swift service components:
                  <codeblock>swift-ring-builder
swift-proxy 
swift-account
swift-container 
swift-object </codeblock></li>
                <li>Modify the <codeph>rgw_keystone_service_type</codeph> in file
                    <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> as follows:
                  <codeblock>rgw_keystone_service_type: object-store</codeblock></li>
                <li>Commit your configuration
                  <codeblock>cd ~/helion/hos/ansible
git add -A 
git commit -m "&lt;commit message>" </codeblock></li>
                <li>Proceed with the cloud deployment steps from the Config Processor Run step.
                </li>
              </ol>
    
        <!--<b>Replace swift with rados gateway in pre-deployed Helion cloud </b><p>To replace OpenStack Swift with Ceph RADOS Gateway in a running HOS cloud, perform the steps listed below: </p><ol id="ol_qmq_sgf_sv"><li>Login to controller node. </li><li>Identify the Swift and Ceph service IDs using following commands. Look for Name <codeph>swift</codeph> and <codeph>ceph</codeph> and note the associated ID for eac: <codeblock>source ~/keystone.osrc 
openstack service list </codeblock></li><li>Delete the Swift and Ceph services using the service ID identified in earlier step: <codeblock>source ~/keystone.osrc
openstack service delete &lt;swift-service-id> 
openstack service delete &lt;ceph-service-id></codeblock></li><li>Update the service type for Ceph service, by logging in to the deployer node and modifying the <codeph>rgw_keystone_service_type</codeph> in file <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> as follows: <codeblock>rgw_keystone_service_type: object-store</codeblock></li><li>Commit your configuration <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>" </codeblock></li><li>Execute the ready-deployment playbook: <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml </codeblock></li><li>Execute the ceph-reconfigure playbook: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li><li>Execute the <codeph>swift-stop playbook</codeph> to stop the Swift service: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml </codeblock><note><ul id="ul_wng_jhf_sv"><li>The above process does not migrate the existing objects in the Swift store to the Ceph Object Store. So, existing objects will not be available after migrating a running cluster to Ceph Object Store.</li><li>The Swift services should be manually uninstalled and its data should be deleted from the node(s), otherwise services like Monasca will report failure for Swift services, logging services will have logs of Swift services, etc.</li></ul></note></li></ol>-->
      </sectiondiv>
    </section>
    <section id="post_install">
      <title outputclass="headerH">Post-Installation Tasks</title>
      <sectiondiv outputclass="insideSection">
        <p>After you have configured Ceph as your Block Storage backend, here are some tasks you
          will want to complete:</p>
        <ul>
          <li><xref href="../operations/blockstorage/creating_voltype.dita">Create a Volume Type for
              your Volumes</xref></li>
          <li><xref href="installation_verification.dita#install_verification/volume_verify">Verify
              Your Block Storage Configuration</xref></li>
        </ul>
      </sectiondiv>
    </section>
  </body>
</topic>
