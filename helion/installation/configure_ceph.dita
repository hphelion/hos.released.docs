<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for Ceph Block Storage
    Backend</title>
  <abstract><shortdesc outputclass="hdphidden">Installation and configuration steps for your Ceph
      backend.</shortdesc>This page describes how to configure your Ceph backend for the Helion
    Entry-scale with Ceph cloud model. It consists of the following steps:</abstract>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
    </section>
    <section id="notes"><title outputclass="headerH">Notes</title>
      <sectiondiv outputclass="insideSection">
        <p>The Ceph cluster expects the network group that will be used for management network
          traffic within the cloud to be left to its default value i.e. MANAGEMENT. Altering the
          name will lead to failures in the cloud deployment, therefore it should be avoided.</p>
        <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
          environment after installing the lifecycle manager.</p>
        <p>For more details, refer to <xref
            href="../operations/blockstorage/ceph/deploy_monitor_standalone_node.dita">Install a
            monitor service on a dedicated resource node</xref>.</p>
        <p>We include steps below to configure Ceph as your Glance backend. This replaces Swift
          which is the default backend option in HPE Helion OpenStack. So if you have any
          pre-existing images in your Glance repo you will want to download them locally prior to
          configuring Ceph as your backend. Once the Ceph configuration is complete you will then
          need to re-upload those to your Glance repo in order for those images to work.</p>
        <p id="multibackend"><b>Concerning using multiple backends:</b> If you are using multiple
          backend options, ensure that you specify each of the backends you are using when
          configuring your <codeph>cinder.conf.j2</codeph> file using a comma delimited list. An
          example would be <codeph>enabled_backends=vsa-1,3par_iSCSI,ceph1</codeph> and is included
          in the steps below. You will also want to create multiple volume types so you can specify
          which backend you want to utilize when creating volumes. These instructions are included
          below as well. In addition to our documentation, you can also read the OpenStack
          documentation at <xref
            href="http://docs.openstack.org/admin-guide-cloud/blockstorage_multi_backend.html"
            scope="external" format="html">Configure multiple storage backends</xref> as well.</p>
      </sectiondiv>
    </section>
    <section id="prereqs"><title outputclass="headerH">Prerequisites</title>
      <sectiondiv outputclass="insideSection"><p>Ensure that disks designated to be used for the OSD
          data and journal storage must meet the conditions below. If they do not, Ceph
          configuration will fail:</p><ul id="ul_xsp_mfx_dv">
          <li>Any existing partitions must be deleted</li>
          <li>The disk should not be mounted</li>
          <li>The disk should not be in use or held by some other device</li>
        </ul> 
      <sectiondiv id="osd-journal">
        <p><b>Concerning OSD journals:</b> HPE Helion OpenStack recommends storing the OSD journal
        on an SSD and the OSD object data on a separate hard disk drive. Considering that SSD drives
        are costly, you can use multiple partitions in a single SSD drive for multiple OSD journals.
        We recommend no more than four or five OSD journals on each SSD disk as a reasonable balance
        between cost and optimal performance. If you have too many OSD journals on a single SSD, and
        the journal disk crashes, you might lose your data on those disks. Also, too many journals
        in a single SSD might negatively affect performance. </p>
      <p>Using an OSD journal as a partition on the data disk itself is supported. However, you
          might see a significant decline in Ceph performance due the fact that each client request
          to store an object is first written to the journal disk before sending an acknowledgement
          to client.</p> 
        <p>Ceph OSD journal size defaults to 5120MB (i.e. 5GB) in HPE Helion OpenStack. This value
          can be changed, however, it does not apply to any existing journal partitions. It will be
          effective on any new OSDs created after the journal size is changed (whether the journal
          is on same disk or separate disk than the data disk).</p>
      </sectiondiv></sectiondiv>
    </section>
    <section id="deployer"><title outputclass="headerH">Set Up the Lifecycle Manager as a Ceph
        Client</title>
      <sectiondiv outputclass="insideSection">
        <p>If you have a separate lifecycle manager node or if you want to set up the lifecycle
          manager as a Ceph client, then the lifecycle manager node should have the Ceph client
          installed on it along with the necessary configuration and keyring files required to
          access to the Ceph cluster. Use the <codeph>ceph-setup-deployer-as-client.yml</codeph>
          playbook to use the lifecycle manager node to access the Ceph cluster:</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</codeblock>
      </sectiondiv>
    </section>

    <section id="config_files">
      <title outputclass="headerH">Edit Your Ceph Environment Input Files</title>
      <sectiondiv outputclass="insideSection">
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref keyref="input_model">Input
                Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal
              filesystems.<p>The following is an example <codeph>disks_osd.yml</codeph> with the OSD
              journal on a separate
              disk:</p><codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg  </codeblock><p>The
              above sample file contains three OSD services, one OSD with dedicated journal disk and
              two OSDs with shared journal disk. </p>The following is an example
              <codeph>disks_osd.yml</codeph> with the OSD journal on a separate
            disk:<codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5 

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      -  name: ceph-osd-data-only
      devices:
         - name: /dev/sdcname: /dev/sdd         
      consumer:            
         - name: ceph  
      attrs:              
          usage: data           </codeblock>The
            above sample file contains one OSD service, without a separate journal disk. For OSDs
            without separate journal disk, Ceph stores the journal on the OSD's data disk (in a
            separate partition).<p>The disk model has the following fields:</p>
            <table frame="all" rowsep="1" colsep="1" id="ceph1">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1"/>
                <colspec colname="c2" colnum="2"/>
                <thead>
                  <row>
                    <entry>Value</entry>
                    <entry>Description</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry><b>device-groups</b></entry>
                    <entry>There can be several device groups. This allows different sets of disks
                      to be used for different purposes.</entry>
                  </row>
                  <row>
                    <entry><b>name</b></entry>
                    <entry>This is an arbitrary name for the device group. The name must be
                      unique.</entry>
                  </row>
                  <row>
                    <entry><b>devices</b></entry>
                    <entry>This is a list of devices allocated to the device group. A
                        <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                        <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                        <codeph>/dev/sdf</codeph> indicates that the device group is used by
                      Ceph.</entry>
                  </row>
                  <row>
                    <entry><b>consumer</b></entry>
                    <entry>This specifies the service that uses the device group. A
                        <codeph>name</codeph> field containing <b>ceph</b> indicates that the device
                      group is used by Ceph.</entry>
                  </row>
                  <row>
                    <entry><b>attrs</b></entry>
                    <entry>These are the attributes associated with the consumer.</entry>
                  </row>
                  <row>
                    <entry><b>usage</b></entry>
                    <entry>There can be several uses of devices for a particular service. In the
                      above sample, <codeph>usage</codeph> field contains <b>data</b> which
                      indicates that the device is used for data storage.</entry>
                  </row>
                  <row>
                    <entry><b>journal_disk</b> [OPTIONAL]</entry>
                    <entry>Disk to be used for storing the journal data. When running multiple Ceph
                      OSD daemons on a single node, a journal disk can be shared between OSDs of the
                        node.<p>If you do not specify this value, Ceph stores the journal on the
                        OSD's data disk (in a separate partition).</p></entry>
                  </row>
                </tbody>
              </tgroup>
            </table>
            <p>
              <note>Ensure that disks designated to be used for the OSD data and journal storage
                meet the requirements outlined in the prequisites at the beginning of this document.
                If they do not then Ceph configuration will fail.</note>
            </p></li>
          <li>[OPTIONAL] There are parameters for Ceph that can be edited by the admin in the
            locations described below. The default values will work but if you choose to change any
            of these values, ensure that you also change them where they are referenced in your
              <codeph>cinder.conf.j2</codeph> file, which we cover in the next section of this
            guide. <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p>
                <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>fsid</entry>
                        <entry>This is a unique identifier for the Ceph cluster and should be
                          generated prior to deploying a cluster (use the <codeph>uuidgen</codeph>
                          command to generate a new FSID). Once set, this value cannot be
                          changed.</entry>
                      </row>
                      <row>
                        <entry>ceph_cluster</entry>
                        <entry>
                          <p>Ceph clusters have a cluster name. The default cluster name is ceph,
                            but you may specify a different cluster name.</p>
                        </entry>
                      </row>
                      <row>
                        <entry>osd_settle_time</entry>
                        <entry>
                          <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                            services.</p>
                        </entry>
                      </row>
                      <row>
                        <entry>osd_journal_size</entry>
                        <entry>
                          <p>The size of the journal in megabytes.</p>
                        </entry>
                      </row>
                      <row>
                        <entry>data_disk_poll_attempts </entry>
                        <entry>Maximum number of attempts before attempting to activate an OSD for a
                          new disk (default value 5) </entry>
                      </row>
                      <row>
                        <entry>data_disk_poll_interval</entry>
                        <entry>Time interval in seconds to wait between data_disk_poll_attempts
                          (default value 12) </entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as follows:</p>
                <codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HPE Helion OpenStack.</p>
                <table frame="all" rowsep="1" colsep="1" id="table_qbk_mpq_35">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                    <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>secret_id</entry>
                        <entry>A UUID that should be generated prior to configuring the Ceph client
                          nodes. The secret_id specified here is used by the libvirt process to
                          access the Ceph cluster while attaching a block device from
                          Cinder.</entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
            </ul></li>
          <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
        </ol>
        <p>After your configuration files are setup, continue with the <xref
            href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
            installation steps.</xref></p>
      </sectiondiv>
    </section>

    <section id="configure_backend">
      <title outputclass="headerH">Configure Ceph as the Backend</title>
      <sectiondiv outputclass="insideSection">
        <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
          will show you how to do this.</p>
        <p>Perform the following procedure on the lifecycle manager to configure Ceph as a volume
          backend:</p>
        <p><b>Prerequisites</b></p>
        <p>In order for Ceph to be used as a backend for volumes, the nodes running these services
          should have the Ceph client installed on them. Use the
            <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on these
          nodes.</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
        <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
        <p>
          <note>The steps below install required packages and configure existing client nodes (i.e.
            Cinder, Glance and Nova Compute nodes) to use the Ceph cluster. However, for any new
            client nodes added later on that need to be configured to use the Ceph cluster, just
            execute the above playbook with the additon of the <codeph>--limit
              &lt;new-client-node&gt;</codeph> switch.</note>
        </p>
        <p><b>Ceph Configuration</b></p>
        <p>Continue with the Ceph configuration with the steps below:</p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section: <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock>
                <note type="important">If you are using multiple backend types, you can use a comma
                  delimited list here. For example, if you are going to use both VSA and Ceph
                  backends, you would specify something like this:
                    <codeph>enabled_backends=vsa-1,ceph1</codeph>.</note></li>
              <li>[OPTIONAL] If you want your volumes to use a default volume type, then enter the
                name of the volume type in the <codeph>[DEFAULT]</codeph> section with the syntax
                below. <b>You will want to remember this value when you create your volume type
                  later.</b>
                <p>
                  <note type="important">If you do not specify a default type then your volumes will
                    default to a non-redundant RAID configuration. It is recommended that you create
                    a volume type and specify it here that meets your environments needs.</note>
                </p>
                <codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li>
              <li>Uncomment the <codeph>ceph</codeph> section and fill the values as per your
                cluster information. If you have more than one cluster, you will need to add another
                similar section with its respective values. In the following example only one
                cluster is added. <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="ceph_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>Use the secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:: <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>&lt;secret ID will be here></b>
pools:
    - name: volumes</codeblock>
                          <note type="important">You should generate and use your own secret ID. You
                            can utilize the <codeph>uuidgen</codeph> command to achieve
                          this.</note></entry>
                      </row>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>Use the username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: &lt;secret ID will be here>
pools:
    - name: volumes</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_pool</entry>
                        <entry>Use the pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below:
                          <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_ceph_conf</entry>
                        <entry>Enter your Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>volume_driver</entry>
                        <entry>Cinder volume driver. Leave this as the default value specified for
                          Ceph.</entry>
                      </row>
                      <row>
                        <entry>volume_backend_name</entry>
                        <entry>Name given to the Ceph backend.
                          <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
              </li>
            </ol></li>
          <li>To enable Cinder to backup to Ceph, make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values: <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="ceph_backup">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>backup_driver</entry>
                        <entry>Cinder volume driver. Leave this as the default value specified for
                          Ceph.</entry>
                      </row>
                      <row>
                        <entry>backup_ceph_conf</entry>
                        <entry>Enter your Ceph configuration file location, usually
                            <codeph>/etc/ceph/ceph.conf</codeph></entry>
                      </row>
                      <row>
                        <entry>backup_ceph_user</entry>
                        <entry>Use the user name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                      </row>
                      <row>
                        <entry>backup_ceph_pool</entry>
                        <entry>Use the pool name value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>
pools:
    - name: <b>backups</b></codeblock></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table></li>
            </ol>
          </li>
          <li>To enable Ceph as your Glance backend, make the following changes to the
              <codeph>~/helion/my_cloud/config/glance/glance-api.conf.j2</codeph> file: <ol>
              <li>Uncomment and edit the following values:
                <codeblock>default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</codeblock></li>
              <li>In the same file, comment out the following references to Swift:
                <codeblock>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</codeblock></li>
            </ol>
          </li>
          <li><b>IMPORTANT:</b> If you have pre-existing images in your Glance repo and want to use
            exclusively Ceph as a backend, you will need to do the following steps prior to
            configuring Ceph as your Glance backend: <ol>
              <li>Snapshot or delete all Nova instances using those images.</li>
              <li>Download the images locally that you want to save.</li>
              <li>Delete all of the images from Glance.</li>
            </ol>
            <p>After you have finished the Ceph configuration you will need to re-add those
              images.</p></li>
          <li>To enable attaching Ceph volumes to Nova provisioned instances, make the following
            changes to the <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph>
            file: <ol>
              <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &lt;ceph-user>
rbd_secret_uuid = &lt;secret-uuid></codeblock>
                <p>where:</p>
                <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                  <tgroup cols="2">
                    <colspec colname="c1" colnum="1"/>
                    <colspec colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry>Value</entry>
                        <entry>Description</entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry>rbd_user</entry>
                        <entry>Use the username value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
                          name: <b>cinder</b>
                          type: openstack
                          secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                      </row>
                      <row>
                        <entry>rbd_secret_uuid</entry>
                        <entry>Use the secret_id value from the
                            <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                          highlighted below::
                          <codeblock>- user:
                          name: <b>cinder</b>
                          type: openstack
                          secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                      </row>
                    </tbody>
                  </tgroup>
                </table>
                <note>To attach a volume provisioned out ofa newly-added Ceph backend to an existing
                  OpenStack instance, the instance must be rebooted after the new backend has been
                  added.</note></li>
            </ol></li>
          <li>Commit your configuration to the <xref href="../installation/using_git.dita">local git
              repo</xref>, as follows:
            <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Update your deployment directory:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Run the Cinder Reconfigure Playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
          <li>If Nova has been configured to attach Ceph backend volumes, run the Nova reconfigure
            playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
          <li>If Ceph has been configured as the Glance backend, run the Glance reconfigure
            playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
        </ol>
      </sectiondiv>
    </section>

    <section id="post_install"><title outputclass="headerH">Post-Installation Tasks</title>
      <sectiondiv outputclass="insideSection">
        <p>After you have configured Ceph as your Block Storage backend, here are some tasks you
          will want to complete:</p>
        <ul>
          <li><xref href="../administration/blockstorage/creating_voltype.dita">Create a Volume Type
              for your Volumes</xref></li>
          <li><xref href="installation_verification.dita#install_verification/volume_verify">Verify
              Your Block Storage Configuration</xref></li>
        </ul>
      </sectiondiv>
    </section>
  </body>
</topic>
