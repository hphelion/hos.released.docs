<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model. It consists of the following steps:</p>
    <ul>
      <li><xref href="configure_ceph.dita#config_ceph/deployer">Set Up the Lifecycle Manager as a
          Ceph Client</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/config_files">Editing your Ceph Environment
          Input Files</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/configure_backend">Configure Ceph as the
          Backend</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/create_voltype">Create a Volume Type for your
          Volumes</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/associate_voltype">Associate the Volume Type
          to the Backend</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/extra_specs">Extra Specification
          Options</xref></li>
      <li><xref href="configure_ceph.dita#config_ceph/post_install">Verifying your Ceph
          backend</xref></li>
    </ul>
    <section id="notes"><title>Notes</title>
      <p>The Ceph cluster expects the network group that will be used for management network traffic
        within the cloud to be left to its default value i.e. MANAGEMENT. Altering the name will
        lead to failures in the cloud deployment, therefore it should be avoided.</p>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
      <p>We include steps below to configure Ceph as your Glance backend. This replaces Swift which
        is the default backend option in HPE Helion OpenStack. So if you have any pre-existing
        images in your Glance repo you will want to download them locally prior to configuring Ceph
        as your backend. Once the Ceph configuration is complete you will then need to re-upload
        those to your Glance repo in order for those images to work.</p>
      <p><b>For HPE Helion OpenStack 2.0 Only</b></p>
      <p>While executing the <codeph>site.yml</codeph> playbook with the <codeph>--limit</codeph>
        option, it is also expected to include the Ceph monitor nodes in the list of restricted
        nodes if the cloud is deployed with a Ceph cluster.</p>
    </section>

    <section id="deployer"><title>Set Up the Lifecycle Manager as a Ceph Client</title>
      <p><b>This section only applies to HPE Helion OpenStack 2.1</b></p>
      <p>If you have a separate lifecycle manager node or if you want to set up the lifecycle
        manager as a Ceph client, then the lifecycle manager node should have the Ceph client
        installed on it along with the necessary configuration and keyring files required to access
        to the Ceph cluster. Use the <codeph>ceph-setup-deployer-as-client.yml</codeph> playbook to
        use the lifecycle manager node to access the Ceph cluster:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</codeblock>
    </section>

    <section id="config_files">
      <title>Edit Your Ceph Environment Input Files</title>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Copy the example configuration files into the required setup directory and edit them to
          contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
          <p>Begin inputting your environment information into the configuration files in the
              <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
          <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
              >Input Model</xref>.</p></li>
        <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and enter
          the details for the additional disks meant for OSD data and journal filesystems. <p>A
            sample <codeph>disks_osd.yml</codeph> is as follows:</p>
          <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
          <p>The above sample file contains three OSD nodes and two journal disks.</p>
          <p>The disk model has the following fields:</p>
          <table frame="all" rowsep="1" colsep="1" id="ceph1">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1"/>
              <colspec colname="c2" colnum="2"/>
              <thead>
                <row>
                  <entry>Value</entry>
                  <entry>Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry><b>device-groups</b></entry>
                  <entry>There can be several device groups. This allows different sets of disks to
                    be used for different purposes.</entry>
                </row>
                <row>
                  <entry><b>name</b></entry>
                  <entry>This is an arbitrary name for the device group. The name must be
                    unique.</entry>
                </row>
                <row>
                  <entry><b>devices</b></entry>
                  <entry>This is a list of devices allocated to the device group. A
                      <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                      <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                      <codeph>/dev/sdf</codeph> indicates that the device group is used by
                    Ceph.</entry>
                </row>
                <row>
                  <entry><b>consumer</b></entry>
                  <entry>This specifies the service that uses the device group. A
                      <codeph>name</codeph> field containing <b>ceph</b> indicates that the device
                    group is used by Ceph.</entry>
                </row>
                <row>
                  <entry><b>attrs</b></entry>
                  <entry>These are the attributes associated with the consumer.</entry>
                </row>
                <row>
                  <entry><b>usage</b></entry>
                  <entry>There can be several uses of devices for a particular service. In the above
                    sample, <codeph>usage</codeph> field contains <b>data</b> which indicates that
                    the device is used for data storage.</entry>
                </row>
                <row>
                  <entry><b>journal_disk</b></entry>
                  <entry>Disk to be used for storing the journal data. When running multiple Ceph
                    OSD daemons on a single node, a journal disk can be shared between OSDs of the
                    node.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <p><note>Ensure that disks designated to be used for the OSD data and journal storage must
              be in a clean state (i.e. any existing partitions must be deleted). If you do not
              perform this step, Ceph configuration fails with errors.</note></p></li>
        <li>[OPTIONAL] There are parameters for Ceph that can be edited by the admin in the
          locations described below. The default values will work but if you choose to change any of
          these values, ensure that you also change them where they are referenced in your
            <codeph>cinder.conf.j2</codeph> file, which we cover in the next section of this guide. <ul>
            <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
              <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                parameters:</p>
              <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>fsid</entry>
                      <entry>This is a unique identifier for the Ceph cluster and should be
                        generated prior to deploying a cluster (use the <codeph>uuidgen</codeph>
                        command to generate a new FSID). Once set, this value cannot be
                        changed.</entry>
                    </row>
                    <row>
                      <entry>ceph_cluster</entry>
                      <entry>
                        <p>Ceph clusters have a cluster name. The default cluster name is ceph, but
                          you may specify a different cluster name.</p>
                      </entry>
                    </row>
                    <row>
                      <entry>osd_settle_time</entry>
                      <entry>
                        <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                          services.</p>
                      </entry>
                    </row>
                    <row>
                      <entry>osd_journal_size</entry>
                      <entry>
                        <p>The size of the journal in megabytes.</p>
                      </entry>
                    </row>
                    <row>
                      <entry>data_disk_poll_attempts </entry>
                      <entry>Maximum number of attempts before attempting to activate an OSD for a
                        new disk (default value 5) </entry>
                    </row>
                    <row>
                      <entry>data_disk_poll_interval</entry>
                      <entry>Time interval in seconds to wait between data_disk_poll_attempts
                        (default value 12) </entry>
                    </row>
                  </tbody>
                </tgroup>
              </table></li>
            <li>
              <p>Add any additional configuration parameters for Ceph in the same file
                  (<codeph>settings.yml</codeph> file) under the 'extra:' category as follows:</p>
              <codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock>
            </li>
            <li>
              <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
              <p>The <codeph>user_model.yml</codeph> has the editable values for the different pools
                created by HPE Helion OpenStack.</p>
              <table frame="all" rowsep="1" colsep="1" id="table_qbk_mpq_35">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                  <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>secret_id</entry>
                      <entry>A UUID that should be generated prior to configuring the Ceph client
                        nodes. The secret_id specified here is used by the libvirt process to access
                        the Ceph cluster while attaching a block device from Cinder.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </li>
          </ul></li>
        <li>Commit your
          configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
      </ol>
      <p>After your configuration files are setup, continue with the <xref
          href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
          installation steps.</xref></p>
    </section>

    <section id="configure_backend">
      <title>Configure Ceph as the Backend</title>
      <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
        will show you how to do this.</p>
      <p>Perform the following procedure on the lifecycle manager to configure Ceph as a volume
        backend:</p>
      <p><b>Prerequisites</b></p>
      <p>In order for Ceph to be used as a backend for volumes, the nodes running these services
        should have the Ceph client installed on them. Use the
          <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on these
        nodes.</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
      <p>
        <note>The steps below install required packages and configure existing client nodes (i.e.
          Cinder, Glance and Nova Compute nodes) to use the Ceph cluster. However, for any new
          client nodes added later on that need to be configured to use the Ceph cluster, just
          execute the above playbook with the additon of the <codeph>--limit
            &lt;new-client-node&gt;</codeph> switch.</note>
      </p>
      <p><b>For HPE Helion OpenStack 2.0 Only</b></p>
      <p>After executing the <codeph>ceph-client-prepare.yml</codeph> playbook, copy the following
        Ceph keyring files from the lifecycle manager node to the <codeph>/etc/ceph</codeph>
        directory of all of the controller nodes:</p>
      <codeblock>/etc/ceph/ceph.client.cinder-backup.keyring
/etc/ceph/ceph.client.cinder.keyring
/etc/ceph/ceph.client.glance.keyring
/etc/ceph/client.cinder.key</codeblock>
      <p>Also ensure that the files have <codeph>0644</codeph> permissions by executing the command
        below for each of the files on each of the controller nodes:
        <codeblock>chmod 0644 &lt;file path></codeblock></p>
      <p>For example:</p>
      <codeblock>chmod 0644 /etc/ceph/ceph.client.cinder-backup.keyring</codeblock>
      <p><b>Ceph Configuration</b></p>
      <p>Continue with the Ceph configuration with the steps below:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Make the following changes to the
            <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
            <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section:
              <codeblock># Configure the enabled backends
enabled_backends=ceph1</codeblock></li>
            <!--<li>[OPTIONAL] If you want a use a default volume type, then enter it in the
                  <codeph>[DEFAULT]</codeph> section with the syntax below. You will want to
                remember this value when you create your volume type in the next section.
                <codeblock>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type></codeblock></li> -->
            <li>Uncomment the <codeph>ceph</codeph> section and fill the values as per your cluster
              information. If you have more than one cluster, you will need to add another similar
              section with its respective values. In the following example only one cluster is
              added. <codeblock>[ceph1]
rbd_secret_uuid = &#60;secret-uuid>
rbd_user = &#60;ceph-cinder-user>
rbd_pool = &#60;ceph-cinder-volume-pool>
rbd_ceph_conf = &#60;ceph-config-file>
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &#60;ceph-backend-name></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="ceph_volume">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>rbd_secret_uuid</entry>
                      <entry>Use the secret_id value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below:: <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>&lt;secret ID will be here></b>
pools:
    - name: volumes</codeblock>
                        <note type="important">You should generate and use your own secret ID. You
                          can utilize the <codeph>uuidgen</codeph> command to achieve
                        this.</note></entry>
                    </row>
                    <row>
                      <entry>rbd_user</entry>
                      <entry>Use the username value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: &lt;secret ID will be here>
pools:
    - name: volumes</codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_pool</entry>
                      <entry>Use the pool name value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below:
                        <codeblock>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <b>volumes</b></codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_ceph_conf</entry>
                      <entry>Enter your Ceph configuration file location, usually
                          <codeph>/etc/ceph/ceph.conf</codeph></entry>
                    </row>
                    <row>
                      <entry>volume_driver</entry>
                      <entry>Cinder volume driver. Leave this as the default value specified for
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry>volume_backend_name</entry>
                      <entry>Name given to the Ceph backend.
                        <!--You will specify this value later in
                          the <xref href="configure_vsa.dita#config_vsa/associate_volume_backend"
                            >Associate the Volume Type to a Backend</xref> steps.--></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </li>
          </ol></li>
        <li>To enable Cinder to backup to Ceph, make the following changes to the
            <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
            <li>Uncomment the <codeph>ceph backup</codeph> section and fill the values: <codeblock>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &#60;ceph-config-file>
backup_ceph_user = &#60;ceph-backup-user>
backup_ceph_pool = &#60;ceph-backup-pool></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="ceph_backup">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>backup_driver</entry>
                      <entry>Cinder volume driver. Leave this as the default value specified for
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry>backup_ceph_conf</entry>
                      <entry>Enter your Ceph configuration file location, usually
                          <codeph>/etc/ceph/ceph.conf</codeph></entry>
                    </row>
                    <row>
                      <entry>backup_ceph_user</entry>
                      <entry>Use the user name value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder-backup</b>
    type: openstack
pools:
    - name: backups</codeblock></entry>
                    </row>
                    <row>
                      <entry>backup_ceph_pool</entry>
                      <entry>Use the pool name value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>
pools:
    - name: <b>backups</b></codeblock></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table></li>
          </ol>
        </li>
        <li>To enable Ceph as your Glance backend, make the following changes to the
            <codeph>~/helion/my_cloud/config/glance/glance-api.conf.j2</codeph> file: <ol>
            <li>Uncomment and edit the following values:
              <codeblock>default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</codeblock></li>
            <li>In the same file, comment out the following references to Swift:
              <codeblock>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</codeblock></li>
          </ol>
        </li>
        <li><b>IMPORTANT:</b> If you have pre-existing images in your Glance repo and want to use
          exclusively Ceph as a backend, you will need to do the following steps prior to
          configuring Ceph as your Glance backend: <ol>
            <li>Snapshot or delete all Nova instances using those images.</li>
            <li>Download the images locally that you want to save.</li>
            <li>Delete all of the images from Glance.</li>
          </ol>
          <p>After you have finished the Ceph configuration you will need to re-add those
            images.</p></li>
        <li>To enable attaching Ceph volumes to Nova provisioned instances, make the following
          changes to the <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> file: <ol>
            <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &lt;ceph-user>
rbd_secret_uuid = &lt;secret-uuid></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>rbd_user</entry>
                      <entry>Use the username value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
                          name: <b>cinder</b>
                          type: openstack
                          secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_secret_uuid</entry>
                      <entry>Use the secret_id value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
                          name: <b>cinder</b>
                          type: openstack
                          secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
              <note>To attach a volume provisioned out ofa newly-added Ceph backend to an existing
                OpenStack instance, the instance must be rebooted after the new backend has been
                added.</note></li>
          </ol></li>
        <!-- DOCS-2313
        <li>Copy the corresponding keyring files to the controller nodes: <ol>
            <li>Log in to the controller nodes as a user with sudo access and run the following
              commands: <p>For Ceph as your volume backend:
                <codeblock>sudo ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring</codeblock></p>
              <p>For Ceph as your volume backup backend:
                <codeblock>sudo ceph auth get-or-create client.cinder-backup | sudo tee -a /etc/ceph/ceph.client.cinder-backup.keyring</codeblock></p>
              <p>For Ceph as your Glance backend:
                <codeblock>sudo ceph auth get-or-create client.glance | sudo tee -a  /etc/ceph/ceph.client.glance.keyring</codeblock></p>
              <p><b>OR</b></p>
            </li>
            <li>You can copy the keyring from the lifecycle manager to <codeph>/etc/ceph</codeph>
              folder on all the controller nodes: <p>For Ceph as your volume backend:</p>
              <codeblock>scp /etc/ceph/ceph.client.cinder.keyring</codeblock>
              <p>For Ceph as your volume backup backend:</p>
              <codeblock>scp /etc/ceph/ceph.client.cinder-backup.keyring</codeblock>
              <p>For Ceph as your Glance backend:</p>
              <codeblock>scp /etc/ceph/ceph.client.glance.keyring</codeblock></li>
          </ol></li>
          -->
        <li>Commit your configuration to a <xref href="using_git.dita">local repository</xref>: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock>
          <note>Before you run any playbooks, remember that you need to export the encryption key in
            the following environment variable: <codeph>export
              HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph> See <xref
              href="install_entryscale_kvm.dita#install_kvm"/> for reference.</note></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the following command to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Run the Cinder Reconfigure Playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
        <li>If Nova has been configured to attach Ceph backend volumes, run the Nova reconfigure
          playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
        <li>If Ceph has been configured as the Glance backend, run the Glance reconfigure playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></li>
      </ol>
    </section>

    <!-- DOCS-2313
    <section id="attach_ceph">
      <title>Configure Nova to Allow Attachment of Ceph Volumes to Instances</title>
      <p>If you want to attach a volume from a newly-added Ceph backend to an existing Nova virtual
        machine, you must reboot your virtual machine after the new backend has been added.</p>
      <p>Perform the following steps to configure Nova to allow attachment of a Ceph volume to an
        instance:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Make the following changes to the
            <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> file: <ol>
            <li>Uncomment the Ceph backend lines and edit them as follows: <codeblock>[libvirt]
rbd_user = &#60;ceph-user>
rbd_secret_uuid = &#60;secret-uuid></codeblock>
              <p>where:</p>
              <table frame="all" rowsep="1" colsep="1" id="nova_volume">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>rbd_user</entry>
                      <entry>Use the username value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</codeblock></entry>
                    </row>
                    <row>
                      <entry>rbd_secret_uuid</entry>
                      <entry>Use the secret_id value from the
                          <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph> file,
                        highlighted below::
                        <codeblock>- user:
    name: <b>cinder</b>
    type: openstack
    secret_id: <b>457eb676-33da-42ec-9a8c-9293d545c337</b></codeblock></entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </li>
          </ol></li>
        <li>Commit your configuration to a <xref href="using_git.dita">local repository</xref>:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;your commit message>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the following command to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Run the Nova Reconfigure Playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
      </ol>
    </section>
    -->

    <section conref="configure_vsa.dita#config_vsa/create_volumetype" id="create_voltype"/>
    <section conref="configure_vsa.dita#config_vsa/associate_volumetype" id="associate_voltype"/>

    <section id="extra_specs"><title>Extra Specification Options</title>
      <p>Ceph supports volumes creation with additional attributes. All these attributes can be
        specified using the extra specs options for your volume type. The administrator is expected
        to define appropriate extra spec for Ceph volume type as per the guidelines provided at
          <xref
          href="http://docs.openstack.org/kilo/config-reference/content/ceph-rados.html#d6e2255"
          format="html" scope="external">here</xref>.</p>
      <p>The following Cinder Volume Type extra-specs option specifies the volume backend name that
        is used:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_ntq_swv_yt">
        <tgroup cols="3">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <thead>
            <row>
              <entry>Key</entry>
              <entry>Value</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>volume_backend_name</entry>
              <entry><i>volume backend name</i></entry>
              <entry>The name of the backend to which you want to associate the volume type, which
                you also specified earlier in the <codeph>cinder.conf.j2</codeph> file.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>Here is what a completed list of extra specs may look like:</p>
      <p><image href="../../media/blockstorage/extraspecs_ceph.png"/></p>
    </section>

    <section id="post_install"><title>Verifying your Ceph backend</title>
      <p>After you have configured Ceph as your Block Storage backend, you can verify this all
        completed successfully by creating a new volume.</p>
      <p>See <xref href="installation_verification.dita">Verifying your Installation</xref> for more
        details.</p>
    </section>
  </body>
</topic>
