<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="restoreSharedController">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Restoring a Shared Controller</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
 
 <section> Sub steps -<xref
        href="../installation/install_entryscale_kvm.dita#install_kvm/setup_deployer"
        >http://docs.hpcloud.com/helion/installation/install_entryscale_kvm.html#install_kvm__setup_deployer</xref></section>
    <section> Deployer disaster recovery -<xref
        href="cloud_control_plane_recovery.dita#topic_tb4_lqy_qt"
        >http://docs.hpcloud.com/helion/bura/cloud_control_plane_recovery.html#topic_tb4_lqy_qt__deployer</xref>
    </section>
    <section>Main Documentation Link -<xref
        href="../operations/replace_controller.dita#replacing_controller"
        >http://docs.hpcloud.com/#helion/operations/replace_controller.html</xref> Set up the
      Lifecycle-manager <ol id="ol_e4s_nfz_bv">
        <li>Ensuring that you use the same version of HPE Helion OpenStack that you previously had
          loaded on your lifecycle manager, you will need to download and install the lifecycle
          management software using the instructions from the installation guide: </li>
        <li> Then you will want to restore your data using theDeployer Disaster
          Recoveryinstructions. These details are included at the End of the document </li>
        <li> Update your cloud model (servers.yml) with the newmac-addr,ilo-ip,ilo-password,
          orilo-userfields where these have changed. Do not change theid,ip-addr,role,
          orserver-groupsettings. (Please follow the procedure for updating your cloud model in the
          git repo) </li>
        <li> Get theservers.ymlfile stored in git:cd ~/helion/my_cloud/definition/datagit checkout
          sitethen change, as necessary, themac-addr,ilo-ip,ilo-password, andilo-userfields of this
          existing controller node. Save and commit the changegit commit -a -m "repaired node X" </li>
        <li> Run the configuration processor as follows:
          <codeblock>cd ~/helion/hos/ansible 
     ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          Then run ready-deployment:
          <codeblock>cd ~/helion/hos/ansible
       ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li> Once that is complete, copy the Cobbler images to the correct location:
          <codeblock>sudo mkdir -p /srv/www/cobbler/ks_mirror/hlinux-cattleprod 
       sudo cp /opt/hlm_packager/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux /srv/www/cobbler/ks_mirror/hlinux-cattleprod
       sudo cp /opt/hlm_packager/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz /srv/www/cobbler/ks_mirror/hlinux-cattleprod</codeblock>
        </li>
        <li> Deploy Cobbler:
          <codeblock>cd ~/helion/hos/ansible
       ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
          Note:After this step you may see failures because MySQL has not finished syncing. If so,
          please rerun this step (7). </li>
        <li> Delete the haproxy user: <codeblock>sudo deluser haproxy</codeblock>
        </li>
        <li> Manually copy the /etc/group file from a backup of the old deployer. </li>
        <li> NOTE: Currently Freezer does not backup /etc/group file, so it won't get restored when
          freezer deployer backup is restored. Till that is fixed, we need to follow this workaround
          Remove the /home/dbadmin/.ssh/id_rsa file to rebuild the SSH keys required to install
          vertica <codeblock>sudo rm /home/dbadmin/.ssh/id_rsa</codeblock>
        </li>
        <li> Configure the necessary keys used for the database etc:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
       ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</codeblock>
          Run osconfig on the replacement controller node. For example: </li>
        <li>
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
   ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
          If the controller being replaced is the first server running the swift-proxy service
          (seeIdentifying the First Proxy Server) you need to restore the Swift Ring Builder files
          to the /etc/swiftlm/builder_dir directory. SeeRecovering Builder Filesfor details. </li>
        <li> Run the hlm-deploy playbook on the replacement controller.If the node being replaced is
          the first proxy node then you only need to use the--limitswitch for that node, otherwise
          you need to specify the hostname of your first proxy and the hostname of the node being
          replaced.cd ~/scratch/ansible/next/hos/ansible </li>
        <li>
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname>,&lt;first-proxy-hostname></codeblock>
          If the node being replaced does not host any swift services then you only need to use the
          --limit switch for that node
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
       ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
        </li>
      </ol>
      <p>Deployer Disater Recovery Options On the lifecycle manager, install the freezer-agent, as
        follows:</p>
      <codeblock>cd ~/helion/hos/ansible/ansible-playbook -i hosts/localhost _deployer_restore_helper.yml -e '{ "old_deployer_hostname": "&lt;here put the hostname of the server that was your deployer>" }'</codeblock>
      <ol id="ol_mts_nfz_bv">
        <li> You must retrieve a few files from any compute or controller node and put them in the
          directory /opt/stack/service/freezer-agent/etc/ of the lifecycle manager:
          /opt/stack/service/freezer-agent/etc/backup.osrcand/opt/stack/service/freezer-agent/etc/systemd_env_vars.cfg
        </li>
      </ol> Restore from a Swift backup Everything is still running (control plane nodes and compute
      nodes) but you lost the lifecycle manager. Shared Deployer/Controller Replacement Procedure
      Thursday, February 04, 2016 10:32 PM HOS 3.0 Page 1 Question: When deployer is down, we can't
      access either compute or controller nodes unless the public keys are stored somewhere But this
      is not documented. #Become root <ol id="ol_rws_nfz_bv">
        <li> sudo su # Run the following command to change the host name </li>
        <li> hostname &lt;here put the hostname of the server that was your deployer> #Edit the
          /etc/hosts and replace the default hLinux install name (hlm) with the old deployer name
          which modified in above step iii. </li>
        <li> Change the hostname of the server so it points to the hostname of the server that was
          your deployer and update /etc/hosts file according On the lifecycle manager: Become root
          <codeblock>sudo su</codeblock> Source credentials
          <codeblock>source /opt/stack/service/freezer-agent/etc/backup.osrc</codeblock> List jobs
          <codeblock>freezer-scheduler -c &lt;hostname> job-list</codeblock>
        </li>
        <li> Question>> backup.osrc file points to HOSTNAME of the VIP address. <p/>When deployer is
          reinstalled it won't have details of the HOSTNAME of the VIP in the /etc/hosts file. This
          command would fail, example <p/>
          <codeblock>curl http://FoxHOS20HDP-ccp1-vip-KEY-API-mgmt:5000 curl
              Could not resolve host: FoxHOS20HDP-ccp1-vip-KEY-API-mgmt </codeblock>
          We need to add to the document to edit the /etc/hosts file of the deployer and add the VIP
          hostname/IP address details. # Get the id of the job corresponding to "HLM Default:
          Deployer restore from Swift"# Stop that job so the freezer-scheduler doesn't begin to
          backup when started
          <codeblock>freezer-scheduler -c &lt;hostname> job-stop -j &lt;job-id></codeblock> If it is
          present, also stop the deployer's ssh backup # Stop the dayzero UI
          <codeblock>systemctl stop dayzero</codeblock> Start the freezer-scheduler
          <codeblock>systemctl start freezer-scheduler</codeblock> Get the id of the job
          corresponding to "HLM Default: deployer restore from Swift"# Launch that job
          <codeblock>freezer-scheduler -c &lt;hostname> job-start -j &lt;job-id></codeblock> Wait
          for some time, you can follow the /var/log/freezer-agent/freezer-agent.log Start the
          dayzero UI <codeblock>systemctl start dayzero</codeblock> When the lifecycle manager is
          restored, re-run the deployment to ensure the lifecycle manager is in the correct state:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
              ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock>
        </li>
        <li> On the lifecycle manager, edit the following file so it contains the same information
          as previously: <codeblock>~/helion/my_cloud/config/freezer/ssh_credentials.yml</codeblock>
        </li>
        <li> On the lifecycle manager:
          <codeblock>cd ~/helion/hos/ansible/
              ansible-playbook -i hosts/localhost _deployer_restore_helper.yml -e '{ "old_deployer_hostname": "&lt;here put the hostname of the server that was your deployer>"}'</codeblock>
        </li>
        <li> #Become root </li>
        <li>
          <codeblock>sudo su</codeblock> # Run the following command to change the host name </li>
        <li>
          <codeblock>hostname &lt;here put the hostname of the server that was your deployer></codeblock>
          #Edit the /etc/hosts and replace the default hLinux install name (hlm) with the old
          deployer name which modified in above step iii. </li>
        <li> Change the hostname of the server so it points to the hostname of the server that was
          your deployer and update /etc/hosts file according Perform the restore, as follows:
          <codeblock>sudo su
              cd /root/deployer_restore_helper/</codeblock> Stop the
          Dayzero UI <codeblock>systemctl stop dayzero</codeblock> execute the restore.
          <codeblock>/deployer_restore_script.sh</codeblock> Start the Dayzero UI
          <codeblock>systemctl start dayzero</codeblock>
        </li>
        <li> When the lifecycle manager is restored, re-run the deployment to ensure the lifecycle
          manager is in the correct state:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
              ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock>
        </li>
        <li> Restore from an SSH backup HOS 3.0 Page 2 </li>
      </ol>
    </section>
  </body>
</topic>
