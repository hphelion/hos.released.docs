<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_tb4_lqy_qt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering the Cloud Control Plane</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>  
    
    <section>There are seven supported recovery scenarios, which are explained in the following
      sections:<p>&nbsp;</p></section>
    <section>
      <ul>
        <li><p outputclass="pageAnchor1" id="pointdb">Point in time database recovery</p></li>
        <li><p outputclass="pageAnchor1" id="pointswift">Point in time swift rings
          recovery</p></li>
        <li><p outputclass="pageAnchor1" id="pointlifecycle">Point in time lifecycle manager
            recovery</p></li>
        <li><p outputclass="pageAnchor1" id="onecontroller">One or two controller disaster recovery</p></li>
        <li><p outputclass="pageAnchor1" id="allcontrollers">Three control plane node disaster
            recovery</p></li>
        <li><p outputclass="pageAnchor1" id="nolifecycle">Deployer disaster recovery</p></li>
        <li><p outputclass="pageAnchor1" id="full">Full disaster recovery</p></li> 
  
      </ul>
      
      
    </section>
    <section outputclass="pageTarget1" id="pointdb_target"><title>Point in time database recovery</title><p>In this scenario, everything is still running (lifecycle manager, cloud controller nodes, and
        compute nodes)  but you want to restore the MySQL database to a previous state. </p></section>
    <section><title>Restore from a Swift backup</title></section>
    <ol>
      <li>On the first MySQL node, run the following steps:
        <codeblock>
# Become root
sudo su
 
# Create restore directory
mkdir /tmp/hlm_mysql_restore/
 
# Source backup environment file
source /opt/stack/service/freezer-agent/etc/backup.osrc
 
# List jobs
freezer-scheduler -c &lt;hostname&gt; job-list
 
# Get the id corresponding to the job "HLM Default: Mysql restore from Swift"
 
# Launch the restore
freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;
 
# Wait for some time, you can follow the /var/log/freezer-agent/freezer-agent.log</codeblock>
      </li>
      <li>From the lifecycle manager run the following steps:
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
      <li>On the first MySQL node run the following steps:
        <codeblock># Clean Mysql directory
sudo rm -r /var/lib/mysql/*
 
# Copy restored files
sudo cp -pr /tmp/hlm_mysql_restore/* /var/lib/mysql</codeblock></li>
      <li>From the lifecycle manager run the following steps:
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock></li>
    </ol>
    <section><title>Restore from an SSH backup</title><p>Follow the same procedure as the one for
        Swift but select the job "HLM Default: Mysql restore from SSH".</p></section>
    <section><title>Restore MySQL Manually </title>
      <p>If restoring MySQL fails during the percona-bootstrap procedure, you can use the following
        procedure instead:</p>
      <ol>
        <li>From the lifecycle manager, launch the following commands to stop the MySQL
          cluster:<codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
        <li>On all MySQL nodes, run the following command to purge the old
          database:<codeblock>
sudo rm -r /var/lib/mysql/*</codeblock></li>
        <li>On the first MySQL node, restore the backup as follows:<ol>
            <li>If you already restored to a temporary directory, copy the files again:
              <codeblock>
sudo cp -pr /tmp/hlm_mysql_restore/* /var/lib/mysql</codeblock></li>
            <li>If you need to restore the files manually from SSH, follow the next
              steps:<codeblock># Become root
sudo su
 
# Create the /root/mysql_restore.ini file containing the following: (Be careful to substitute the {{ value }} )
# SSH informations are the one configured for backup before installing.
 
[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
restore_from_host = {{ hostname of the first MySQL node }}
 
# Execute the restore
freezer-agent --config /root/mysql_restore.ini</codeblock></li>
            <li>On the first MySQL node, follow the next steps to start the
              cluster:<codeblock># Become root
sudo su
 
# When the last step executed successfully, start the MySQL cluster
/etc/init.d/mysql bootstrap-pxc
 
# Start the process with systemctl to make sure the process is monitored by upstard
systemctl start mysql
 
# Make sure the mysql process started successfully
systemctl status mysql</codeblock></li>

          </ol></li>
        <li>From the lifecycle manager, launch the following commands to start all
          MySQLs:<codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-start.yml
 
# Mysql cluster status can be checked using 
ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock></li>
        <li>On all MySQL nodes run the following
          commands:<codeblock>
sudo su
touch /var/lib/mysql/galera.initialised
chown mysql:mysql /var/lib/mysql/galera.initialised</codeblock></li>
        <li>After approx. 10-15 minutes the output of percona-status should show all the MySQL nodes
          in
          sync:<codeblock># Mysql cluster status can be checked using 
ansible-playbook -i hosts/verb_hosts percona-status.yml
 
TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] ************* 
ok: [helion-cp1-c1-m1-mgmt] => {
    "msg": "mysql is synced."
}
ok: [helion-cp1-c1-m2-mgmt] => {
    "msg": "mysql is synced."
}
ok: [helion-cp1-c1-m3-mgmt] => {
    "msg": "mysql is synced."
}</codeblock></li>
      </ol>
    </section>
    <section outputclass="pageTarget1" id="pointswift_target"><title>Point in time swift rings recovery</title><p>Everything is still running (lifecycle manager, control plane nodes, and compute nodes) but you
        want to restore the Swift rings to an old state.</p><note>Freezer backs up and restores for Swift rings only, not Swift
      data.</note></section>
    <section><title>Restore from a Swift backup</title><ol>
        <li>On the first Swift Proxy (SWF-PRX[0]) node run the following
          steps:<codeblock>
# Become root
sudo su
 
# Create restore directorie
mkdir /tmp/hlm_builder_dir_restore/ 
 
# Source backup environment file
source /opt/stack/service/freezer-agent/etc/backup.osrc
 
# List jobs
freezer-scheduler -c &lt;hostname&gt; job-list
 
# Get the id corresponding to the job "HLM Default: Swift restore from Swift"
 
# Launch the restore
freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;
 
# Wait for some time, you can follow the /var/log/freezer-agent/freezer-agent.log</codeblock></li>
        <li>On the lifecycle manager:
          <codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml</codeblock></li>
        <li>On the first Swift Proxy (SWF-PRX[0]) run the following steps:
          <codeblock>
# Copy restored files
cp -pr /tmp/hlm_builder_dir_restore/* /etc/swiftlm/builder_dir/</codeblock></li>
        <li>On the lifecycle manager:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
      </ol></section>
    <section><title>Restore from an SSH backup</title><p>Follow the same procedure as for Swift but
        select the job "HLM Default: Swift restore from SSH".</p></section>
    <section outputclass="pageTarget1" id="pointlifecycle_target"><title>Point in time lifecycle manager recovery</title><p>Everything is still running (lifecycle manager, control plane nodes, and compute nodes) but you
        want to restore the lifecycle manager to an old state.</p></section>
    <section>
      <title>Restore from a Swift backup</title><p>On the lifecycle manager, run the following
        steps:
        <codeblock>
# Become root
sudo su
 
# Source backup environment file
source /opt/stack/service/freezer-agent/etc/backup.osrc
 
# List jobs
freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-list
 
# Get the id corresponding to the job "HLM Default: Deployer restore from swift"
 
# Stop the Dayzero UI
systemctl stop dayzero
 
# Launch the restore
freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-start -j &lt;job-id&gt;
 
# Wait for some time, you can follow the /var/log/freezer-agent/freezer-agent.log
 
# Start the Dayzero UI
systemctl start dayzero</codeblock></p>
    </section>
    <section><title>Restore from an SSH backup</title><p> Follow the same procedure as for Swift but
        select the job "HLM Default: Deployer restore from SSH".</p></section>
    <section outputclass="pageTarget1" id="onecontroller_target"><title>One or two control plane node disaster recovery</title><p>Everything is still running (lifecycle manager, control plane node, and compute nodes) but you
        lost one or two CCNs.</p></section>
    <section><title>Restore from other nodes</title><p>On the lifecycle manager, install the nodes
        that got destroyed:
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit padawan-ccp-c1-m2-mgmt</codeblock></p></section>
    <section><title>Restore from a Swift backup</title><p>You shouldn’t need to restore anything:
        the MySQL database will be recovered automatically from other running nodes.</p></section>
    <section><title>Restore from an SSH backup </title><p>You shouldn’t need to restore anything:
        the MySQL database will be recovered automatically from other running nodes.</p></section>
    <section outputclass="pageTarget1" id="allcontrollers_target"><title>3 control plane node disaster recovery</title><p>All control plane nodes are destroyed.</p><ul>
        <li><b>Restore from a Swift backup: </b><p>This is not possible (Swift is gone).</p></li>
        <li><b>Restore from an SSH backup: </b><p>
            <ol>
              <li>On lifecycle manager, follow the procedure to deploy the control plane nodes:
                <codeblock>
cd ~/scratch/ansible/next/hos/ansible
 
ansible-playbook -i hosts/verb_hosts site.yml --limit padawan-ccp-c1-m1-mgmt,padawan-ccp-c1-m2-mgmt,padawan-ccp-c1-m3-mgmt -e '{ "freezer_backup_jobs_upload": false }'</codeblock></li>
              <li>You can now perform the procedure: "Point in time database recovery:"</li>
              <li>Once everything is restored, re-enable the backups. From the lifecycle manager:
                <codeblock>
cd ~/scratch/ansible/next/hos/ansible
 
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
            </ol>
          </p></li>
      </ul></section>
    <section outputclass="pageTarget1" id="nolifecycle_target"><title>Deployer disaster recovery</title><p>Everything is still running (control plane nodes and compute nodes) but you lost the lifecycle
        manager.</p><ul>
        <li><b>Restore from a Swift backup</b>
          <ol>
            <li>On the lifecycle manager, install the freezer-agent, as
              follows:<codeblock>
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock></li>
            <li>You must retrieve a few files from any compute or controller node and put them in
              the directory /opt/stack/service/freezer-agent/etc/ of the lifecycle
              manager:<codeblock>
/opt/stack/service/freezer-agent/etc/backup.osrc
and
/opt/stack/service/freezer-agent/etc/systemd_env_vars.cfg</codeblock></li>
            <li>You must retrieve the /etc/hosts file from any compute or controller node and
              replace the lifecycle manager's one with the retrieved
              one:<codeblock>
# Be sure to edit the 127.0.0.1 line so it points to hlinux like
 
127.0.0.1       localhost
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters
127.0.0.1       hlinux</codeblock></li>
            <li>On the lifecycle manager:
              <codeblock>
# Become root
sudo su
 
# Source credentials
source /opt/stack/service/freezer-agent/etc/backup.osrc
 
# List jobs
freezer-scheduler -c &lt;hostname> job-list
 
# Get the id of the job corresponding to "HLM Default: deployer backup"
 
# Stop that job so the freezer-scheduler doesn't begin to backup when started
freezer-scheduler -c &lt;hostname> job-stop -j &lt;job-id&gt;
 
# If it is present, also stop the deployer's ssh backup
 
# Stop the dayzero UI
systemctl stop dayzero
 
# Start the freezer-scheduler
systemctl start freezer-scheduler
 
# Get the id of the job corresponding to "HLM Default: deployer restore from Swift"
 
# Launch that job
freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;
 
# Wait for some time, you can follow the /var/log/freezer-agent/freezer-agent.log
 
# Start the dayzero UI
systemctl start dayzero</codeblock></li>
            <li>When the lifecycle manager is restored, re-run the deployment to ensure the
              lifecycle manager is in the correct
              state:<codeblock>
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
          </ol></li>
        <li><b>Restore from an SSH backup</b>
          <ol>
            <li>On the lifecycle manager, edit the following file so it contains the same
              information as previously:
              <codeblock>~/helion/my_cloud/config/freezer/ssh_credentials.yml</codeblock></li>
            <li>On the lifecycle manager:
              <codeblock>
cp -r ~/hp-ci/padawan/* ~/helion/my_cloud/definition/
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock></li>
            <li>Perform the restore, as
              follows:<codeblock>
sudo su
cd /root/deployer_restore_helper/
 
# Stop the Dayzero UI
systemctl stop dayzero
 
# execute the restore
./deployer_restore_script.sh
 
# Start the Dayzero UI
systemctl start dayzero</codeblock></li>
            <li>When the lifecycle manager is restored, re-run the deployment to ensure the
              lifecycle manager is in the correct state:
              <codeblock>
cd ~/scratch/ansible/next/hos/ansible
 
ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
          </ol></li>
      </ul></section>
    <section outputclass="pageTarget1" id="full_target"><title>Full disaster recovery</title><p>Everything is dead.</p><ul>
        <li><b>Restore from a Swift backup:</b><p>This is not possible (Swift is gone).</p></li>
        <li><b>Restore from an SSH backup: </b><ol>
            <li>On the lifecycle manager, edit the following file so it contains the same
              information as previously:
              <codeblock>~/helion/my_cloud/config/freezer/ssh_credentials.yml file</codeblock></li>
            <li>On the lifecycle manager:
              <codeblock>cp -r ~/hp-ci/padawan/* ~/helion/my_cloud/definition/
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock></li>
            <li>Perform the restore:
              <codeblock>sudo su
cd /root/deployer_restore_helper/
 
# Stop the Dayzero UI
systemctl stop dayzero
 
# execute the restore
./deployer_restore_script.sh
 
 
# Start the Dayzero UI
systemctl start dayzero</codeblock></li>
            <li>Follow the procedure to deploy your cloud:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
 
ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_backup_jobs_upload": false }'</codeblock></li>
            <li>You can now perform the procedures to restore MySQL and Swift.</li>
            <li>Once everything is restored, re-enable the backups from the lifecycle manager:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
 
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
          </ol></li>
      </ul></section>
    <section><title>Swift rings recovery</title><ul>
        <li><b>Restore from the Swift deployment backup</b>
          <p>As long as you have one Swift Proxy or one Swift object node still working, you can
            recover the Swift rings from it. This is made possible because rings are created at the
            deployment step, backed up, and copied to all Swift nodes in the form of:
            /etc/swiftlm/swift-rings-tarball.tar.</p><p>To manage rings, make sure the content of
            that tar file is in the directory /etc/swiftlm/builder_dir/ on the first Swift Proxy
            node (SWF-PXR[0] role). <ul>
              <li>Run swift-reconfigure if the rings were
                corrupted:<codeblock># On the lifecycle manager run 
 
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
              <li>Or swift-deploy if one node was
                lost:<codeblock># On the lifecycle manager run 
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
            </ul>
          </p></li>
        <li><b>Restore from the SSH Freezer backup</b><p>In the very specific use case where you
            lost all system disks of all object nodes, and swift proxy nodes are corrupted, a copy
            of the Swift rings is stored in Freezer. This means that Swift data is still there (the
            disks used by Swift needs to be still accessible).</p><ol>
            <li>Recover the rings, as
              follows:<codeblock># You will need a node with the freezer-agent installed.
 
# Become root
sudo su
 
# Create the temporary directory to restore files
mkdir /tmp/hlm_builder_rid_restore/
 
# Create a restore file with the following content
cat &lt;&lt; EOF &gt; ./restore_config.ini
[default]
action = restore
storage = ssh
compression = bzip2
restore_abs_path = /tmp/hlm_builder_rid_restore/
ssh_key = /etc/freezer/ssh_key
ssh_host = &lt;freezer_ssh_host&gt;
ssh_port = &lt;freezer_ssh_port&gt;
ssh_user name = &lt;freezer_ssh_user name&gt;
container = &lt;freezer_ssh_base_rid>/freezer_swift_backup_name = freezer_swift_builder_backup
restore_from_host = &lt;hostname of the old first Swift-Proxy (SWF-PRX[0])&gt;
EOF
 
# Edit the file and repave all &lt;tags&gt; with the right informations.
vim ./restore_config.ini
 
# You will also need to put the ssh key used to do the backups in /etc/freezer/ssh_key (don't forget to set the right permissions: 600)
 
# Execute the restore
freezer-agent --config ./restore_config.ini
 
# You now have the Swift rings in /tmp/hlm_builder_dir_restore/</codeblock></li>
            <li>If the SWF-PRX[0] is already deployed:
                <codeblock># Copy the content of the restored directory (/tmp/hlm_builder_dir_restore/) to /etc/swiftlm/builder_dir/ on the SWF-PRX[0]

# Then from the deployer run
 
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock><p>If the SWF-PRX[0] is<b> not </b>deployed:
                <codeblock># From the deployer run
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-PRX[0]-hostname&gt;
 
# Copy the content of the restored directory (/tmp/hlm_builder_dir_restore/) to /etc/swiftlm/builder_dir/ on the SWF-PRX[0]
# You will have to create the directories : /etc/swiftlm/builder_dir/
 
# From the deployer run
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml</codeblock></p></li>
          </ol></li>
      </ul></section>

  </body>

</topic>
