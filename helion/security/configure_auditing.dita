<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_esg_pft_lv">
  <title>Configuring Auditing</title>
  <body>

    <section> Audit logging is disabled by default. Before enabling it, please ensure that all
      management controller nodes have disks setup for audit logging. Please use one of the
      following tables as a guidance to arrive at the target disk size for the /var/audit partition
      for the desired retention period. 
      <table frame="all" rowsep="1" colsep="1" id="table1">
        <tgroup cols="4">
          <colspec colnum="1" colname="col1"/>
          <colspec colnum="2" colname="col2"/>
          <colspec colnum="3" colname="col3"/>
          <colspec colnum="4" colname="col4"/>



          <thead>
            <row>
              <entry>Service Name</entry>
              <entry> Peak size of audit logs per day in MB (for 100 nodes) </entry>
              <entry>Estimated size for on-disk retention (in GB) for 1 week, 2 weeks, 1 month, 45
                days, 6 months, 1 year</entry>


              <entry>Comments</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Keystone</entry>
              <entry>95</entry>
              <entry> </entry>
              <entry morerows="9">
                <p><b>Assumptions:</b></p>
                <ul id="ul_iy2_2kt_lv">
                  <li>Compression ratio: 80%</li>
                  <li>Rotated once daily</li>
                  <li>Estimated size calculation: Peak_Size +
                    (Peak_Size*Compression_Percent*Num_Of_Days)</li>
                </ul>
                <p><b>Note:</b> It is recommended to have an <b>additional</b>
                  <b>25%</b> of the Total to allow some room for growth</p>
              </entry>
            </row>

            <row>
              <entry>Barbican</entry>
              <entry>2.6</entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Nova</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Cinder</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Neutron</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Glance</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Heat</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>Ironic</entry>
              <entry> </entry>
              <entry> </entry>

            </row>
            <row>
              <entry>USAN</entry>
              <entry> </entry>
              <entry> </entry>
            </row>
            <row>
              <entry><b>Total</b></entry>
              <entry> </entry>
              <entry> </entry>

            </row>
          </tbody>
        </tgroup>
      </table> 
      
      <table frame="all" rowsep="1" colsep="1" id="table2">
        <tgroup cols="4">
          <colspec colnum="1" colname="col1"/>
          <colspec colnum="2" colname="col2"/>
          <colspec colnum="3" colname="col3"/>
          <colspec colnum="4" colname="col4"/>
          
          
          
          <thead>
            <row>
              <entry>Service Name</entry>
              <entry> Peak size of audit logs per day in MB (for 300 nodes) </entry>
              <entry>Estimated size for on-disk retention (in GB) for 1 week, 2 weeks, 1 month, 45
                days, 6 months, 1 year</entry>
              
              
              <entry>Comments</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Keystone</entry>
              <entry>285</entry>
              <entry> </entry>
              <entry morerows="9">
                <p><b>Assumptions:</b></p>
                <ul id="ul_iy2_2">
                  <li>Compression ratio: 80%</li>
                  <li>Rotated once daily</li>
                  <li>Estimated size calculation: Peak_Size +
                    (Peak_Size*Compression_Percent*Num_Of_Days)</li>
                </ul>
                <p><b>Note:</b> It is recommended to have an <b>additional</b>
                  <b>25%</b> of the Total to allow some room for growth</p>
              </entry>
            </row>
            
            <row>
              <entry>Barbican</entry>
              <entry>4.2</entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Nova</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Cinder</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Neutron</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Glance</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Heat</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Ironic</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>USAN</entry>
              <entry> </entry>
              <entry> </entry>
            </row>
            <row>
              <entry><b>Total</b></entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
          </tbody>
        </tgroup>
      </table>
      
      <table frame="all" rowsep="1" colsep="1" id="table3">
        <tgroup cols="4">
          <colspec colnum="1" colname="col1"/>
          <colspec colnum="2" colname="col2"/>
          <colspec colnum="3" colname="col3"/>
          <colspec colnum="4" colname="col4"/>
          
          
          
          <thead>
            <row>
              <entry>Service Name</entry>
              <entry> Peak size of audit logs per day in MB (for 500 nodes) </entry>
              <entry>Estimated size for on-disk retention (in GB) for 1 week, 2 weeks, 1 month, 45
                days, 6 months, 1 year</entry>
              
              
              <entry>Comments</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Keystone</entry>
              <entry>475</entry>
              <entry> </entry>
              <entry morerows="9">
                <p><b>Assumptions:</b></p>
                <ul id="ul_2kt_lv">
                  <li>Compression ratio: 80%</li>
                  <li>Rotated once daily</li>
                  <li>Estimated size calculation: Peak_Size +
                    (Peak_Size*Compression_Percent*Num_Of_Days)</li>
                </ul>
                <p><b>Note:</b> It is recommended to have an <b>additional</b>
                  <b>25%</b> of the Total to allow some room for growth</p>
              </entry>
            </row>
            
            <row>
              <entry>Barbican</entry>
              <entry>5.6</entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Nova</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Cinder</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Neutron</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Glance</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Heat</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>Ironic</entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
            <row>
              <entry>USAN</entry>
              <entry> </entry>
              <entry> </entry>
            </row>
            <row>
              <entry><b>Total</b></entry>
              <entry> </entry>
              <entry> </entry>
              
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>


  <section><title>HLM Input Model Changes</title>
    
    <p>The value of the "Total" field from one of the tables above should provide a good idea on what size disk should be added. The following sections details the steps required to setup the disk for the audit partition. This could be a standard 
      Add disk(s) to the management controller nodes</p>
    
    <p>Please refer to the manufacturer's instructions appropriate to the type of server node used by the management controller 
      cluster to add disks for the audit log partition. For example, if your server supports adding disk to a free slot without having to shutdown
      the node, the following are the general steps (for each controller node):</p>
    <ul><li>
      Insert disk into the desired free slot
    </li><li>Configure RAID (if possible without taking the system down)
    </li><li>Run lsblk and note down the new disk partition name (example: /dev/sdd)
    </li></ul>
    
    
    <p>If you are running in a padawan, it already creates 5 extra disks in each of the controller nodes by default (/dev/sdc..sdg). Please identify an unused one and use it for the steps below.
      Update the disk template for controller nodes</p>
      
      <ol><li>ssh to your lifecycle manager node </li><li>Copy the appropriate cloud templates for your cloud from the ~/helion/examples/&lt;cloud type> folder to ~/helion/my_cloud/definition/ folder. Note: If you already have the required templates under the ~/helion/my_cloud/definition/data folder, you may skip this step.
        Example:
        <codeblock>$ cp -r ~/helion/examples/entry-scale-esx/* ~/helion/my_cloud/definition/
          
          cd ~/helion/my_cloud/definition/data</codeblock>
        
        
        
      </li><li>Edit the file disks_controller.yml (if you copied the files from mid-scale template, please edit disks_control_common.yml instead):
        <ol><li>Uncomment the "- name: audit-vg" section
        </li><li>The physical volume setting defaults to "/dev/sdz". Change it to match the disk you added, if required
        </li><li>Review the settings under logical-volumes. These are preset to optimal defaults
          
        </li></ol></li>
        <li>
          QA note: For the step 3, once the patch https://review.hpcloud.net/#/c/99320 merges, details about adding a audit-vg will be added (commented) to all input model disk_controller.yml templates. If you want to test before that merges, please refer to the files
          2.0/examples/entry-scale-esx/data/disks_controller_1TB.yml and 2.0/examples/entry-scale-esx/data/server_roles.yml as an example.
        </li><li> Save your changes
          <codeblock>$ cd ~/helion
$ git checkout site
$ git add -A
# Verify the added files
$ git status
$ git commit -m "Setup disks for audit logging"</codeblock>
        </li><li> Enable Audit Logging ssh to your lifecycle manager node
          <codeblock>cd ~/helion/my_cloud/definition</codeblock>
        </li><li>Edit the file cloudConfig.yml:
              audit-settings:
              <codeblock>audit-dir: /var/audit
                default: disabled
                #enabled-services:
                #  - keystone
                #  - barbican
                disabled-services:
                - nova
                - barbican
                - keystone
                - cinder
                - ceilometer
                - neutron</codeblock>    </li><li>
                  
                  Uncomment the "enabled-services" list
                  
                </li><li>Move the services that you want to have enabled auditing from the "disabled-services" list to the "enabled-services" list
                  
                  
                  Audit logging support is available only for the services listed under "disabled-services".
                  
                </li><li>Save your changes and re-run the configuration processor
          <codeblock>$ cd ~/helion
$ git add -A
# Verify the added files
$ git status
$ git commit -m "Enable Audit Logging"
         
$ cd ~/helion/hos/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li><li>Run osconfig playbook to create the /var/audit volume group
          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
# Activate the ansible venv to run the ad-hoc ansible command
$ source /opt/stack/venv/ansible-hos-3.0.0/bin/activate</codeblock>
          # The osconfig playbook uses the stub file /etc/hos/osconfig-ran to decide if disks were
          already configured so that # it could be idempotent. Remove this stub file on the
          controller nodes before re-running osconfing-run playbook
          <codeblock>$ ansible -i hosts/verb_hosts KEY-API -a 'sudo rm -f /etc/hos/osconfig-ran'
$ ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API</codeblock>
          # Note: I've used KEY-API here as an example to cover the management controller cluster.
          If you need to enable auditing for # a service that is not run in the same cluster, please
          suitably add it to the --limit flag above # For example: --limit KEY-API:MON-API </li><li> Run service playbooks to reconfigure them for audit logging
          <codeblock>$ cd ~/scratch/ansible/next/hos/ansible
$ ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml
$ ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</codeblock>
          # (More &lt;-reconfigure.yml to run if more are enabled or disabled.) </li><li>
                  Alternatively, <!-- once the patch https://review.hpcloud.net/#/c/99998/ merges, -->the following top level playbook could be used:
                  <codeblock>$ ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
                </li></ol>
      <!--The above steps are based on the wiki: Enable or Disable HOS Audit-->
</section>

<section><title>FAQs</title>





  <p><b>Where are the audit logs generated?</b></p>
  
  <p>The audit logs are generated by services running in the cloud management controller nodes. The events that signal auditing are formatted to be in a CADF compliant structure and then persisted to disk files.</p>
  
  <p><b>How and where are the audit logs stored?</b></p>
  
  <p>The audit logs are stored as disk files in the /var/audit partition in the management controller nodes. The audit logs for each service is stored in a file under its respective directory. For example, all nova audit logs are stored in the file /var/audit/nova/nova-audit.log
    Note: The default name of the audit partition is /var/audit. </p>
  
  
  <p><b>Are the audit logs centrally logged?</b></p>
  
  <p>No. Centralized Logging is not enabled for audit logs in HOS 3.0. Audit logs are more sensitive than regular logs and they need to be stored in separate indices with access controls enabled.</p>
  
  <p><b>How long are the audit logs retained?</b></p>
  
  <p>This is determined by the available space in the /var/audit partition.
    Audit logs typically have larger retention periods due to security compliance regulations. Hence, they are not included in the standard logrotate configuration. Customers are expected to configure rotation *after* ensuring sufficient retention or offline backup of these files.</p>
  
  
  <p><b>If a management controller node goes down, do I lose all the audit logs in that node?</b></p>
  
  <p>We strongly advise backing up the /var/audit partition in each of the management controller nodes for protection against any data loss, should a management controller node go down.</p>


</section>







  </body>
</topic>
