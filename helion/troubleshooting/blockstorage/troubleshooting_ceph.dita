<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_mnk_sq1_5t">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Service: Troubleshooting</title>
  <abstract><shortdesc outputclass="hdphidden">Troubleshooting scenarios with resolutions for the
    Ceph service.</shortdesc>This page describes troubleshooting scenarios for Ceph.</abstract>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
    </section>
    
    <section id="issue1"><title outputclass="headerH">Issue 1: If no Ceph monitor nodes are defined
      for the cloud, then Ceph cloud deployment fails</title>
      <sectiondiv outputclass="insideSection">
        <p>If no Ceph monitor nodes are defined for the cloud, then Ceph cloud deployment fails with
          the following error while executing the <codeph>site.yml</codeph> playbook:</p>
        <codeblock>TASK: [_CEP-CMN | install | Install ceph] ************************************* 
          changed: [stratushelion-cp1-ceph0001-mgmt]
          changed: [stratushelion-cp1-ceph0002-mgmt]
          changed: [stratushelion-cp1-ceph0005-mgmt]
          changed: [stratushelion-cp1-ceph0003-mgmt]
          changed: [stratushelion-cp1-ceph0004-mgmt]
          
          TASK: [_CEP-CMN | configure | Copy "{{ deployer_ceph_dir }}/ceph.client.admin.keyring" to /etc/ceph directory] *** 
          fatal: [stratushelion-cp1-ceph0001-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
          fatal: [stratushelion-cp1-ceph0002-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
          fatal: [stratushelion-cp1-ceph0003-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
          fatal: [stratushelion-cp1-ceph0004-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
          fatal: [stratushelion-cp1-ceph0005-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
          
          FATAL: all hosts have already failed -- aborting</codeblock>
        <p><b>Resolution:</b></p>
        <p>Compare the cloud control plane defined in the
          <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file with the
          example one. Ensure that either: <ol id="ol_ir1_2s4_5t">
            <li>The service component includes <codeph>- ceph-monitor</codeph> for
              <codeph>server-role: CONTROLLER-ROLE</codeph>
              <p>OR</p></li>
            <li>The resource nodes for Ceph Monitor are added to the
              <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, which
              have the service-components (<codeph>- ntp-client</codeph> and <codeph>-
                ceph-monitor</codeph>) defined.</li>
          </ol></p>
      </sectiondiv>
    </section>
    
    <section id="issue2"><title outputclass="headerH">Issue 2: If the disk presented as OSD data
      and/or a journal disk has some pre-existing partitions, then the Ceph cloud deployment
      fails</title>
      <sectiondiv outputclass="insideSection">
        <p>If the disk presented as OSD data and/or a journal disk has some pre-existing partitions,
          then the Ceph cloud deployment fails with the following error while executing
          <codeph>site.yml</codeph> playbook:</p>
        <p>
          <codeblock>TASK: [CEP-OSD | configure | Configure the osds of {{ inventory_hostname }}] *** 
            failed: [stratushelion-cp1-ceph0001-mgmt] => (item={'key': '/dev/sdd', 'value': '/dev/sdc'}) => {"failed": true, "item": {"key": "/dev/sdd", "value": "/dev/sdc"}}
            msg: Exception: 'ceph-disk -v prepare --cluster-uuid 2645bbf6-16d0-4c42-8835-8ba9f5c95a1d --cluster ceph --osd-uuid 41cfdfe8-1c97-4c8c-9561-5ced0e88ebfd --fs-type xfs --zap-disk /dev/sdd /dev/sdc' failed with error DEBUG:ceph-disk:Zapping partition table on /dev/sdd
            INFO:ceph-disk:Running command: /sbin/sgdisk --zap-all -- /dev/sdd
            Caution: invalid backup GPT header, but valid main header; regenerating
            backup header from main header.
            
            INFO:ceph-disk:Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/sdd
            DEBUG:ceph-disk:Calling partprobe on zapped device /dev/sdd
            INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
            INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
            INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
            INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
            INFO:ceph-disk:Running command: /sbin/parted --machine -- /dev/sdc print
            WARNING:ceph-disk:OSD will not be hot-swappable if journal is not the same device as the osd data
            DEBUG:ceph-disk:Creating journal partition num 4 size 5120 on /dev/sdc
            INFO:ceph-disk:Running command: /sbin/sgdisk --new=4:0:+5120M --change-name=4:ceph journal --partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0 --typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
            Could not create partition 4 from 0 to 10485759
            Unable to set partition 4's name to 'ceph journal'!
            Could not change partition 4's type code to 45b0969e-9b03-4f30-b4c6-b4b80ceff106!
            Error encountered; not saving changes.
            ceph-disk: Error: Command '['/sbin/sgdisk', '--new=4:0:+5120M', '--change-name=4:ceph journal', '--partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0', '--typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdc']' returned non-zero exit status 4</codeblock>
        </p>
        <p><b>Resolution:</b></p>
        <p>You must manually delete any partitions on the (failed) disk presented as OSD data and/or
          journal disk and re-execute the <codeph>site.yml</codeph> playbook to resume the cloud
          deployment.<codeblock>cd ~/scratch/ansible/next/hos/ansible
            ansible-playbook -i hosts/verb_hosts site.yml </codeblock></p>
      </sectiondiv>
    </section>
    
    <section id="issue3"><title outputclass="headerH">Issue 3: If on a entry-scale-kvm-ceph cloud,
      the controller nodes are not always a part of the actionable nodes then executing the
      site.yml playbook for a particular node (using --limit &lt;node>) fails</title>
      <sectiondiv outputclass="insideSection">
        <p>If on a entry-scale-kvm-ceph cloud, the controller nodes are not always a part of the
          actionable nodes then executing the <codeph>site.yml</codeph> playbook for a particular
          node (using --limit &lt;node>) fails with the following error:</p>
        <codeblock>TASK: [_CEP-CMN | check_network | Set cluster_network_enabled to True if available] *** 
          skipping: [localhost]
          
          TASK: [ceph-deployer | configure | Generate "/etc/ceph/{{ ceph_cluster }}.conf" file] *** 
          fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
          fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
          
          FATAL: all hosts have already failed -- aborting  
        </codeblock>
        <p><b>Resolution</b>:</p>
        <p>Edit the limit file or limit string specified while executing the
          <codeph>site.yml</codeph> playbook and include the hostnames of all the <b>Ceph monitor
            nodes</b> in the cloud. You can execute the <codeph>site.yml</codeph> option in either
          of the following ways:<ol id="ol_jxh_bfq_5t">
            <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml --limit
              new-compute</codeph><p>You must add monitor nodes in the above
                command:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit new-compute,monitor-1,monitor-2,monitor-3</codeblock></p></li>
            <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml -limit
              @new_nodes.txt</codeph><p>You can add the monitor hosts to the text file (named
                <codeph>new_nodes.txt</codeph>), which specifies the new compute host (along with
                monitor hosts) as follows:<ul id="ul_ak5_fgq_5t">
                  <li>new-compute</li>
                  <li>monitor-1</li>
                  <li>monitor-2</li>
                  <li>monitor-3</li>
                </ul></p></li>
          </ol></p>
      </sectiondiv>
    </section>
    
    <section id="issue4"><title outputclass="headerH">Issue 4: If the time required for placement
      group creation is slower than usual (>50 secs for PGs to get created), the
      ceph-client-prepare.yml playbook fails with an error while creating placement groups</title>
      <sectiondiv outputclass="insideSection">
        <p>If the time required for placement group creation is slower than usual (>50 secs for PGs
          to get created), the <codeph>ceph-client-prepare.yml</codeph> playbook fails with the
          following error while creating placement groups:</p>
        <codeblock>Symptom:
          Following error traces seen during site.yml playbook execution:
          TASK: [ceph-client-prepare | prepare-cluster-user | Set pool {{ item.1.name }} pgp_num to {{ item.1.attrs.pg }}] ***
          changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'cinder-volume'}, 'name': 'volumes', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 100, 'permission': 'rwx'}}))
          changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'nova'}, 'name': 'vms', 'attrs': {'creation_policy': 'eager', 'type': 'replicated', 'pg': 100, 'permission': 'rwx'}}))
          skipping: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'type': 'erasure', 'creation_policy': 'lazy', 'replica_size': 2, 'pg': 128, 'permission': 'rwx'}}))
          failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'glance'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'creation_policy': 'eager', 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set images pgp_num 128", "delta": "0:00:00.456878", "end": "2015-10-19 12:08:37.379630", "item": [{"user": {"name": "glance", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128}, "name": "images", "usage": {"purpose": "glance-datastore"}}], "rc": 16, "start": "2015-10-19 12:08:36.922752", "warnings": []}
          stderr: Error EBUSY: currently creating pgs, wait
          failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder-backup'}}, {'usage': {'purpose': 'cinder-backup'}, 'name': 'backups', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set backups pgp_num 128", "delta": "0:00:00.242184", "end": "2015-10-19 12:08:37.824764", "item": [{"user": {"name": "cinder-backup", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128, "replica_size": 3, "type": "replicated"}, "name": "backups", "usage": {"purpose": "cinder-backup"}}], "rc": 16, "start": "2015-10-19 12:08:37.582580", "warnings": []}
          stderr: Error EBUSY: currently creating pgs, wait
          
          FATAL: all hosts have already failed -- aborting</codeblock>
        <p><b>Resolution</b>:</p>
        <p>Pause for a minute and check the status of <codeph>ceph --cluster &lt;ceph-cluster> -s
          |grep creating</codeph>. If no results are returned by this command then re-execute the
          <codeph>ceph-client-prepare.yml</codeph> playbook.</p>
      </sectiondiv>
    </section>
    
    <section id="issue5"><title outputclass="headerH">Issue 5: During configuration of a new OSD
      disk, if there is an error similar to "disk does not exist" during the "ceph-disk activate"
      phase</title>
      <sectiondiv outputclass="insideSection">
        <p>During configuration of a new OSD disk, if there is an error similar to <codeph>disk does
          not exist</codeph> during the <codeph>ceph-disk activate</codeph> phase it means that
          the partition created is taking more time than usual to appear. You can edit the
          <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph> file and change the values
          for the parameters below to assist:</p>
        <codeblock>data_disk_poll_attempts
          data_disk_poll_interval</codeblock>
        <p>The default values of the parameters above (5 and 12 respectively) result in a timeout
          for polling data disk availability of one minute. Ideally, increasing the
          <codeph>data_disk_poll_attempts</codeph> value for 50 would set the timeout value to 10
          minutes.</p>
      </sectiondiv>
    </section>
    
    <section id="issue6"><title outputclass="headerH">Issue 6: If the volume of requests to Ceph is
      very high, the logs generated by Ceph will fill up the disk quickly.</title>
      <sectiondiv outputclass="insideSection">
        <p>If the volume of requests to Ceph is very high, the logs generated by Ceph will fill up
          the disk quickly. The default logrotate configuration for Ceph rotates at a daily
          frequency, which will not suffice for heavy workloads. In such a scenario, the
          administrator will need to perform the following changes so that the Ceph logs get rotated
          more frequently (every 30 minutes in the example below) if the log file reaches the size
          of <codeph>50M</codeph>.</p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Edit the file below:
            <codeblock>~/helion/hos/ansible/roles/_CEP-CMN/files/logrotate.conf</codeblock></li>
          <li>Add the <codeph>size 50M</codeph> line after the rotation frequency line.
            <p>Example:</p>
            <codeblock>/var/log/ceph/*.log {
              rotate 7
              daily
              size 50M
              compress</codeblock></li>
          <li>Commit the changes to git:
            <codeblock>git add -A
              git commit -m "Added size 50M restriction to Ceph logrotate conf"</codeblock></li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible/
              ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Create the deployment directory:
            <codeblock>cd ~/helion/hos/ansible/
              ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Reconfigure the Ceph service to complete the changes:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
              ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</codeblock></li>
          <li>Once that playbook is complete, log in to each Ceph node and add a cron job to trigger
            the Ceph log rotation every 30 minutes by using these steps: <ol>
              <li>Execute this command: <codeblock>crontab -e</codeblock></li>
              <li>Choose an editor.</li>
              <li>Add this line to the bottom of the file:
                <codeblock>*/30 * * * * /usr/sbin/logrotate /etc/logrotate.d/ceph &gt;/dev/null 2&gt;&amp;1</codeblock></li>
              <li>Save the file.</li>
            </ol>
            <note>If the log disk utilization is very high, it is advisable to manually free up some
              space. In case the in-use log file has grown too large and has been deleted, Ceph
              processes need to be informed of the same, by executing the <codeph>systemctl reload
                'ceph-*'</codeph> command on the corresponding Ceph node.</note></li>
        </ol>
      </sectiondiv>
    </section>
    
    <section id="issue7"><title outputclass="headerH">Issue 7: Cinder volume create fails
      consistently as insufficient disk space gets reported when an OSD is down.</title>
      <sectiondiv outputclass="insideSection">
        <p><b>Resolution:</b></p>
        <p>Scenario 1: An OSD node is down</p>
        <ol>
          <li>If an OSD node has failed, note the OSD numbers on that node, by executing the 'ceph
            --cluster &gt;cluster-name> osd tree' command on a controller node. A sample of OSD tree
            output is as follows: <codeblock>$ ceph osd tree
              # id    weight  type name       up/down reweight
              -1      6       root default
              -2      3               host padawan-ceph-ccp-ceph0003-mgmt
              0       1                       osd.0   up      1
              3       1                       osd.3   up      1
              6       1                       osd.6   up      1
              -4      3               host padawan-ceph-ccp-ceph0001-mgmt
              1       1                       osd.1   down    0
              4       1                       osd.4   down    0
              7       1                       osd.7   down    0
              -3      3               host padawan-ceph-ccp-ceph0002-mgmt
              2       1                       osd.2   up      1
              5       1                       osd.5   up      1
              8       1                       osd.8   up      1</codeblock>
            <p>For example, if <codeph>padawan-ceph-ccp-ceph0001-mgmt</codeph> is down, the OSD
              numbers 1, 4, and 7 are relevant.</p></li>
          <li>For each OSD number identified above perform below steps:
            <codeblock>ceph --cluster &gt;cluster-name> osd out &lt;osd-num>
              ceph --cluster &lt;cluster-name> osd crush remove osd.&lt;osd-num>
              ceph --cluster &lt;cluster-name> auth del osd.&lt;osd-num>
              ceph --cluster &lt;cluster-name> osd rm &lt;osd-num></codeblock></li>
          <li>Remove the OSD host from crush map if all osd's on the particular host is down:
            <codeblock>ceph --cluster &lt;cluster-name> osd crush remove &lt;osd-hostname></codeblock></li>
        </ol>
        <p>Scenario 2: One or more OSDs on an OSD node is/are down</p>
        <ol>
          <li>If the OSD node is up, but one/more OSDs on that node are down, remove those OSD's
            from the crush map, by repeating the step below for each OSD number (identified using
            method mentioned in step #1 above): <codeblock>ceph --cluster &lt;cluster-name> osd crush remove osd.&lt;osd-num></codeblock>
            <p>Once the maintenance is performed, the OSD can be added back once it is up:</p>
            <codeblock>ceph --cluster &lt;cluster-name> osd crush add osd.&lt;osd-num> 1.0 host=&lt;osd-hostname></codeblock></li>
        </ol>
      </sectiondiv>
    </section>
  </body>
</topic>
