<?xml version="1.0" encoding="UTF-8"?>
  <!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="troubleshootingNova">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting Nova Service</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="about">
      <p>Nova offers scalable, on-demand, self-service access to compute resources. You can use this
        guide to help with known issues and troubleshooting of Nova services.</p>
    </section>
    <section id="troubleshooting">
      <title>Troubleshooting</title>
      <!-- nova-consoleauth NEED JIRA -->
      <p><b>Nova-consoleauth</b></p>
      <p>The <codeph>nova-consoleauth</codeph> service runs by default on the first controller node,
        that is, the host with <codeph>consoleauth_host_index=0</codeph>. If
          <codeph>nova-consoleauth</codeph> fails on the first controller node, you can switch it to
        another controller node by running the ansible playbook <codeph>nova-start.yml</codeph> and
        passing it the index of the next controller node.</p>
      <p>The command to switch nova-consoleauth to another controller node (controller 2 for
        instance) is:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</codeblock>
      <!-- DOCS-2563 -->
      <p><b>Issue with Swap File Underlays being Deleted when Rebooting Compute Nodes</b></p>
      <p>There is an issue in upstream Nova (outlined in <xref
          href="https://bugs.launchpad.net/nova/+bug/1457527" scope="external" format="html"
          >https://bugs.launchpad.net/nova/+bug/1457527</xref> where if you have Nova instances that
        are booted from a flavor with a swap volume configured you will experience an issue where
        the swap file underlays get deleted when Nova compute nodes are rebooted. The swap disk is
        not being registered in the block device mappings table so the Nova compute manager cleans
        up the qcow2 underlay file prematurely. This does not have any impact on a running instance,
        however, if the Nova node it resides on is rebooted, when the compute manager restarts it
        fails to restart the instances using the swap files for which the underlay file is missing.
        The instance will not start and the status will be set to ERROR.</p>
      <p>The issue can be resolved by resetting the instance status to ACTIVE and hard rebooting the
        instance. An admin user can do this by using these steps below.</p>
      <p>If the admin has access, you can do this from the lifecycle manager by sourcing the admin
        creds with <codeph>source ~/service.osrc</codeph>.</p>
      <ol>
        <li>List the impacted instances:
          <codeblock>nova list --all-tenants --field=host,status</codeblock></li>
        <li>Reset the instance state:
          <codeblock>nova reset-state --active &lt;instance ID></codeblock></li>
        <li>Hard reboot the instance(s):
          <codeblock>nova reboot --hard &lt;instance ID></codeblock></li>
      </ol>
      <note type="important">This is a manual procedure that needs to be performed each time a
        Compute node is rebooted.</note>
      <!-- DOCS-2698 jb-->
      <p><b>Port Limitation Error Seen When Live Migrating Over 64 Instances</b></p>
      <p>This is a bug that exists in the version of libvirt used in HPE Helion OpenStack 2.x. It is
        outlined <xref href="https://bugzilla.redhat.com/show_bug.cgi?id=1159245" format="html"
          scope="external">here</xref>.</p>
      <p>If you will be migrating over 64 instances, each time you reached the 64th live migration,
        you will need to perform this workaround:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Disable the <codeph>nova-compute</codeph> service to prevent new instances from
          provisioning on the Compute host while you perform the rest of these steps:
          <codeblock>nova service-disable &lt;hostname> nova-compute --reason "restarting libvirt"</codeblock></li>
        <li>SSH in to the Compute host.</li>
        <li>Restart libvirt with this series of commands:
          <codeblock>systemctl stop nova-compute
systemctl restart libvirt
system start nova-compute</codeblock></li>
        <li>Exit the Compute host so you are back at your lifecycle manager.</li>
        <li>Re-enable provisioning on the Compute host:
          <codeblock>nova service-enable &lt;hostname> nova-compute</codeblock></li>
        <li>Repeat these steps on each of the nodes you are performing live migrations on.</li>
      </ol>
    </section>
  </body>
</topic>
