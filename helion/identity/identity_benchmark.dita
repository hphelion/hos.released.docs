<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="keystone_benchmarking">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Keystone Identity Performance Benchmark</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>


    <section id="overview"><title>Overview</title> The purpose of this performance benchmark is to
      provide a reference for HPE Helion OpenStack Keystone users to know how the system performs
      under a particular environment and workload. There are several major operations where
      performance is critical: token creation, token validation, and user lookup when an LDAP back
      end is applied. <p>Other operations are available but performed less frequently. Thus we focus
        here on the benchmark tests and documentation for only these three major operations.</p>. </section>
    <section id="testEnv"><title>Performance Test Environment</title>
      <b>Server Models and Specification</b></section>
    <section> Ideally, a performance test environment would be an exact replica of the production
      system but it’s not always possible. For this performance test, HP KS-PERF, as described in
      Table 1, was performed against a 7-node, HP DL360 Gen9-based servers in 'baremetal' mode. </section>
    <simpletable frame="all" relcolwidth="1.0* 1.0* 1.0* 1.0* 1.0* 1.0*" id="simpletable_cts_tcm_25">
      <sthead>
        <stentry>Server</stentry>
        <stentry>Nodes</stentry>
        <stentry>Model</stentry>
        <stentry>Specification</stentry>
        <stentry>Operating System</stentry>
        <stentry>Software Versions</stentry>
      </sthead>
      <strow>
        <stentry>
          <p>KS-Perf Servers</p>
        </stentry>
        <stentry>7</stentry>
        <stentry>
          <p>HP ProLiant DL360 Gen9)</p>
        </stentry>
        <stentry>
          <ul id="ol_mfn_xcm_25">
            <li>2 x Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz</li>
            <li>132Gb RAM, dual 10Gb NIC in bonding mode</li>
          </ul>
        </stentry>
        <stentry>
          <p>hLinux cattleprod (3.14.44-1 kernel)</p>
        </stentry>
        <stentry><ul>
            <li>HPE Helion OpenStack2.0</li>
            <li>HPE Helion OpenStack 1.1.1</li>
            <li>MySQL v5.5.41</li>
          </ul></stentry>
      </strow>
      <strow>
        <stentry>
          <p>OpenLDAP Server</p>
        </stentry>
        <stentry>1</stentry>
        <stentry>
          <p>HP SE2170s Gen7</p>
        </stentry>
        <stentry>
          <ul id="ol_scv_2dm_25">
            <li>2 x Intel(R) Xeon(R) CPU X5650 @ 2.67GHz</li>
            <li>192Gb RAM, single 1Gb NIC</li>
          </ul>
        </stentry>
        <stentry>
          <p>Ubuntu 14.04.02 LTS (Trusty)</p>
        </stentry>
        <stentry>
          <p>OpenLDAP v2.4.31</p>
        </stentry>
      </strow>
    </simpletable>
    <section>Table 1 Server specifications for performance test </section>
    <section id="Keystone-LDAP"><title>Keystone-LDAP</title>
      <p>For the test cases with a domain-specific LDAP back end, and an OpenLDAP server employed,
        as described in Table 1, there are four LDAP domains (i.e. openldap50, openldap5k,
        openldap50k and openldap500k) with different numbers of users (i.e. 50, 5,000, 50,000 and
        500,000) respectively, as depicted in Figure 1 and Table 2. </p>
      <image href="../../media/keystone/HOSKeystoneDomain-SpecificBackend.png" id="image_eht_kcm_25"/>
      <p>Figure 1 LDAP users and groups for Keystone domains</p>
      <simpletable frame="all" relcolwidth="1.0* 1.0* 1.0* 1.0* 1.0*" id="simpletable_ths_xdm_25">
        <sthead>
          <stentry>LDAP</stentry>
          <stentry>openldap50</stentry>
          <stentry>openldap5k</stentry>
          <stentry>openldap50k</stentry>
          <stentry>openldap500k</stentry>
        </sthead>
        <strow>
          <stentry>
            <p>User Container</p>
          </stentry>
          <stentry>
            <p>ou=Users50,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_orh_b2m_25">
              <li>user50_000001 to user50_000050</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users5k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_urv_b2m_25">
              <li>user5k_000001 to user50_005000</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users50k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_nj3_c2m_25">
              <li>user50k_000001 to user50k_050000</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users500k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_nr5_c2m_25">
              <li>user500k_000001 to user500k_500000</li>
            </ul>
          </stentry>
        </strow>
        <strow>
          <stentry>
            <p>Group Container</p>
          </stentry>
          <stentry>
            <p>ou=Groups50,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_kl5_d2m_25">
              <li>
                <p> members of group50_1: </p>
                <p>user50_000001 to user50_000050 under Users50</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups5k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_eff_22m_25">
              <li>
                <p> members of group5k_1: </p>
                <p>user5k_000001 to user5k_000050 under Users5k</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups50k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_efq_22m_25">
              <li>
                <p>members of group50k_1: </p>
                <p>user50k_000001 to user50k_000050 under Users50k</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups500k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_p52_f2m_25">
              <li>
                <p>members of group500k_1: </p>
                <p>user500k_000001 to user500k_000050 under Users500k</p>
              </li>
            </ul>
          </stentry>
        </strow>
      </simpletable><p>Table 2 LDAP user and group data structure for four Keystone domains (i.e.
        openldap50, openldap5k, openldap50k and openldap500k) </p></section>

    <section id="cache_mech"><title>Cache Mechanism</title> Keystone supports a caching layer that
      is above the configurable subsystems (e.g. token, identity, etc). HPE Helion OpenStack
      Keystone uses the dogpile.cache library, allowing for flexible cache back ends. The majority
      of the caching configuration options are set in the [cache] section. However, each section
      that has the ability to be cached usually has a caching boolean value that will toggle caching
      for that specific section. For the performance test cases with caching turned on, the
      subsystem and the global toggle setting are enabled and dogpile.cache.memory is employed for
      this study as configured below. Please be advised that dogpile.cache.memory is an experimental
      cache provider and is not ready for production-level applications. <ol>
        <li>To begin, edit <codeph>etc/keystone.conf</codeph> with the [cache] section shown below
          <codeblock>[cache]
# enable caching across all of keystone
enabled = true
  
# use in-memory cache
backend = dogpile.cache.memory</codeblock>
        </li>
        <li>Verify that subsystem caching is enabled
          <codeblock>[catalog]
caching = true
  
[domain_config]
caching = true
  
[identity]
caching = true
  
[resource]
caching = true
  
[revoke]
caching = true
  
[role]
caching = true
  
[token]
caching = true</codeblock>
        </li>
        <li>Restart Apache2 for the update to take effect.</li>
      </ol> Additional Note: Concurrent test cases with the label 1K2P17U represent test conditions
      with one Keystone node, two processes configured in Apache2 and 17 concurrent users.
      Similarly, 3K2P50U has three Keystone nodes, two processes configured in Apache2 and 50
      concurrent users. <ul>
        <li>K represents number of Keystone nodes</li>
        <li>P represents number of processes configured in Apache2 sever</li>
        <li>U represents number of concurrent users</li>
      </ul>
    </section>
    <section id="domain_spec_back_end"><title>HPE Helion OpenStack 2.0 with domain-specific LDAP
        back end</title>
      <p><b>LDAP user lookup performance - HPE Helion OpenStack 2.0 Keystone user lookup performance
        </b></p> Figures 2.a through 2.c represent the performance on non-concurrent LDAP user
      lookup by name for HPE Helion OpenStack. The study for the Keystone-LDAP query efficiency
      skips the authorization process by using an admin token. A baseline test using a direct LDAP
      client (i.e. ldapserach) is also performed for comparison. <ul>
        <li>Generally, having more LDAP users per domain results in higher query response time for a
          domain user lookup.</li>
        <li>The response times grow roughly linearly with the number of domain users in LDAP, as
          shown in Figures 2.b and Figure 2.c. The response time slope for HPE Helion OpenStack 2.0
          is much flatter than for HPE Helion OpenStack 1.1.1.</li>
        <li>HPE Helion OpenStack 2.0 behaves between 10 and several thousand times better than HPE
          Helion OpenStack 1.1.1 for LDAP user lookup operations, especially for large LDAP users
          (e.g. 500k users) cases.</li>
      </ul>
      <image placement="break" href="../../media/keystone/Non-ConcurrentLDAPUserLookup_bars.png"
        id="image_rs1_5fv_q5" align="" outputclass=""/>
      <p>Figure 2.a Non-Concurrent LDAP User lookup by name</p><image placement="break"
        href="../../media/keystone/Non-ConcurrentLdapUserLookup_lines.png" id="image_srj_zfv_q5"/>
      <p>Figure 2.b Non-Concurrent LDAP User lookup by name</p>
      <image placement="break"
        href="../../media/keystone/Non-ConcurrentLdapUserLookup_base_lines.png"
        id="image_v5f_kjh_r5"/>
      <p>Figure 2.c Non-concurrent LDAP user lookup by name </p></section>

<section>
      <p><b>LDAP user lookup performance - concurrency impact on HPE Helion OpenStack 2.0
          Keystone</b></p> Figures 3.a and 3.b represent HPE Helion OpenStack 2.0 LDAP user lookup
      under concurrent conditions. <ul>
        <li>The response times grow roughly linearly with the number of domain users in LDAP even
          under concurrent conditions, as shown in Figure 3.b, but response time  slopes vary for
          different race conditions. </li>
        <li>Compared with baseline non-concurrent LDAP user lookup for a specific number of LDAP
          users, the response time increases for cases with concurrent conditions by a factor of 5
          to 10. When increasing the number of concurrent users from 17 to 50 and Keystone nodes
          from 1 to 3 with the same average load per Keystone node (e.g. 50/3 ~ 17), the response
          time grows much higher by another factor of 2 to 3 for the case of 500,000 LDAP users.
          This shows that race conditions have caused quite a large impact on the efficiency of the
          servers (e.g. Keystone, Apache2, Keystone, and LDAP) leading to poorer responsiveness. </li>
      </ul>
      <image placement="break" href="../../media/keystone/HOS2_LdapUserLookup_bars.png"
        id="image_mrq_xtb_r5"/>
      <p>Figure 3.a LDAP User lookup by name for HPE Helion OpenStack 2.0 under concurrent
        conditions</p>
      <image href="../../media/keystone/HOS2_LdapUserLookup_lines.png" id="image_ggk_15b_r5"/>
      <p>Figure 3.b LDAP User lookup by name for HPE Helion OpenStack 2.0 with concurrent
        conditions</p>
    </section>
    <section><title>Create project-scoped token</title>
      <p><b>Keystone performance on project-scoped token creation</b></p> Figures 4.a and 4.b
      represent performance on project-scoped token creation for a single Keystone node without
      concurrent conditions. <ul>
        <li>The response times for project-scoped token creation grow roughly linearly with the
          number of domain users in LDAP, as shown in Figure 4.b, but response time slopes vary for
          HPE Helion OpenStack versions and cached conditions. </li>
        <li>For non-cached cases of project-scoped token creation , HPE Helion OpenStack 2.0
          performs slightly better than HPE Helion OpenStack1.1.1 within 5%. </li>
        <li>With the cache mechanism activated for the single Keystone node, the system shows
          obvious improvement in responsiveness for all cases of project-scoped token creation,
          especially the cases for HPE Helion OpenStack 2.0 (see further detail in Figure 6.b).
        </li>
      </ul>
      <image href="../../media/keystone/CPST_Nonconcurrent_barrs.png" id="image_bzm_c3v_q5"
        placement="break"/>
      <p>Figure 4.a Performance on project-scoped token creation for a single Keystone node without
        concurrency</p>
      <image href="../../media/keystone/CPST_Nonconcurrent_lines.png" id="image_irn_f3v_q5"
        placement="break"/>
      <p>Figure 4.b Performance on project-scoped token creation for a single Keystone node without
        concurrency</p>
      <p>Figures 5.a and 5.b below represent performance on project-scoped token creation under
        concurrent conditions.</p>
      <ul>
        <li>The response times for a single Keystone node with concurrency grow roughly linearly
          with the number of domain users in LDAP, as shown in Figure 5.b, but response time slopes
          vary for HPE Helion OpenStack versions and and cached conditions. </li>
        <li>For non-cached cases of project-scoped token creation, HPE Helion OpenStack 2.0 performs
          slightly better than HPE Helion OpenStack1.1.1&#151;within 5%. </li>
        <li>With the cache mechanism activated for the single Keystone node, the system observes
          obvious improvement on responsiveness for all cases of project-scoped token creation. The
          improvement by cache for HPE Helion OpenStack 2.0 is even bigger than HPE Helion OpenStack
          1.1.1 (see further detail in Figure 6.b and percentage Figure 6.a and 6.b). </li>
      </ul>
      <image placement="break" href="../../media/keystone/CPST_Concurrent_bars.png"
        id="image_gtl_53v_q5"/>
      <p>Figure 5.a Performance on project-scoped token creation with concurrency (1K2P17U)
        </p><image placement="break" href="../../media/keystone/CPST_Concurrent_lines.png"
        id="image_kkc_z3v_q5"/>
      <p>Figure 5.b Performance on project-scoped token creation with concurrency (1K2P17U)
          </p><p><b>HPE Helion OpenStack Keystone Improvement Using Cache</b></p>
      <p>The following figures represent performance improvement percentage on project-scoped token
        creation for a single Keystone node with non-concurrent condition and 17 concurrent users
        condition.</p><ul>
        <li>With the cache mechanism activated for the single Keystone node, the system observes
          obvious improvement on responsiveness for all cases of project-scoped token creation. </li>
        <li>The improvement for non-concurrent HPE Helion OpenStack 2.0 project-scoped token
          creation for a single Keystone node is about 45% and concurrent HPE Helion OpenStack 2.0
          about 40%. The improvement percentages stay stable for cases of different numbers of LDAP
          users. </li>
        <li>The improvement for non-concurrent HPE Helion OpenStack 1.1.1 project-scoped token
          creation for a single Keystone node is about 15%- 30% and concurrent HPE Helion OpenStack
          1.1.1 about 20%-28%. The improvement decreases with larger numbers of LDAP users. </li>
      </ul>
      <image placement="break" href="../../media/keystone/CPST_ImprovementByCache_bars.png"
        id="image_mtp_ppb_r5"/>
      <p>Figure 6.a Create-project-scoped-token performance improvement percentage by cache.
        (Concurrency cases based on 1K2P17U)</p>
      <image placement="break" href="../../media/keystone/CPST_ImprovementByCache_lines.png"
        id="image_qf2_spb_r5"/>
      <p>Figure 6.b Create-project-scoped-token performance improvement percentage by cache.
        (Concurrency cases based on 1K2P17U)</p>
      <p><b>Concurrency Impact on HPE Helion OpenStack 2.0 Keystone</b></p>
      <p>The following figures represent concurrency impact on HPE Helion OpenStack 2.0 performance
        for project-scoped token creation.</p>
      <ul>
        <li>The response times grow roughly linearly with the number of domain users in LDAP for all
          test cases here, as shown in Figure 7.b, but response time slopes vary for different race
          and cached conditions. </li>
        <li>Compared with the baseline non-cached non-concurrent project-scoped token creation for a
          specific number of LDAP users, the response time is getting much higher for cases with
          concurrent conditions by factor of 10. While increase numbers of concurrent users from 17
          to 50 and Keystone nodes from 1 to 3 with about the same average load (i.e. 50/3 ~ 17) per
          Keystone node, the response times do not have obvious changes for a specific number of
          LDAP users under 50,000. However, the response times for the case of 500,000 LDAP users
          grows by a factor of 2 to 3. The race conditions have caused quite an impact on the
          efficiency of the servers (e.g. Apache2, Keystone, and LDAP) for the case of half-million
          LDAP users. </li>
        <li>Turning on the cache mechanism within Keystone nodes, the system observes obvious
          improvement on responsiveness for all cases of project-scoped token creation. </li>
        <li>The improvement by cache for HPE Helion OpenStack 2.0 project-scoped token creation with
          either 17 or 50 non-concurrent users is about 40%. The improvement percentages stay stable
          for cases of different numbers of LDAP users. </li>
      </ul>
      <image placement="break" href="../../media/keystone/HOS2_CPST_KPU_bars.png"
        id="image_hcd_qqb_r5"/> Figure 7.a Concurrency impact on Keystone under cached and
      non-cached conditions <image placement="break"
        href="../../media/keystone/HOS2_CPST_KPU_lines.png" id="image_wk4_wqb_r5"/>
      <p>Figure 7.b Concurrency impact on Keystone under cached and non-cached conditions</p>
    </section>
    <section><title>Summary</title> Below is a summary of the performance improvements for LDAP user
      lookups and creating project-scoped tokens. <p><b>LDAP User Lookup</b></p>
      <ul>
        <li>Generally, larger numbers of LDAP users for a domain results in higher query response
          times for a domain user lookup in a linear relationship with slopes varied, regardless of
          whether concurrency or non-concurrency is involved.  </li>
        <li>HPE Helion OpenStack 2.0 behaves between 10 and several thousand times better than HPE
          Helion OpenStack 1.1.1 for LDAP user lookup operations, The greatest improvements are most
          noticeable for large user deployments. For example, the 500,000-user tests yielded a
          performance increase of several thousand lookup operations over the same period.</li>
        <li>The response time grows by factor of 5-10 for cases with concurrent conditions when
          compared with baseline non-concurrent LDAP user lookup for a specific number of LDAP
          users. While increasing numbers of concurrent users from 17 to 50 and Keystone nodes from
          1 to 3 with the same average load per Keystone node (e.g. 50/3 ~ 17), the response time
          grows much higher by another factor of 2 to 3 for the case of half-million LDAP users.
          This shows the race conditions have caused quite a bit impact on the efficiency for the
          servers (e.g. Keystone, Apache2, Keystone, and LDAP) leading toward slow responsiveness.
        </li>
      </ul>
      <p><b>Create Project-scoped Token</b></p>
      <ul>
        <li>The response times for project-scoped token creation grow roughly linearly with the
          number of domain users in LDAP, but response time slopes vary for HPE Helion OpenStack
          versions and cached conditions, regardless of whether concurrency or non-concurrency is
          involved. </li>
        <li>HPE Helion OpenStack 2.0 performs slightly better than the HPE Helion OpenStack1.1.1
          within 5% for project-scoped token creation. </li>
        <li>With the cache mechanism activated for the single Keystone node without concurrency, the
          system observes obvious improvement on responsiveness for project-scoped token creation. <ul>
            <li>The improvement for non-concurrent HPE Helion OpenStack 2.0 project-scoped token
              creation for a single Keystone node is about 45% and concurrent HPE Helion OpenStack
              2.0 about 40%. The improvement percentages stay stable for cases of different numbers
              of LDAP users. </li>
            <li>The improvement for non-concurrent HPE Helion OpenStack 1.1.1 project-scoped token
              creation for a single Keystone node is about 15%- 30% and concurrent HPE Helion
              OpenStack 1.1.1 about 20%-28%. The improvement decreases for  larger numbers of LDAP
              users. </li>
          </ul></li>
        <li>With the cache mechanism activated for the single Keystone node with concurrency, the
          system observes obvious improvement on responsiveness for project-scoped token creation. <ul>
            <li>The improvement by cache for HPE Helion OpenStack 2.0 project-scoped token creation
              with either 17 or 50 concurrent users is about 40%. The improvement percentages stay
              stable for  different numbers of LDAP users. </li>
          </ul></li>
        <li>Compared with the baseline non-cached non-concurrent project-scoped token creation for a
          specific number of LDAP users, the response times get much higher for cases with
          concurrent conditions by factor of 10. When the number of concurrent users is increased
          from 17 to 50 and the number of Keystone nodes is increased from 1 to 3 with about the
          same average load (i.e. 50/3 ~ 17) per Keystone node, the response times do not have
          obvious changes for a specific number of LDAP users under 50,000. However, the response
          times for the case of half-million LDAP users grows a factor of 2 to 3. The race
          conditions have caused quite an impact on the efficiency of the servers (e.g. Apache2,
          Keystone, and LDAP) for the case of 500,000 LDAP users. </li>
      </ul>
    </section>
  </body>
</topic>
