<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="keystone_benchmarking">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Keystone Identity Performance Benchmark</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>


    <section id="overview"><title>Overview</title> The objective of this performance benchmark is to
      provide a reference for HPE Helion OpenStack Keystone users to know how the system performs in
      terms of responsiveness under a particular environment and workload. For HPE Helion OpenStack
      Keystone, there are several major operations where performance is critical: token creation,
      token validation, as well as user lookup when a LDAP backend is applied. The rest of
      operations may occur less frequently. Therefore, this report would focus on the analysis and
      benchmark for these three items. </section>
    <section id="testEnv"><title>Performance Test Environment</title> Server Models and
      Specification Ideally performance test environment should be an exact replica of the
      production system but it’s not always possible due to its associated cost, time and
      challenges. For this performance test, HP KS-PERF, as described in Table 1, has been
      established using 7-node DL360 Gen9 baremetal servers. </section>
    <simpletable frame="all" relcolwidth="1.0* 1.0* 1.0* 1.0* 1.0* 1.0*" id="simpletable_cts_tcm_25">
      <sthead>
        <stentry>Server</stentry>
        <stentry>Nodes</stentry>
        <stentry>Model</stentry>
        <stentry>Specification</stentry>
        <stentry>Operating System</stentry>
        <stentry>Softrware Versions</stentry>
      </sthead>
      <strow>
        <stentry>
          <p>KS-Perf Servers</p>
        </stentry>
        <stentry>7</stentry>
        <stentry>
          <p>HP ProLiant DL360 Gen9)</p>
        </stentry>
        <stentry>
          <ul id="ol_mfn_xcm_25">
            <li>2 x Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz</li>
            <li>132Gb RAM, dual 10Gb NIC in bonding mode</li>
          </ul>
        </stentry>
        <stentry>
          <p>hLinux cattleprod (3.14.44-1 kernel)</p>
        </stentry>
        <stentry><ul>
            <li>HPE Helion OpenStack2.0</li>
            <li>HPE Helion OpenStack 1.1.1</li>
            <li>MySQL v5.5.41</li>
          </ul></stentry>
      </strow>
      <strow>
        <stentry>
          <p>OpenLDAP Server</p>
        </stentry>
        <stentry>1</stentry>
        <stentry>
          <p>HP SE2170s Gen7</p>
        </stentry>
        <stentry>
          <ul id="ol_scv_2dm_25">
            <li>2 x Intel(R) Xeon(R) CPU X5650 @ 2.67GHz</li>
            <li>192Gb RAM, single 1Gb NIC</li>
          </ul>
        </stentry>
        <stentry>
          <p>Ubuntu 14.04.02 LTS (Trusty)</p>
        </stentry>
        <stentry>
          <p>OpenLDAP v2.4.31</p>
        </stentry>
      </strow>
    </simpletable>
    <section>Table 1 Server specifications for performance test </section>
    <section id="Keystone-LDAP"><title>Keystone-LDAP</title> For the test cases with domain specific
      LDAP backend, an OpenLDAP server employed, as described in Table 1, there are four LDAP
      domains (i.e. openldap50, openldap5k, openldap50k and openldap500k) carrying different amount
      of users (i.e. 50, 5k, 50k and 500k) respectively, as depicted in Figure 1 and Table 2. For
      the test cases with domain specific LDAP backend, an OpenLDAP server employed, as described in
      Table 1, there are four LDAP domains (i.e. openldap50, openldap5k, openldap50k and
      openldap500k) carrying different amount of users (i.e. 50, 5k, 50k and 500k) respectively, as
      depicted in Figure 1 and Table 2. <image
        href="../../media/keystone/HOSKeystoneDomain-SpecificBackend.png" id="image_eht_kcm_25"/>
      <p>Figure 1 LDAP users and groups for Keystone domains</p>
      <simpletable frame="all" relcolwidth="1.0* 1.0* 1.0* 1.0* 1.0*" id="simpletable_ths_xdm_25">
        <sthead>
          <stentry>LDAP</stentry>
          <stentry>openldap50</stentry>
          <stentry>openldap5k</stentry>
          <stentry>openldap50k</stentry>
          <stentry>openldap500k</stentry>
        </sthead>
        <strow>
          <stentry>
            <p>User Container</p>
          </stentry>
          <stentry>
            <p>ou=Users50,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_orh_b2m_25">
              <li>user50_000001 to user50_000050</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users5k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_urv_b2m_25">
              <li>user5k_000001 to user50_005000</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users50k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_nj3_c2m_25">
              <li>user50k_000001 to user50k_050000</li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Users500k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_nr5_c2m_25">
              <li>user500k_000001 to user500k_500000</li>
            </ul>
          </stentry>
        </strow>
        <strow>
          <stentry>
            <p>Group Container</p>
          </stentry>
          <stentry>
            <p>ou=Groups50,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_kl5_d2m_25">
              <li>
                <p> members of group50_1: </p>
                <p>user50_000001 to user50_000050 under Users50</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups5k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_eff_22m_25">
              <li>
                <p> members of group5k_1: </p>
                <p>user5k_000001 to user5k_000050 under Users5k</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups50k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_efq_22m_25">
              <li>
                <p>members of group50k_1: </p>
                <p>user50k_000001 to user50k_000050 under Users50k</p>
              </li>
            </ul>
          </stentry>
          <stentry>
            <p>ou=Groups500k,dc=cdl,dc=hp,dc=com</p>
            <ul id="ul_p52_f2m_25">
              <li>
                <p>members of group500k_1: </p>
                <p>user500k_000001 to user500k_000050 under Users500k</p>
              </li>
            </ul>
          </stentry>
        </strow>
      </simpletable><p>Table 2 LDAP user and group data structure for four Keystone domains (i.e.
        openldap50, openldap5k, openldap50k and openldap500k) </p></section>

    <section id="cache_mech"><title>Cache Mechanism</title> Keystone supports a caching layer that
      is above the configurable subsystems (e.g. token, identity, etc). HPE Helion OpenStack
      Keystone uses the dogpile.cache library allowing for flexible cache backends. The majority of
      the caching configuration options are set in the [cache] section. However, each section that
      has the capability to be cached usually has a caching boolean value that will toggle caching
      for that specific section. For the performance test cases with cached on, the subsystem and
      the global toggle setting are enabled and dogpile.cache.memory is employed for this study as
      configured below: Please be adviced that dogpile.cache.memory is an experimental cache
      provider and is not ready ready for production level application. <ul>
        <li>Edit etc/keystone.conf with the "[caching]" section shown below
          <codeblock>[cache]
# enable caching across all of keystone
enabled = true
  
# use in-memory cache
backend = dogpile.cache.memory</codeblock>
        </li>
        <li>Verify subsystem caching enabled
          <codeblock>[catalog]
caching = true
  
[domain_config]
caching = true
  
[identity]
caching = true
  
[resource]
caching = true
  
[revoke]
caching = true
  
[role]
caching = true
  
[token]
caching = true</codeblock>
        </li>
        <li>Restart Apache2 for the update to take effect.</li>
      </ul> Additional Note: For concurrent test cases with label 1K2P17U, it represents test
      conditions with 1 Keystone node, 2 processes configured in Apache2 and 17 concurrent users.
      Similarly, 3K2P50U 3 Keystone nodes, 2 processes configured in Apache2 and 50 concurrent
      users. where <ul>
        <li>K represents number of Keystone nodes</li>
        <li>P represents number of processes configured in Apache2 sever</li>
        <li>U represents number of concurrent users</li>
      </ul>
    </section>
    <section id="domain_spec_back_end"><title>HPE Helion OpenStack 2.0 with domain-specific LDAP
        back end</title>
      <b>LDAP user lookup performance - HPE Helion OpenStack 2.0 Keystone user lookup performance
      </b> Figure 2.a - 2.c represents the performance on non-concurrent LDAP User lookup by name
      for HPE Helion OpenStack. The study for the Keystone-LDAP query efficiency skip authorization
      process using an admin token. A baseline test using direct LDAP client (i.e. ldapserach) is
      also performed for comparison. <ul>
        <li>Generally, larger LDAP users for a domain results in higher query response time for a
          domain user lookup.</li>
        <li>The response times grow roughly linearly with amount of domain users in LDAP, as shown
          in Figure 2.b and Figure 2.c. The responsiveness slopes for HPE Helion OpenStack 2.0 is
          much lower HPE Helion OpenStack 1.1.1.</li>
        <li>HPE Helion OpenStack 2.0 behaves 10 up to several thousand times better than HPE Helion
          OpenStack 1.1.1 for LDAP user lookup operations, especially for large LDAP users (e.g.
          500k users) cases.</li>
      </ul>
      <image placement="break" href="../../media/keystone/Non-ConcurrentLDAPUserLookup_bars.png"
        id="image_rs1_5fv_q5" align="" outputclass=""/>
      <p>Figure 2.a Non-Concurrent LDAP User lookup by name</p><image placement="break"
        href="../../media/keystone/Non-ConcurrentLdapUserLookup_lines.png" id="image_srj_zfv_q5"/>
      <p>Figure 2.b Non-Concurrent LDAP User lookup by name</p>
    </section>

<section>
      <b>Concurrency Impact on HPE Helion OpenStack 2.0 Keystone</b> Figures 3.a and 3.b represent
      HPE Helion OpenStack 2.0 LDAP user lookup with concurrent conditions. <ul>
        <li>The response times grow roughly linearly with amount of domain users in LDAP even under
          concurrent conditions, as shown in Figure 3.b, but responsiveness slopes vary for
          different race conditions. </li>
        <li>Compared with baseline non-concurrent LDAP user lookup for a specific amount of LDAP
          users, the response time grows higher for cases with concurrent conditions by factor of
          5-10. While increasing numbers of concurrent users from 17 to 50 and Keystone nodes from 1
          to 3 with the same average load per Keystone node (e.g. 50/3 ~ 17), the response time
          grows much higher by another factor of 2 to 3 for the case of half-million LDAP users.
          This shows the race conditions have caused quite a bit impact on the efficiency for the
          servers (e.g. Keystone, Apache2, Keystone, and LDAP) leading toward slow responsiveness.
        </li>
      </ul>
      <image placement="break" href="../../media/keystone/HOS2_LdapUserLookup_bars.png"
        id="image_mrq_xtb_r5"/>
      <p>Figure 3.a LDAP User lookup by name for HPE Helion OpenStack 2.0 with concurrent
        conditions</p>
      <image href="../../media/keystone/HOS2_LdapUserLookup_lines.png" id="image_ggk_15b_r5"/>
      <p>Figure 3.b LDAP User lookup by name for HPE Helion OpenStack 2.0 with concurrent
        conditions</p>
    </section>
    <section><title>Create project-scoped token</title>
      <b>Keystone performance on project-scoped token creation</b> The following figures represent
      performance on project scoped token creation for a single Keystone node without concurrent
      conditions. <ul>
        <li>The response times for project scoped token creation grow roughly linearly with amount
          of domain users in LDAP, as shown in Figure 4.b, but responsiveness slopes vary for HPE
          Helion OpenStack versions and cached conditions. </li>
        <li>For non-cached cases of project scoped token creation , HPE Helion OpenStack 2.0
          performs slightly better than the HPE Helion OpenStack1.1.1 within 5%. </li>
        <li>With the cache mechanism activated for the single Keystone node, the system observes
          obvious improvement on responsiveness for all cases of project scoped token creation,
          especially the cases for HPE Helion OpenStack 2.0 (see further detail in Figure 6.b).
        </li>
      </ul>
      <image href="../../media/keystone/CPST_Nonconcurrent_barrs.png" id="image_bzm_c3v_q5"
        placement="break"/>
      <p>Figure 4.a Performance on project scoped token creation for a single Keystone node without
        concurrency</p>
      <image href="../../media/keystone/CPST_Nonconcurrent_lines.png" id="image_irn_f3v_q5"
        placement="break"/>
      <p>Figure 4.b Performance on project scoped token creation for a single Keystone node without
        concurrency</p>
      <p>The following figures represent performance on project-scoped token creation with
        concurrent conditions.</p>
      <ul>
        <li>The response times for a single Keystone node with concurrency grow roughly linearly
          with amount of domain users in LDAP, as shown in Figure 5.b, but responsiveness slopes
          vary for HPE Helion OpenStack versions and and cached conditions. </li>
        <li>For non-cached cases of project scoped token creation , HPE Helion OpenStack 2.0
          performs slightly better than the HPE Helion OpenStack1.1.1 within 5%. </li>
        <li>With the cache mechanism activated for the single Keystone node, the system observes
          obvious improvement on responsiveness for all cases of project scoped token creation. The
          improvement by cache for HPE Helion OpenStack 2.0 is even bigger than HPE Helion OpenStack
          1.1.1 (see further detail in Figure 6.b and percentage Figure 6.a and 6.b). </li>
      </ul>
      <image placement="break" href="../../media/keystone/CPST_Concurrent_bars.png"
        id="image_gtl_53v_q5"/>
      <p>Figure 5.a Performance on project-scoped token creation with concurrency (1K2P17U)
        </p><image placement="break" href="../../media/keystone/CPST_Concurrent_lines.png"
        id="image_kkc_z3v_q5"/>
      <p>Figure 5.b Performance on project-scoped token creation with concurrency (1K2P17U)
          </p><p><b>HPE Helion OpenStack Keystone Improvement Using Cache</b></p>
      <p>The following figures represent performance improvement percentage on project-scoped token
        creation for a single Keystone node with non-concurrent condition and 17 concurrent users
        condition.</p><ul>
        <li>With the cache mechanism activated for the single Keystone node, the system observes
          obvious improvement on responsiveness for all cases of project scoped token creation. </li>
        <li>The improvement for non-concurrent HPE Helion OpenStack 2.0 project scoped token
          creation for a single Keystone node is about 45% and concurrent HPE Helion OpenStack 2.0
          about 40%. The improvement percentages stay stable for cases of different amount of LDAP
          users. </li>
        <li>The improvement for non-concurrent HPE Helion OpenStack 1.1.1 project scoped token
          creation for a single Keystone node is about 15%- 30% and concurrent HPE Helion OpenStack
          1.1.1 about 20%-28%. The improvement percentages get lower for cases of larger amount of
          LDAP users. </li>
      </ul>
      <image placement="break" href="../../media/keystone/CPST_ImprovementByCache_bars.png"
        id="image_mtp_ppb_r5"/>
      <p>Figure 6.a Create-project-scoped-token performance improvement percentage by cache.
        (Concurrency cases based on 1K2P17U)</p>
      <image placement="break" href="../../media/keystone/CPST_ImprovementByCache_lines.png"
        id="image_qf2_spb_r5"/>
      <p>Figure 6.b Create-project-scoped-token performance improvement percentage by cache.
        (Concurrency cases based on 1K2P17U)</p>
      <p><b>Concurrency Impact on HPE Helion OpenStack 2.0 Keystone</b></p>
      <p>The following figures represent concurrency impact on HPE Helion OpenStack 2.0 performance
        for project scoped token creation.</p>
      <ul>
        <li>The response times grow roughly linearly with amount of domain users in LDAP for all
          test cases here, as shown in Figure 7.b, but responsiveness slopes vary for different race
          and cached conditions. </li>
        <li>Compared with the baseline non-cached non-concurrent project scoped token creation for a
          specific amount of LDAP users, the response time is getting much higher for cases with
          concurrent conditions by factor of 10. While increase numbers of concurrent users from 17
          to 50 and Keystone nodes from 1 to 3 with about the same average load (i.e. 50/3 ~ 17) per
          Keystone node, the response times do not have obvious changes for a specific amount of
          LDAP users under 50k. However, the response times for the case of half-million LDAP users
          grows a factor of 2 to 3. The race conditions have caused quite a bit impact on the
          efficiency of the servers (e.g. Apache2, Keystone, and LDAP) for the case of half-million
          LDAP users. </li>
        <li>Turning on the cache mechanism within Keystone nodes, the system observes obvious
          improvement on responsiveness for all cases of project scoped token creation. </li>
        <li>The improvement by cache for HPE Helion OpenStack 2.0 project scoped token creation with
          either 17 or 50 con-concurrent users is about 40%. The improvement percentages stay stable
          for cases of different amount of LDAP users. </li>
      </ul>
      <image placement="break" href="../../media/keystone/HOS2_CPST_KPU_bars.png"
        id="image_hcd_qqb_r5"/> Figure 7.a Concurrency impact on Keystone under cached and
      non-cached conditions <image placement="break"
        href="../../media/keystone/HOS2_CPST_KPU_lines.png" id="image_wk4_wqb_r5"/>
      <p>Figure 7.b Concurrency impact on Keystone under cached and non-cached conditions</p>
    </section>
    <section><title>Summary</title> Below is a summary of th performance improvements for LDAP user
      lookups and creating project-scoped tokens.
      <p><b>LDAP User Lookup</b></p>
      <ul>
        <li>Generally, larger LDAP users for a domain results in higher query response time for a
          domain user lookup in a linear relationship with slopes varied, no matter concurrency or
          non-concurrency is involved. </li>
        <li>HPE Helion OpenStack 2.0 behaves 10 up to several thousands times better than HPE Helion
          OpenStack 1.1.1 for LDAP user lookup operations, especially for larger LDAP users like
          half-million users by a factor of several thousands. </li>
        <li>The response time grows higher for cases with concurrent conditions by factor of 5-10,
          when compared with baseline non-concurrent LDAP user lookup for a specific amount of LDAP
          users. While increasing numbers of concurrent users from 17 to 50 and Keystone nodes from
          1 to 3 with the same average load per Keystone node (e.g. 50/3 ~ 17), the response time
          grows much higher by another factor of 2 to 3 for the case of half-million LDAP users.
          This shows the race conditions have caused quite a bit impact on the efficiency for the
          servers (e.g. Keystone, Apache2, Keystone, and LDAP) leading toward slow responsiveness.
        </li>
      </ul>
      <b>Create Project Scoped Token</b>
      <ul>
        <li>The response times for project scoped token creation grow roughly linearly with amount
          of domain users in LDAP, but responsiveness slopes vary for HPE Helion OpenStack versions
          and cached conditions, , no matter concurrency or non-concurrency is involved. </li>
        <li>HPE Helion OpenStack 2.0 performs slightly better than the HPE Helion OpenStack1.1.1
          within 5% for project scoped token creation. </li>
        <li>With the cache mechanism activated for the single Keystone node without concurrency, the
          system observes obvious improvement on responsiveness for project scoped token creation. <ul>
            <li>The improvement for non-concurrent HPE Helion OpenStack 2.0 project scoped token
              creation for a single Keystone node is about 45% and concurrent HPE Helion OpenStack
              2.0 about 40%. The improvement percentages stay stable for cases of different amount
              of LDAP users. </li>
            <li>The improvement for non-concurrent HPE Helion OpenStack 1.1.1 project scoped token
              creation for a single Keystone node is about 15%- 30% and concurrent HPE Helion
              OpenStack 1.1.1 about 20%-28%. The improvement percentages get lower for cases of
              larger amount of LDAP user </li>
          </ul></li>
        <li>With the cache mechanism activated for the single Keystone node with concurrency, the
          system observes obvious improvement on responsiveness for project scoped token creation. <ul>
            <li>The improvement by cache for HPE Helion OpenStack 2.0 project scoped token creation
              with either 17 or 50 concurrent users is about 40%. The improvement percentages stay
              stable for cases of different amount of LDAP users. </li>
          </ul></li>
        <li>Compared with the baseline non-cached non-concurrent project scoped token creation for a
          specific amount of LDAP users, the response times get much higher for cases with
          concurrent conditions by factor of 10. While increase numbers of concurrent users from 17
          to 50 and Keystone nodes from 1 to 3 with about the same average load (i.e. 50/3 ~ 17) per
          Keystone node, the response times do not have obvious changes for a specific amount of
          LDAP users under 50k. However, the response times for the case of half-million LDAP users
          grows a factor of 2 to 3. The race conditions have caused quite a bit impact on the
          efficiency of the servers (e.g. Apache2, Keystone, and LDAP) for the case of half-million
          LDAP users. </li>
      </ul>
    </section>
  </body>
</topic>
