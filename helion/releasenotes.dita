<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN"  "topic.dtd" >
<topic xml:lang="en-us" id="release_notes">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Release Notes</title>
  <body>
    <!--Needs Edit-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/releasenotes.html" scope="external" format="html">Click
        here</xref> for a frameless version of this page where the links should work.</note>
    <section id="about">
      <p>This document provides an overview of the features contained within <keyword
          keyref="kw-hos-phrase"/>, including known issues and workarounds for this release:</p>
      <ul>
        <li><xref href="releasenotes.dita#release_notes/features">Features Available in this
            Release</xref></li>
        <li><xref href="releasenotes.dita#release_notes/known_issues">Known Issues in this
            Release</xref></li>
      </ul>
    </section>

    <section id="features"><title>Features Available in this Release</title>
      <p><b>New GUI Installer</b></p>
      <p><keyword keyref="kw-hos-phrase"/> introduces a GUI installer for configuring and installing
        the operating system, the cloud, and OpenStack services. It provides an easy way to populate
        the configuration files needed to define the <xref href="input_model.dita">Input
          Model</xref>, and uses them to configure and install your cloud. For help using the
        installer, see the page <xref href="gui_installer.dita"/>.</p>
      <p><b>Transport Layer Security (TLS) Support</b>
      </p><p>TLS is now supported for public endpoints. See <xref href="security/tls.dita">Enabling
          TLS for Public Endpoints</xref> for more information.</p>
      <p><b>Encryption of sensitive data</b></p>
      <p>In this release, connection details and passwords are encrypted and or protected. See <xref
          href="security/encrypted_storage.dita">Encryption of Passwords and Sensitive Data
        </xref>for more information.</p>
      <p><b>AppArmor enabled</b></p>
      <p>AppArmor in <keyword keyref="kw-hos-phrase"/> is installed and enabled on the KVM compute
        nodes by default. It runs in enforce mode. It enforces mandatory access control policies for
        the libvirt process. See <xref href="security/using_apparmor.dita#topic_rmq_j1v_4t">AppArmor
          in <keyword keyref="kw-hos-phrase"/></xref> for more information.</p>
      <p><b>The lifecycle manager only supports American English language</b></p>
      <p>The lifecycle manager installation process prompts you to select a language. The only
        supported language at this time is American English.</p>
      <p><b>Local Git repository for config tracking</b></p>
      <p>In <keyword keyref="kw-hos-phrase"/>, a local git repository is used to track configuration
        changes as well as acting as a repository for the configuration processor to gather
        configuration settings from. The operations work as explained below: <ul>
          <li>Operations under <codeph>~/helion</codeph> are under the aegis of git. You’ll need to
            commit any config into git (on the "site" branch) before the configuration processor can
            run it. The script that runs the config processor now guards against uncommitted
            changes.</li>
          <li>Deployment operations are run from a scratch directory that's assembled from various
            parts (the ansible output of the configuration processor and the playbooks). The
            directory is created using <codeph>ready-deployment.yml</codeph>.</li>
          <li>All deployment operations should be run from
              <codeph>~/scratch/ansible/next/hos/ansible</codeph>.</li>
        </ul>
      </p>
      <p>The configuration processor uses the config files stored in the repo to apply configuration
        settings. An explanation can be found <xref href="installation/using_git.dita">in this
          topic</xref>.</p>
      <p><b>New UEFI support</b></p>
      <p><keyword keyref="kw-hos-phrase"/> includes support for UEFI (which replaces the BIOS on
        newer servers), while continuing to support Legacy BIOS. Select the required mode on each
        node through its BIOS settings before beginning the install process. The installer will
        detect and use the mode you have selected for each node.</p>
      <p><b>Updated <keyword keyref="kw-hos"/> Services</b></p>
      <p>We have included the core set of OpenStack services from the <xref
          href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo" scope="external" format="html"
          >Kilo release</xref> with the exception of limitations notated in this document.</p>
      <p><b>More Block Storage Backend Options Available</b></p>
      <p>You can now choose between Ceph, VSA, and 3PAR for Block Storage backends.</p>
      <p>See: <xref href="blockstorage/blockstorage_overview.dita">Block Storage Overview</xref> for
        more details</p>
      <dl>
        <dlentry>
          <dt>DNSaaS 2.0</dt>
          <dd>
            <b>New Features</b>
            <ul>
              <li>New API V2 <ul>
                  <li>Adds Zones that replaces Domains from V1.</li>
                  <li>Adds RecordSets that replaces Records from V1. </li>
                </ul></li>
              <li>Scalability and Resilience improvements <ul>
                  <li>New service PoolManager that handles coordination / resiliency.</li>
                  <li>New service MiniDNS.</li>
                </ul></li>
              <li>Support for wildcard records.</li>
              <li>Support for Secondary Zones.</li>
            </ul>
            <b>Known Issues</b>
            <ul>
              <li>Selecting a flavor larger than m1.medium for the Deployer or Control Plane nodes
                fails due to a failure to re-size LVM. The recommended flavors for the Deployer and
                Control Plane nodes are m1.small or m1.medium.</li>
            </ul>
            <b>Not included in the release</b>
            <ul>
              <li>Backend support for BIND9 / DynECT / Akamai is not included in this release.</li>
              <li>Support for Ceilometer is not included in this release.</li>
              <li>There is currently no backup and restore functionality.</li>
            </ul>
            <b>Bugfixes</b>
            <ul>
              <li>API TTL input validation.</li>
              <li>Improve error message on API actions.</li>
              <li>Launchpad 1471161 <ul>
                  <li>CVE-2015-5695 Quotas were being bypassed.</li>
                  <li>CVE-2015-5694 does not enforce the DNS protocol limit concerning record set
                    sizes.</li>
                </ul></li>
              <li>Launchpad 1471158: Incorrect regular expressions used for schema validation.</li>
              <li>Launchpad 1497031: Authenticated Denial of Service in Blacklists.</li>
            </ul></dd>
        </dlentry>
      </dl>
      <p><b>Neutron FWaaS, LBaaS and VPNaaS Extensions</b></p>
      <p>We have implemented networking extensions firewall, load balancing, and virtual private
        networks. Here is a description of each of these: <ul>
          <li>
            <p>LBaaS (Load-Balancing-as-a-Service) is a Neutron extension that introduces a load
              balancing feature set. <keyword keyref="kw-hos-phrase"/> installs by default LBaaS
              version 2 which uses an implementation of haproxy but third-party load balancers can
              be deployed as well. If a third-party driver doesn’t support LBaaS V2, then installing
              LBaaS V1 instead of V2 is possible.</p>
            <ul>
              <li>For more information, see: <xref href="networking/lbaas_admin.dita">Load Balancer
                  (LBaaS) Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>FWaaS (Firewall-as-a-Service) is a Neutron extension that introduces a firewall
              feature set. The included reference implementation is using iptables to filter traffic
              but third-party firewalls can be deployed with this extension into <keyword
                keyref="kw-hos"/> as well.</p>
            <ul>
              <li>For more information, see: <xref href="networking/fwaas.dita">Firewall (FWaaS)
                  Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>VPNaaS (Virtual-Private-Network-as-a-Service) is a Neutron extension that introduces
              a VPN feature set. It allows for point-to-point traffic between two routers in
              (potentially) different datacenters with configurable encryption hence supporting
              secure multi Availability Zone architectures for customers.</p>
            <ul>
              <li>For more information, see: <xref href="networking/vpnaas.dita">VPNaaS
                  Configuration</xref></li>
            </ul>
          </li>
          <li>
            <p>Multiple networks is a feature in <keyword keyref="kw-hos-phrase"/> that enables
              bridging of multiple external physical networks to a Neutron network. Using this
              functionality a tenant can dictate the path taken by the data in the physical network
              outside the cloud. It can also be used for multi-homing to two physical networks.</p>
            <ul>
              <li>For more information, see: <xref href="networking/multinetwork.dita">Multiple
                  External Network Configuration</xref></li>
            </ul>
          </li>
        </ul>
      </p>
      <p><b>New Network Configuration Options</b></p><ul>
        <li>Network interface (NIC) bonding with multiple modes will allow you to set up a highly
          available and performant network infrastructure.</li>
        <li>Multi-network support will allow you to bridge multiple external physical networks to a
          Neutron network. This will enable you to dictate the path taken by the data in the
          physical network outside of your cloud deployment.</li>
        <li>Network separation (both physical and VLAN) will allow you to segregate traffic by type.
          For example, traffic can be separated into management, tenant, external, and service
          networks.</li>
      </ul>
      <p><b>NIC Mapping Feature</b></p> The new NIC mappings feature in <keyword keyref="kw-hos"/>
      allows the cloud administrator to map network interface names to PCI bus addresses. This is
      important because for well-understood reasons, Linux can sometimes name the network interfaces
      in a pseudo-random order. So in a cluster of identical machines, the <b>eth3</b> device can
      appear as <b>eth4</b> on one or more machines in the cluster. Once this erroneous name is
      determined by the operating system, it will use persistence rules to ensure that the NIC
      device is called <b>eth4</b> thereafter. This makes cluster configuration using Helion quite
      difficult if not impossible, if most machines understand the Management Network to be
        <b>eth3</b>, but some subset of machines understand that network to be on <b>eth4</b> ( and
      whatever was on eth4 is similarly swapped to <b>eth3</b>). <p>To resolve this, and to ensure
        that all NIC devices have consistent naming across the entire cloud, <keyword
          keyref="kw-hos-phrase"/> includes NIC Mapping facilities. This is a mechanism in the Input
        Model to define a specific NIC interface for a given PCI address for a given class of
        machine. Generally, when all of the network interfaces are defined in the <xref
          href="input_model.dita#input_model/co_nicmappings" type="section">Input Model</xref>, it
        is advisable to use the traditional naming, ie <b>ethN</b>. However, if one or more
        interfaces are not defined in the Input Model (perhaps because the Ansible network is not
        defined, or because there is a customer-specific network which is outside the scope of
        Helion), then it is recommended that a different naming convention is used, to prevent the
        definitions from colliding with existing configuration data. For example, the NIC mapping
        definitions could use <b>hos0</b> through <b>hosN</b> rather than <b>eth0</b> through
          <b>ethN</b>, bearing in mind that the new naming must be consistent across the entire
        Input Model.</p>
      <p><b>Monasca-based Monitoring</b></p><p>Monasca is a multi-tenant, scalable, fault-tolerant,
        monitoring service. It uses a REST API for high-speed metrics processing and querying, and
        has a streaming alarm and notification engine. In the <keyword keyref="kw-hos-phrase"/>
        release, the OpenStack monitoring standard, Monasca, is supported as the <keyword
          keyref="kw-hos"/> monitoring solution with the following exceptions (which are not
        implemented):</p>
      <ul>
        <li>Transform Engine</li>
        <li>Events Engine </li>
        <li>Anomaly and Prediction Engine</li>
      </ul>
      <p>Monitoring is integrated for the following services: Logging, Neutron, Swift, Ceilometer,
        Heat, Rabbit, MySQL, HA Proxy, Glance, Cinder, Monitoring of Monasca, Nova, HPE Linux for HP
        Helion.</p><p>Learn more about Monasca here: <xref href="operations/monitoring_service.dita"
          >Monasca Overview</xref></p>
      <p><b>Monitoring Configuration Options Available for Metrics Database</b></p><p>In this
        release you will have the option to specify which database platform you want to use for the
        metrics database. The default option with the installation is Vertica. The opensource
        alternative is InfluxDB.</p> Learn more about these options here: <xref
        href="administration/configure_monitoring.dita">Configuring the Monitoring Service</xref>
      <p><b>Centralized Logging</b></p> Logging for the services below is enabled in the default
      configuration. You will have the ability to disable per-service logging as needed. <p>Logging
        is integrated for the following services: Nova, Glance, Swift, Neutron, Ceilometer, Monasca,
        Horizon, Keystone, Cinder, Heat, OpsConsole, DNSaaS, LBaaS, FWaaS, and Trove.</p><p>Learn
        more about Centralized Logging here: <xref href="operations/centralized_logging.dita"
          >Centralized Logging Overview</xref></p>
      <p><b>Core Features in <keyword keyref="kw-hos-phrase"/></b></p>
      <!-- 
        
        Core and Non-core descriptions provided by Carolyn Adler (cadler@hpe.com) in Legal.
        DO NOT CHANGE THE FOLLOWING TWO PARAGRAPHS
        
        -->
      <p>Core features in the <tm tmtype="reg">OpenStack</tm> Foundation’s Kilo release (as
        identified below) are enabled in the <keyword keyref="kw-hos-phrase"/> standard settings and
        are included in Helion standard product support. </p>
      <p>Non-core services (listed in the Features tables below) are provided as-is. Some non-core
        services are included in this release to provide early access to features that HPE may
        support fully in future releases, but HPE cannot guarantee that support will be provided or
        issues resolved in a future release. Other non-core features are provided in this release
        for a limited time and may be deprecated in future releases. HPE will attempt to provide
        guidance and telephone support for these features at its discretion. If you need additional
        support on any of these features for a specific use case, please contact the Helion
        professional services team. </p>
      <table frame="none">
        <tgroup cols="2">
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Status</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Ceilometer</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Cinder</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Glance</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Heat</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Horizon</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Keystone</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Monasca</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Nova</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Swift</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Tempest</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron LBaaS v1</entry>
              <entry>Non-core</entry>
            </row>
            <row>
              <entry>Neutron LBaaS v2</entry>
              <entry>Core</entry>
            </row>
            <row>
              <entry>Neutron FWaaS</entry>
              <entry>Non-core</entry>
            </row>
            <row>
              <entry>Neutron VPNaaS</entry>
              <entry>Non-core</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <!-- End Section "CoreFeatures"-->
    </section>


    <section id="known_issues">
      <title>Known Issues in this Release</title>
      
      
      <!-- INSTALLATION -->
      
      <!-- https://jira.hpcloud.net/browse/DOCS-2265 -->
        <p id="DOCS-2265"><b>The Use of the --limit When Ansible Playbooks Fail Should Be Avoided</b></p>
      <p>When an Ansible playbook fails, the error will display some text that looks similar to
        this:</p>
      <codeblock>'to retry, use: --limit @/home/stack/site.retry'</codeblock>
      <p>However, in the HPE Helion OpenStack environment this switch is not needed, you can simply
        rerun the same playbook with no additional content.</p>
      <p>In addition, the use of the <codeph>--limit</codeph> switch can cause issues with Vertica
        where it will change the dbadmin SSH key which will cause various issues with your
        environment.</p>
      
      <!-- https://jira.hpcloud.net/browse/TODO -->
      <p><b>Installing Swift</b></p>
      <p>Swift installs with the following limitations:</p>
      <ul>
        <li>Only one Swift zone is supported.</li>
      </ul>
      
      <!-- https://jira.hpcloud.net/browse/DOCS-2143 -->
      <p id="DOCS-2143"><b>Rsync Requests are Blocked by the Firewall on Systems with a Dedicated Swift
        Network</b></p>
      <p>When a dedicated network is used for the Swift backend network traffic, requests by the
        Swift replication process to rsync are blocked by the firewall. To resolve this issue, add
        the <codeph>swift-rsync</codeph> service to the <codeph>component-endpoints</codeph> in the
          <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file before
        deploying for the SWIFT network group.</p>
      <p>For example:</p>
      <codeblock>
  #
  # SWIFT
  #
  # This is the network group that will be used to for
  # Swift back-end traffic between proxy, container, account
  # and object servers
  #
  - name: SWIFT
    hostname-suffix: swift
  
    component-endpoints:
      - swift-container
      - swift-account
      - swift-object
      - swift-rsync</codeblock>
      <!-- SERVICES -->
      <!-- MONITORING -->
      <p><b>Monitoring (Monasca) Limitations</b></p> The monitoring service has the following
      limitations: <ul id="ul_fn1_d45_tt">
        <li>Vertica is the default metrics database option in the install. If you want to use
          InfluxDB then read <xref
            href="administration/configure_monitoring.dita#configure_monitoring/config"
            type="section">Configuring Monasca</xref> for more details.</li>
        <li>InfluxDB is not configured behind a VIP. It must directly access one node in the
          cluster.</li>
        <li>Backup/restore of Monasca configuration and metrics data is not available.</li>
      </ul>
      
      <!--https://jira.hpcloud.net/browse/DOCS-1889-->
      <p id="DOCS-1889"><b>Issue: Monasca Command-line Client (CLI) Redirect Fails</b></p>
      <p>If you are using the Monasca CLI and you receive the error below, we have included a
        workaround:</p>
      <p>Error:
        <codeblock>$ monasca metric-list|grep hostname
'ascii' codec can't encode characters in position 1058218-1058219: ordinal not in range(128)</codeblock></p>
      <p>Workaround:
        <codeblock>PYTHONIOENCODING=utf-8 monasca metric-list | grep hostname</codeblock></p>
      
      <!--https://jira.hpcloud.net/browse/DOCS-1850-->
      <p id="DOCS-1850"><b>Monasca Forwarder Log Reporting False Errors</b></p>
      <p>There is an issue where the <codeph>/var/log/monasca/agent/forwarder.log</codeph> log file
        will contain many lines containing text like this:</p>
      <codeblock>2015-09-23 17:33:34 UTC | ERROR | forwarder |
monascaclient.exc(exc.py:60) | exception: Authentication failed.</codeblock>
      <p>This is due to an issue where the monasca-agent is attempting to authenticate when it's
        token has expired. These can be ignored.</p>
      
      <!--https://jira.hpcloud.net/browse/DOCS-1832-->
      <p id="DOCS-1832"><b>Issue: The 'Cinderlm diagnostics monitor' alarm alerts with an 'UNDETERMINED'
        state</b></p>
      <p>This alert is not always reported but may occur after the system has been running for some
        time.</p>
      <p>Check if any other Cinder related alarms are alerting. If no other Cinder related alarms
        are alerting then this alert for the Cinderlm diagnostics monitor alarm can be ignored. If
        other Cinder related alarms are alerting then follow the mitigation steps described in the
          <xref href="operations/alarms.dita#alarmdefinitions/blockstorage">Service Alarm
          Definitions</xref> section.</p>
      
      
      <!--https://jira.hpcloud.net/browse/DOCS-1833-->
      <p id="DOCS-1833"><b>Issue: The 'Swiftlm-scan' alarm sticks</b></p>
      <p>The <codeph>swiftlm-scan</codeph> monitor alarm may be triggered. However, once triggered,
        the alarm does not automatically return to the normal state even if the condition that
        caused it has been resolved. To confirm this, run the following command:</p>
      <codeblock>monasca measurement-list swiftlm.swift.swift_services -3 --merge_metrics --dimensions hostname=&#60;hostname></codeblock>
      <p>
        <note>The <codeph>hostname</codeph> value is the host reporting the 'swiftlm-scan' monitor
          alarm.</note>
      </p>
      <p>If the command output reports that the metrics are being reported then the system is fine
        and the alarm can be ignored.</p>
      
      
      <!--https://jira.hpcloud.net/browse/DOCS-2056-->
      <p id="DOCS-2056"><b>The Threshold Engine is not deployed properly if you use a single Control Plane
        node</b></p>
      <p>If you are using a cloud model that has a single control plane (controller) node, as
        specified by having the <codeph>member-count</codeph> value in your
          <codeph>control_plane.yml</codeph> set to <codeph>1</codeph> for your control-planes, then
        the Storm supervisor processes will not get properly installed. This causes the threshold
        engine to not run.</p>
      <p>The workaround is as follows:</p>
      <ol>
        <li>Make the following change to the
            <codeph>~/helion/hos/ansible/roles/storm/vars/nodes.yml</codeph> file. <p>Change the
              <codeph>thresh_node_count</codeph> line below:</p>
          <p>From:
            <codeblock>thresh_node_count: "{{ MON_THR.members.admin | length }}"</codeblock></p>
          <p>To: <codeblock>thresh_node_count: "{{ groups['MON-THR'] | length }}"</codeblock></p>
        </li>
        <li>Commit the changes:
          <codeblock>cd ~helion/hos/ansible
git add ~/helion/hos/ansible/roles/storm/vars/nodes.yml
git commit -m "update storm_supervisor_enabled value"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Create the deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Deploy the change:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible 
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
      </ol>
      
      
      
      <!-- OPS CONSOLE -->
      <!-- https://jira.hpcloud.net/browse/TODO -->
      <p><b>Operations Console (Ops Console)</b></p> The Operations Console has the following
      limitations (listed below by menu option in the console): <ul id="ul_wv1_d45_tt">
        <li>Dashboard <ul id="ul_s2b_d45_tt">
            <li>There are two notification sections for the Storage services. The one in the upper
              left is the Compute service mislabeled as Storage.</li>
            <li>The New Alarms section uses the UTC timezone to determine timeframes regardless of
              what timezone is in the settings.</li>
            <li>The New Alarms section will only display alarms that have a service dimension
              defined.</li>
            <!-- https://jira.hpcloud.net/browse/DOCS-1867 -->
          <li id="DOCS-1867">Platform Services (HDP Summary) alarm panels will only contain data if the Helion
              Development Platform services are installed.</li>
          </ul>
        </li>
        <li>Logging Dashboard <ul id="ul_jfb_d45_tt">
            <li>There is no auto-logon function, users must use the credentials in the <xref
                href="operations/centralized_logging.dita#centralized_logging/interface">Logging
                Overview</xref> to access the UI.</li>
            <li>The Kibana console intermittently will not function correctly if Kibana is on a
              different network than the Operations Console. If both Kibana and the Operations
              Console are on the same network then this issue should not present itself.</li>
          </ul></li>
        <li>Alarms <ul id="ul_egb_d45_tt">
            <li>The tile view of alarms with lengthy dimensions does not render cleanly.</li>
          </ul></li>
        <li>Compute Instances <ul id="ul_rgb_d45_tt">
            <li>Compute instance utilization metrics are not available.</li>
          </ul>
        </li>
        <li>Networking - Alarm Summary <ul id="ul_dhb_d45_tt">
            <li>Alarms may be limited to only those with the dimension "service=networking".</li>
          </ul></li>
        <li>Storage - Alarm Summary <ul id="ul_shb_d45_tt">
            <li>Alarms with the dimension "service=compute" sometimes appear in this list in
              addition to the "service=block-storage" and "service=object-storage" ones.</li>
          </ul></li>
        <li>HDP - Alarm Summary <ul id="ul_h3b_d45_tt">
            <!-- https://jira.hpcloud.net/browse/DOCS-1867 -->
            <li>This section will only contain data if the Helion Development Platform services are
              installed.</li>
          </ul></li>
        <li>Masthead Items <ul id="ul_v3b_d45_tt">
            <li>The Help and EULA links will not work properly. You can view the Help files here:
                <xref href="operations/opsconsole_overview.dita">Operations Console
              Overview</xref></li>
            <li>User information panel fails to load making it so a user cannot update their email
              address or password.</li>
          </ul></li>
      </ul>
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1866 -->
      <p id="DOCS-1866"><b>Issue: The Operations Console does not support alarm pagination</b></p>
      <p>The Operations Console UI does not support >10,000 simultaneous alarm instances.
        Configuring alarm definitions such that the number of alarms instances exceeeds the 10,000
        mark may cause some alarms to not show in the UI. The workaround for this issue is to access
        those alarms via either the API or the command-line clients.</p>
      
        
      <!-- LOGGING -->
      <p id="logging"><b>Centralized Logging Limitations</b></p>
      <p>The Centralized Logging service has the following limitations:</p>
      <ul>
        <li>Alarm definition and generation for Kibana and curator are not integrated.</li>
        <li>Logging-monitor 'stop' playbook is not integrated.</li>
      </ul>
      <p><b>Issue: The Elasticsearch Cluster Name Must Be Unique</b></p>
      <p>The default Elasticsearch cluster name is set to <codeph>elasticsearch</codeph> in the
        logging configuration files. It is necessary during installation to change this value to a
        unique name. This step is included in our installation documentation.</p>
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1929 -->
      <p id="DOCS-1929"><b>The Elasticsearch high/low watermark alarms will show up in "Undefined"</b></p>
      <p>In the Operations Console, the Elasticsearch high/low watermark alarms will show up in the
        “Undefined” section instead of in the "Logging" section.</p>
      
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1891 -->
      <p id="DOCS-1891"><b>Issue: The Heap parameters for Elasticsearch and Logstash are auto-tuned
        incorrectly</b></p>
      <p>There is an issue where the Heap parameters for Elasticsearch and Logstash are incorrectly
        auto-tuned. The default values are listed in <xref
          href="administration/configure_logging.dita#configure_logging/general_config">Configuring
          the Centralized Logging Service</xref>. There is a bug where Ansible is reporting the
        memory values incorrectly as values that are slightly less than what they actually are. So,
        for example, if you have 32GB of RAM you should fall into the "Small" auto-tuning category
        but because of this incorrectly reported value you are actually falling into the "Demo"
        category. We recommend that you manually modify the heap values on the lifecycle manager
        node for systems with the specified amount of RAM in the table below to the values listed.
        You will edit these values in the <codeph>logging_possible_tunings</codeph> section of the
        file below:</p>
      <codeblock>~/helion/my_cloud/config/logging/main.yml</codeblock>
      <table frame="all" rowsep="1" colsep="1" id="table_pbd_k2y_5t">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>RAM</entry>
              <entry>Key</entry>
              <entry>elasticsearch_heap_size</entry>
              <entry>logstash_heap_size</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>32GB</entry>
              <entry>demo</entry>
              <entry>from 256m to 8g</entry>
              <entry>from 256m to 2g</entry>
            </row>
            <row>
              <entry>64GB</entry>
              <entry>small</entry>
              <entry>from 8g to 16g</entry>
              <entry>from 2g to 4g</entry>
            </row>
            <row>
              <entry>128GB</entry>
              <entry>medium</entry>
              <entry>from 16g to 32g</entry>
              <entry>from 4g to 8g</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      
      
      
      <p><b>Issue: Logrotate Settings for MySQL, keepalived, and RabbitMQ are Invalid</b></p>
      <p>There is currently a workaround needed on the controller nodes to fix the logrotate
        settings for the MySQL, keepalived, and RabbitMQ services.</p> The logrotate for MySQL,
      keepalived and RabbitMQ is being overwritten by an invalid logrotate configuration supplied in
      the logging-server component. A valid logrotate configuration for these services is supplied
      in the following files on the deployer:
        <codeblock>/home/stack/scratch/ansible/next/hos/ansible</codeblock><p>The files in that
        directory are:</p><codeblock>
mysql: ./roles/FND-MDB/templates/etc/logrotate.d/mysql
keepalived: ./roles/keepalived/templates/keepalived.logrotate.j2
rabbit: ./roles/rabbitmq/templates/rabbitmq-server.logrotate.j2</codeblock>
      <p>To correct this problem if the above configuration is copied manually into
          <codeph>/etc/logrotate.d</codeph> on the controller nodes to replace the existing
        configuration then this should correct the logrotate configuration for these
        services.</p><p>Note any variables such as <codeph>{{ }}</codeph> need to be replaced
        correctly.</p> In the case of RabbitMQ and keepalived this can be achieved by editing the
        <codeph>logrotate.j2</codeph> with a comment and running a reconfigure
      (rabbitmq-reconfigure.yml and FND-CLU-reconfigure.yml on the deployer) <p>In the case of MySQL
        this needs to be corrected manually as below: <codeblock>File ./roles/FND-MDB/templates/etc/logrotate.d/mysql</codeblock>
        <codeblock>
{ log_dir }}/*.log {
        daily
        rotate 7
        missingok
        create 640 mysql adm
        notifempty
        compress
        delaycompress
        sharedscripts
        postrotate
               if test -x /usr/bin/mysqladmin &#38;&#38; \
                   /usr/bin/mysqladmin ping &#38;>/dev/null
               then
                   roothome=~root
                  /usr/bin/mysqladmin --defaults-file=${roothome}/.my.cnf --socket="{{ server_socket}}" flush-logs
               fi
        endscript
}</codeblock></p>
      <p>should be copied into /etc/logrotate.d/mysql on the controller nodes and variables replaced
        to result in this config:</p>
      <codeblock>
/var/log/mysql/*.log {
          daily
          rotate 7
          missingok
          create 640 mysql adm
          notifempty
          compress
          delaycompress
          sharedscripts
          postrotate
                  if test -x /usr/bin/mysqladmin &#38;&#38; \
                     /usr/bin/mysqladmin ping &#38;>/dev/null
                  then
                     roothome="$(realpath ~root)"
                     /usr/bin/mysqladmin --defaults-file=${roothome}/.my.cnf --socket="/var/run/mysqld/mysqld.sock" flush-logs
                  fi
          endscript
}</codeblock>
      
      
      <!-- BLOCK STORAGE -->
      <p><b>Issue: Kilo functionality not supported by the Kilo version of
        python-cinderclient</b></p>
      <p>The version of the python-cinderclient available in <keyword keyref="kw-hos-phrase"/> does
        not support some functionality available from the cinder services, including incremental
        backup and private volume types.</p> If you install a new version of python-cinderclient,
      and if you have OS_ENDPOINT_TYPE defined, you must also define CINDER_ENDPOINT_TYPE. For
      example: <codeblock>export CINDER_ENDPOINT_TYPE=${OS_ENDPOINT_TYPE}</codeblock>
      <p>The workaround for this is the following:</p>
      <ol>
        <li>Create a new virtual environment in a suitable directory:
          <codeblock>cd &#60;dir>
virtualenv cinder_venv</codeblock></li>
        <li>Activate the new virtual environment:
          <codeblock>source ./cinder_venv/bin/activate</codeblock></li>
        <li>Use pip to install the python-cinderclient: <codeblock>pip install python-cinderclient</codeblock>
          <p>or if the machine is behind a proxy, use:</p>
          <codeblock>pip install --proxy [user:passwd@]proxy.server:port python-cinderclient</codeblock></li>
        <li>Verify the version you have installed is version 1.4.0 or newer:
          <codeblock>cinder --version</codeblock></li>
        <li>To use this virtual environment at a later time, or from a different shell session, you
          must activate the virtual environment as stated in step #2 above:
          <codeblock>source &#60;some-dir>/cinder_venv/bin/activate
cinder --version</codeblock></li>
      </ol>
      
      
      
      <p><b>In the Cinder CLI no more than 1000 Cinder volumes can be displayed</b></p>
      <p>Due to a known issue with the version of the OpenStack python-cinderclient in <keyword
          keyref="kw-hos-phrase"/> (1.1.3), the maximum number of volumes that can be displayed is
        1000. The fixed client is 1.3.1. The workaround is to use python-cinderclient 1.3.1.</p>
      
      
      
      <!-- HORIZON -->
      <p><b>Cannot open Launch Instance from Volume on Horizon Dashboard</b></p>
      <p>User cannot open the Launch Instance wizard from a newly created bootable volume’s actions
        dropdown on the Volumes Page. </p><p>The workaround is to refresh the page.</p>
      
      
      
      <p><b>Launch Instance form in Horizon not populated when choosing volumes</b></p>
      <p>When using the Launch Instance action in the Volumes table, the Launch Instance Wizard form
        doesn't get pre-populated with the Volumes option selected, nor does it select the given
        Volume. The workaround is to select an available bootable volume by selecting the Volume
        option from the Select Boot Source dropdown on Launch Instance wizard.</p>
      <p><b>Start Instances in Horizon enabled</b></p>
      <p>Note that for Start Instances at Instances table level in Horizon, the More Actions
        dropdown is enabled even if an instance status is Active.</p>
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1770 -->
      <p id="DOCS-1770"><b>Issue: The <codeph>Start
            Instances</codeph> Option is Available In Horizon for Instances that are Already
          Started</b></p>
      <p>There is currently an OpenStack bug where the <codeph>Start Instances</codeph> option is
        available in the instance action drop-down even if your instances are already in an Active
        state.</p>
      
      
      <p><b>A Horizon create a 'bootable volume' fails UI task</b></p>
      <p>When you navigate to 'volumes' and choose to create a 'bootable volume,' if you do not
        navigate elsewhere, the attempt to launch an instance from that volume will fail. That is,
        nothing will happen.</p><p> To work around this issue, after choosing to create a 'bootable
        volume,' refresh the page in the browser and then proceed.</p>
      
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1932 -->
      <p id="DOCS-1932"><b>Horizon throws an error when Nova compute is not present</b></p>
      <p>If using the Entry-scale with Swift example, Horizon displays the error "Invalid Service
        Catalog service: compute."</p>
      <p>To avoid this error, you may alter the default panel (this changes the default for both
        admins and normal users to the object storage page and doesn't raise an error, although the
        problematic pages are still available to be chosen).</p>
      <p>To alter the default, on each controller node perform the following steps:</p>
      <ol>
        <li>Edit the file below:
          <codeblock>/opt/stack/service/horizon/etc/local_settings.py</codeblock></li>
        <li>Below line 37, add the following line:
          <codeblock>HORIZON_CONFIG['user_home'] = "/project/containers"</codeblock></li>
        <li>Then run the following command to restart Apache to have Horizon pick up the change
          (again, on each controller): <codeblock>sudo service apache2 reload</codeblock>
        </li>
      </ol>
      <p>These steps will need to be performed after reinstall or upgrade.</p>
 
      
      <!-- HEAT -->
      <!-- https://jira.hpcloud.net/browse/DOCS-1765 -->
      <p id="DOCS-1765"><b>Heat autoscaling not supported</b></p>
      <p>In <keyword keyref="kw-hos-phrase"/>, the auto-scaling in Heat is not supported because the
        alarm creation in Ceilometer is disabled.</p>
      
      <!-- BAREMETAL -->
      <p><b>Restarting lifecycle manager leaves Apache2 down</b></p>
      <p>If you restart your lifecycle management node, note that Apache2 will not restart on its
        own. You will need to restart it manually.</p>
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1881 -->
      <p id="DOCS-1881"><b>Moonshot remote console goes blank after operating system install</b></p>
      <p>If you are using Moonshot compute nodes you may find that their remote console is blank or
        garbled after the OS install. If affected, you need to make the following change to
        /etc/default/grub on each affected node, run update-grub and reboot:</p><ol
        id="ol_p1c_d45_tt">
        <li>Find the line starting with GRUB_CMDLINE_LINUX= </li>
        <li>Add the following at the end of the value i.e. inside the double quotes:
          <codeblock>vga=788 console=ttyS0,9600 earlyprintk=serial,ttyS0,9600</codeblock>
        </li>
      </ol>
      
      <!--https://jira.hpcloud.net/browse/DOCS-1902-->
      <p id="DOCS-1902"><b>Softlockup is observed when attempting to remove many LUNs simultaneously</b></p>
      
      <p>If many multipath LUNs are detached from a compute host in rapid succession a softlockup
        and RCU (read-copy-update) stall may be observed in the kernel log file as follows:</p>
      
      <codeblock>Oct  7 12:48:52 hlm kernel: [31164.262063]  rport-0:0-2: blocked FC remote port time out: removing target and saving binding
Oct  7 12:48:52 hlm kernel: [31164.262859]  rport-0:0-0: blocked FC remote port time out: removing target and saving binding
Oct  7 12:49:09 hlm kernel: [31181.132369] BUG: soft lockup - CPU#23 stuck for 22s! [systemd-udevd:554]
Oct  7 12:49:09 hlm kernel: [31181.133060] Modules linked in: dm_service_time xt_mac xt_physdev xt_set iptable_mangle xt_comment 
     iptable_nat nf_nat_ipv4 nf_nat iptable_raw ip_set_hash_net ip_set nfnetlink vhost_net vhost macvtap macvlan tun veth bridge 
     ib_iser rdma_cm iw_cm ib_cm ib_sa ib_mad ib_core ib_addr iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi md_mod 
     fuse binfmt_misc xt_tcpudp xt_multiport xt_LOG xt_limit xt_conntrack xt_pkttype ip6table_filter ip6_tables iptable_filter 
     ip_tables x_tables 8021q garp stp mrp llc bonding openvswitch gre libcrc32c nf_conntrack_ipv6 nf_defrag_ipv6 nf_conntrack_ipv4 
     nf_defrag_ipv4 nf_conntrack dm_multipath scsi_dh evdev iTCO_wdt iTCO_vendor_support x86_pkg_temp_thermal intel_powerclamp 
     coretemp kvm_intel kvm crc32_pclmul crc32c_intel ghash_clmulni_intel aesni_intel psmouse aes_x86_64 lrw gf128mul glue_helper 
     ablk_helper cryptd pcspkr serio_raw ioatdma mgag200 dca ttm ipmi_si drm_kms_helper ipmi_msghandler hpilo drm lpc_ich wmi mfd_core 
     agpgart i2c_algo_bit syscopyarea sysfillrect sysimgblt processor i2c_core thermal_sys acpi_power_meter hpwdt button autofs4 ext4 
     crc16 mbcache jbd2 dm_mod sd_mod sg lpfc qla2xxx crc_t10dif uhci_hcd ehci_pci crct10dif_generic xhci_hcd ehci_hcd scsi_transport_fc 
     hpsa usbcore usb_common scsi_mod be2net crct10dif_common
Oct  7 12:49:09 hlm kernel: [31181.133114] CPU: 23 PID: 554 Comm: systemd-udevd Not tainted 3.14.51-1-amd64-hlinux #hlinux1
Oct  7 12:49:09 hlm kernel: [31181.133115] Hardware name: HPE ProLiant BL460c Gen9, BIOS I36 05/06/2015
Oct  7 12:49:09 hlm kernel: [31181.133117] task: ffff883fcf0262c0 ti: ffff883f979ca000 task.ti: ffff883f979ca000
Oct  7 12:49:09 hlm kernel: [31181.133118] RIP: 0010:[&lt;ffffffff81514f5b>]  [&lt;ffffffff81514f5b>] _raw_spin_trylock+0xb/0x40</codeblock>
      
      <p>The associated stack from the softlockup will show the instruction point either holding or
        attempting to acquire a spinlock. After all the LUNs have successfully detached and the
        devices have been flushed, the messages will subside. It is recommended that you include a
        delay when detaching multiple LUNs to avoid the softlockup and RCU stall messages.</p>
      
      
      
      
      <!-- NETWORKING -->
      
      <!-- https://jira.hpcloud.net/browse/DOCS-1835 -->
      <p id="DOCS-1835"><b>Blocking IPv6 Traffic</b></p>
      <p>The <keyword keyref="kw-hos-phrase"/> release includes firewall rules to prevent
        non-essential traffic from accessing the physical systems. However, these rules are limited
        to IPv4 traffic only. In order to prevent unsolicited or nefarious IPv6 traffic, you will
        need to block IPv6 at your router, or within the infrastructure.</p><p> You can disable IPv6
        on your operating systems by adding the following lines to <b>/etc/sysctl.conf</b>:
        <codeblock>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</codeblock>
        Once these have been added, you can either reboot the node or run the following command (as
        root): <codeblock># sysctl -p</codeblock> Please refer to your router documentation to block
        IPv6 traffic from reaching your network at the router.</p>
      
      <!-- MESSAGING -->
      <p><b>Error: Failed to consume message from queue</b></p>
      <p>If RabbitMQ or QPID is chosen as the messaging service backend, OpenStack services might
        display an error-level log message when an application exits, stating: “Failed to consume
        message from queue.” Since this log mesage is benign and since the fix depends on other
        changes including new features, any attempt to fix the issue will cause instability. For
        these reasons, the OpenStack community has decided not to backport the fix from
        stable/liberty into stable/kilo release.</p>
      <p>The solution is to ignore the following error message in the log for both Nova and
        Ceilometer:</p>
      <codeblock>(oslo_messaging._drivers.impl_rabbit): 2015-08-20 01:18:21,280 ERROR impl_rabbit _error_callback Failed 
      to consume message from queue:(amqp): 2015-08-20 01:18:21,282 DEBUG channel _do_close Closed channel #1</codeblock>
    </section>
  </body>
</topic>
