<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd" >
<topic id="alarmdefinitions">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Suggested Service Alarm Resolutions</title>
  <shortdesc>This topic contains a list of service-specific alarms and the recommended
    troubleshooting steps.</shortdesc>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>When alarms are triggered it may be helpful for you to review the service logs. For details
      on how to use the logging user interface built into the Operations Console, see <xref
        href="centralized_logging.dita#centralized_logging/interface"/>.</p>
    <section id="services"><title>Alarms by Service</title>
      <p>We have organized these alarm by the dashboard card they are located under in the Ops
        Console and by the service dimension specified in the alarm.</p>
    </section>



    <!-- COMPUTE SECTION INCLUDES COMPUTE/GLANCE -->
    <section id="compute">
      <title>Compute</title>
      <table frame="all" rowsep="1" colsep="1" id="compute_alarms">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry morerows="2">compute</entry>
              <entry>Process Check</entry>
              <entry>Separate alarms for each of these Nova services, specified by the
                  <codeph>component</codeph> dimension: <ul>
                  <li>nova-api</li>
                  <li>nova-cert</li>
                  <li>nova-compute</li>
                  <li>nova-consoleauth</li>
                  <li>nova-conductor</li>
                  <li>nova-scheduler</li>
                  <li>nova-novncproxy</li>
                </ul></entry>
              <entry>Process specified by the <codeph>component</codeph> dimension has crashed on
                the host specified by the <codeph>hostname</codeph> dimension.</entry>
              <entry>Restart the process on the affected node using these steps: <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Use the Nova start playbook against the affected node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname></codeblock></li>
                </ol>
                <p>Review the associated logs. The logs will be in the format of
                    <codeph>&lt;service>.log</codeph>, such as <codeph>nova-compute.log</codeph> or
                    <codeph>nova-scheduler.log</codeph>.</p></entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>This is a <codeph>nova-api</codeph> health check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the <codeph>nova-api</codeph> process on the affected node. Review the
                  <codeph>nova-api.log</codeph> files. Try to connect locally to the http port that
                is found in the dimension field of the alarm to see if the connection is
                accepted.</entry>
            </row>
            <row>
              <entry>Process Bound Check</entry>
              <entry><codeph>process_name=nova-api</codeph><p>This alarm checks that the number of
                  processes found is in a predefined range</p></entry>
              <entry>Process crashed or too many processes running</entry>
              <entry>Stop all the processes and restart the nova-api process on the affected host.
                Review the system and nova-api logs.</entry>
            </row>
            <row>
              <entry>image-service</entry>
              <entry>HTTP Status</entry>
              <entry>Separate alarms for each of these Glance services, specified by the
                  <codeph>component</codeph> dimension: <ul>
                  <li>glance-api</li>
                  <li>glance-registry</li>
                </ul></entry>
              <entry>API is unresponsive.</entry>
              <entry>Restart the process on the affected node using these steps: <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Use the Glance start playbook against the affected node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-start.yml --limit &lt;hostname></codeblock></li>
                </ol>
                <p>Review the associated logs.</p></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>



    <!-- STORAGE SECTION INCLUDES OBJECT AND BLOCK STORAGE -->
    <section id="storage">
      <title>Storage</title>
      <table frame="all" rowsep="1" colsep="1" id="storage_alarms">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="1"/>
          <colspec colname="c3" colnum="2"/>
          <colspec colname="c4" colnum="3"/>
          <colspec colname="c5" colnum="4"/>
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry morerows="18">object-storage</entry>
              <entry>swiftlm-scan monitor</entry>
              <entry>Alarms if <codeph>swiftlm-scan</codeph> cannot execute a monitoring
                task.</entry>
              <entry>The <codeph>swiftlm-scan</codeph> program is used to monitor and measure a
                number of metrics. If it is unable to monitor or measure something, it raises this
                alarm.</entry>
              <entry>Click on the alarm to examine the <codeph>Details</codeph> field and look for a
                  <codeph>msg</codeph> field. The text may explain the error problem. To
                view/confirm this, you can also log into the host specified by the
                  <codeph>hostname</codeph> dimension, and then run this command: <codeblock>sudo swiftlm-scan | python -mjson.tool</codeblock>
                <p>The <codeph>msg</codeph> field is contained in the <codeph>value_meta</codeph>
                  item.</p></entry>
            </row>
            <row>
              <entry>Swift account replicator last completed in 12 hours</entry>
              <entry>Alarms if an <codeph>account-replicator</codeph> process did not complete a
                replication cycle within the last 12 hours.</entry>
              <entry>This can indicate that the <codeph>account-replication</codeph> process is
                stuck.</entry>
              <entry>
                <p>Restart the process on the affected host with this
                  command:<codeblock>sudo systemctl restart swift-account-replicator</codeblock></p>
                <p>Another cause of this problem is that a file system may be corrupt. Look for sign
                  of this in the logs located in <codeph>/var/log/swift/swift.log</codeph> and
                    <codeph>/var/log/kern.log</codeph>. The file system may need to be wiped and
                  reformatted (the <codeph>swift-deploy.yml</codeph> playbook will reformat a wiped
                  file system)</p>
              </entry>
            </row>
            <row>
              <entry>Swift container replicator last completed in 12 hours</entry>
              <entry>Alarms if a container-replicator process did not complete a replication cycle
                within the last 12 hours</entry>
              <entry>This can indicate that the container-replication process is stuck.</entry>
              <entry>
                <p>Restart the process with
                  <codeblock>sudo systemctl restart swift-container-replicator</codeblock></p>
                <p>Another cause of this problem is that a file system may be corrupt. Look for sign
                  of this in the <codeph>/var/log/swift/swift.log</codeph> and
                    <codeph>/var/log/kern.log</codeph>. The files ystem may need to be wiped and
                  reformatted (the <codeph>swift-deploy.yml</codeph> playbook will reformat a wiped
                  file system)</p>
              </entry>
            </row>
            <row>
              <entry>Swift object replicator last completed in 24 hours</entry>
              <entry>Alarms if an object-replicator process did not complete a replication cycle
                within the last 24 hours</entry>
              <entry>This can indicate that the object-replication process is stuck.</entry>
              <entry>
                <p>Restart the process with
                  <codeblock>sudo systemctl restart swift-account-replicator</codeblock></p>
                <p>Another cause of this problem is that a file system may be corrupt. Look for sign
                  of this in the <codeph>/var/log/swift/swift.log</codeph> and
                    <codeph>/var/log/kern.log</codeph>. The file system may need to be wiped and
                  reformatted (the <codeph>swift-deploy.yml</codeph> playbook will reformat a wiped
                  file system)</p>
              </entry>
            </row>
            <row>
              <entry>Swift file ownership</entry>
              <entry>Alarms if files/directories in <codeph>/srv/node/</codeph> or
                  <codeph>/etc/swift</codeph> are not owned by Swift.</entry>
              <entry>
                <p>For files in <codeph>/etc/swift</codeph>, somebody may have manually edited or
                  created a file.</p>
                <p>For directories in <codeph>/srv/node/*</codeph>, it may happen that the root
                  partition was reimaged or reinstalled and the UID assigned to the Swift user
                  changes. The directories and files are then not owned by the UID assigned to the
                  Swift user.</p>
              </entry>
              <entry>
                <p>For files in <codeph>/etc/swift</codeph>, use this command to change the file
                  ownership:<codeblock>sudo chown swift.swift /etc/swift/, /etc/swift/*</codeblock></p>
                <p>For directories and files in <codeph>/srv/node/*</codeph>, compare the swift UID
                  of this system and other systems and the UID of the owner of
                    <codeph>/srv/node/*</codeph>. If possible, make the UID of the Swift user match
                  the directories/files. Otherwise, change the ownership of all files and
                  directories under the <codeph>/srv/node</codeph> path using a similar command as
                  above.</p>
              </entry>
            </row>
            <row>
              <entry>Drive URE errors</entry>
              <entry>Alarms if <codeph>swift-drive-audit</codeph> reports an unrecoverable read
                error on a drive used by the Swift service.</entry>
              <entry>An unrecoverable read error has occurred when Swift attempted to access a
                directory.</entry>
              <entry>
                <p>The UREs reported only apply to file system metadata (i.e., directory
                  structures). For UREs in object files, the Swift system automatically deletes the
                  file and replicates a fresh copy from one of the other replicas.</p>
                <p>UREs are a normal feature of large disk drives. It does not mean that the drive
                  has failed. However, if you get regular UREs on a specific drive, then this may
                  indicate that the drive has indeed failed and should be replaced.</p>
                <p>You can use standard XFS repair actions to correct the UREs in the file
                  system.</p>
                <p>If the XFS repair fails, you should wipe the GPT table as follows (where
                  &lt;drive_name> is replaced by the actual drive name):</p>
                <codeblock>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; bs=$((1024*1024)) count=1</codeblock>
                <p>Then, follow the steps below which will reformat the drive, remount it, and
                  restart Swift services on the affected node.</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Swift reconfigure playbook, specifying the affected node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts _swift-configure.yml --limit &#60;hostname></codeblock></li>
                </ol>
                <p>It is safe to reformat drives containing Swift data because Swift maintains other
                  copies of the data (usually, Swift is configured to have three replicas of all
                  data).</p>
              </entry>
            </row>
            <row>
              <entry>Swift service</entry>
              <entry>Alarms if a Swift process, specified by the <codeph>component</codeph> field,
                is not running. </entry>
              <entry>A daemon specified by the <codeph>component</codeph> dimension on the host
                specified by the <codeph>hostname</codeph> dimension has stopped running.</entry>
              <entry>
                <p>Examine the <codeph>/var/log/swift/swift.log</codeph> file for possible error
                  messages related the Swift process. The process in question is listed in the alarm
                  dimensions in the <codeph>component</codeph> dimension.</p>
                <p>Restart Swift processes by running the <codeph>swift-start.yml</codeph> playbook,
                  with these steps:</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Swift start playbook against the affected host:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &#60;hostname></codeblock></li>
                </ol>
              </entry>
            </row>
            <row>
              <entry>Swift filesystem mount status</entry>
              <entry>Alarms if a file system/drive used by Swift is not correctly mounted.</entry>
              <entry>
                <p>The device specified by the <codeph>device</codeph> dimension is not correctly
                  mounted at the mountpoint specified by the <codeph>mount</codeph> dimension.</p>
                <p>The most probable cause is that the drive has failed or that it had a temporary
                  failure during the boot process and remained unmounted.</p>
                <p>Other possible causes are a file system corruption that prevents the device from
                  being mounted.</p>
              </entry>
              <entry>
                <p>Reboot the node and see if the file system remains unmounted.</p>
                <p>If the file system is corrupt, see the process used for the "Drive URE errors"
                  alarm to wipe and reformat the drive.</p>
              </entry>
            </row>
            <row>
              <entry>Swift rings checksum</entry>
              <entry>Alarms if the swift rings checksums do not match on all hosts.</entry>
              <entry>
                <p>The Swift ring files must be the same on every node. The files are located in
                    <codeph>/etc/swift/*.ring.gz</codeph></p>
                <p>If you have just changed any of the rings and you are still deploying the change,
                  it is normal for this alarm to trigger.</p>
              </entry>
              <entry>
                <p>Use <codeph>sudo swift-recon --md5</codeph> to find which node has outdated
                  rings.</p>
                <p> Run the <codeph>swift-configure.yml</codeph> playbook, using the steps below.
                  This should deploy the same set of rings to every node.</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Swift start playbook against the affected host:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-configure.yml</codeblock></li>
                </ol>
              </entry>
            </row>
            <row>
              <entry>Swift host socket connect</entry>
              <entry>Alarms if a socket cannot be opened to the Identity service (Keystone) token
                validation API.</entry>
              <entry>The Identity service (Keystone) server may be down. Another possible cause is
                that the network between the host reporting the problem and the Keystone server or
                the <codeph>haproxy</codeph> process is not forwarding requests to Keystone.</entry>
              <entry>The <codeph>URL</codeph> dimension contains the name of the virtual IP address.
                Use cURL or a similar program to confirm that a connection can or cannot be made to
                the virtual IP address. Check that <codeph>haproxy</codeph> is running. Check that
                the Keystone service is working.</entry>
            </row>
            <row>
              <entry>Swift memcached connect</entry>
              <entry>Alarms if a socket cannot be opened to the specified target memcached
                server.</entry>
              <entry>The server may be down. The memcached deamon running the server may have
                stopped.</entry>
              <entry>
                <p>If the server is down, restart it.</p>
                <p>If memcached has stopped, you can restart it by using the
                    <codeph>memcached-start.yml</codeph> playbook, using the steps below. If this
                  fails, rebooting the node will restart the process.</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the memcached start playbook against the affected host:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts memcached-start.yml --limit &lt;hostname></codeblock></li>
                </ol>
                <p>If the server is running and memcached is running, there may be a network problem
                  blocking port 11211.</p>
                <p>If you see sporadic alarms on different servers, the system may be running out of
                  resources. Contact HPE Support for advice.</p>
              </entry>
            </row>
            <row>
              <entry>Swift individual disk usage exceeds 80%</entry>
              <entry>Alarms when a disk drive used by Swift exceeds 80% utilization.</entry>
              <entry>Generally all disk drives will fill roughly at the same rate. If an individual
                disk drive becomes filled faster than other drives it can indicate a problem with
                the replication process.</entry>
              <entry>
                <p>If many/most of your disk drives are 80% full you need to add more nodes to your
                  system or delete existing objects.</p>
                <p>If one disk drive is noticeably (more than 30%) more utilized than the average of
                  other disk drives, you should check that Swift processes are working on the server
                  (use the steps below) and also look for alarms related to the host. Otherwise
                  continue to monitor the situation.</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Swift status:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
                </ol>
              </entry>
            </row>
            <row>
              <entry>Swift individual disk usage exceeds 90%</entry>
              <entry>Alarms when a disk drive used by Swift exceeds 90% utilization.</entry>
              <entry>Generally all disk drives will fill roughly at the same rate. If an individual
                disk drive becomes filled faster than other drives it can indicate a problem with
                the replication process.</entry>
              <entry>If one disk drive is noticeably (more than 30%) more utilized than the average
                of other disk drives, you should check that Swift processes are working on the
                server, using these steps: <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Swift status:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
                </ol>
                <p>Also look for alarms related to the host. An individual disk drive filling can
                  indicate a problem with the replication process.</p>
                <p>Restart Swift on that host using the <codeph>--limit</codeph> argument to target
                  the host:</p>
                <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Stop the Swift service:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname></codeblock></li>
                  <li>Start the Swift service back up:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname></codeblock></li>
                </ol>
                <p>If the utilization does not return to similar values as other disk drives, you
                  can reformat the disk drive. You should only do this if the average utilization of
                  all disk drives is less than 80%. To format a disk drive contact HPE Support for
                  instructions.</p></entry>
            </row>
            <row>
              <entry>Swift total disk usage exceeds 80%</entry>
              <entry>Alarms when the average disk utilization of Swift disk drives exceeds 80%
                utilization.</entry>
              <entry>The number and size of objects in your system is beginning to fill the
                available disk space. Account and container storage is included in disk utilization.
                However, this generally consumes 1-2% of space compared to objects, so object
                storage is the dominate consumer of disk space.</entry>
              <entry>
                <p>You need to add more nodes to your system or delete existing objects to remain
                  under 80% utilization.</p>
                <p>Note: if you delete a project/account, the objects in that account are not
                  removed until a week later (so this is not a good way of quickly freeing up
                  space)</p>
              </entry>
            </row>
            <row>
              <entry>Swift total disk usage exceeds 90%</entry>
              <entry>Alarms when the average disk utilization of Swift disk drives exceeds 90%
                utilization.</entry>
              <entry>The number and size of objects in your system is beginning to fill the
                available disk space. Account and container storage is included in disk utilization.
                However, this generally consumes 1-2% of space compared to objects, so object
                storage is the dominate consumer of disk space.</entry>
              <entry>
                <p>If your disk drives are 90% full you must immediately stop all applications that
                  put new objects into the system. At that point you can either delete objects or
                  add more servers.</p>
                <p>You should also set the <codeph>fallocate_reserve</codeph> value (in the
                  account/container/object configuration files) to a value higher than the currently
                  available space on disk drives. This will prevent more objects being created.</p>
                <p>If you allow your file systems to become full, you will be unable to delete
                  objects or add more nodes to the system. This is because the system needs some
                  free space to handle the replication process when adding nodes. With no free
                  space, the replication process cannot work.</p>
              </entry>
            </row>
            <row>
              <entry>Swift service per-minute availability</entry>
              <entry>Alarms if the swift service reports unavailable for the previous
                minute.</entry>
              <entry>The <codeph>swiftlm-uptime-monitor</codeph> service runs on the first proxy
                server. It monitors the Swift endpoint and reports latency data. If the endpoint
                stops reporting, it generates this alarm.</entry>
              <entry>
                <p>There are many reasons why the endpoint may stop running. Check:</p>
                <ul id="ul_ghg_4hc_4v">
                  <li>Is <codeph>haproxy</codeph> running on the control nodes?</li>
                  <li>Is <codeph>swift-proxy-server</codeph> running on the Swift proxy
                    servers?</li>
                </ul>
              </entry>
            </row>
            <row>
              <entry>Swift rsync connect</entry>
              <entry>Alarms if a socket cannot be opened to the specified rsync server</entry>
              <entry>The rsync deamon on the specified node cannot be contacted. The most probable
                cause is that the node is down. The rsync service might also have been stopped on
                the node.</entry>
              <entry>
                <p>Reboot the server if it is down.</p>
                <p>Attempt to restart rsync with this
                  command:<codeblock>systemctl restart rsync.service</codeblock></p>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Alarms when the specified process is not running.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift Service" alarm for possible causes.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-storage</codeph>,
                see the description of the "Swift Service" alarm for possible mitigation
                tasks.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>, see
                the description of the "Swift host socket connect" alarm for possible
                causes.</entry>
              <entry>If the <codeph>service</codeph> dimension is <codeph>object-storage</codeph>,
                see the description of the "Swift host socket connect" alarm for possible mitigation
                tasks.</entry>
            </row>
            <row>
              <entry morerows="6">block-storage</entry>
              <entry>Process Check</entry>
              <entry>Separate alarms for each of these Cinder services, specified by the
                  <codeph>component</codeph> dimension: <ul>
                  <li>cinder-api</li>
                  <li>cinder-backup</li>
                  <li>cinder-scheduler</li>
                  <li>cinder-volume</li>
                </ul></entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=cinder-backup</entry>
              <entry>Process crashed.</entry>
              <entry>Alert may be incorrect if the service has migrated. Validate that the service
                is intended to be running on this node before restarting the service. Review the
                associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=cinder-scheduler</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=cinder-volume</entry>
              <entry>Process crashed.</entry>
              <entry>Alert may be incorrect if the service has migrated. Validate that the service
                is intended to be running on this node before restarting the service. Review the
                associated logs.</entry>
            </row>
            <row>
              <entry>Cinder backup running &lt;hostname&gt; check</entry>
              <entry>Cinder backup singleton check.</entry>
              <entry>
                <p>Backup process is either:</p>
                <p>
                  <ul>
                    <li>running on a node it should not be on, or</li>
                    <li>not running on a node it should be on</li>
                  </ul>
                </p>
              </entry>
              <entry>Run the cinder-migrate-volume playbook to migrate the volume and backup to the
                correct node.</entry>
            </row>
            <row>
              <entry>Cinder volume running &lt;hostname&gt; check</entry>
              <entry>Cinder volume singleton check.</entry>
              <entry>
                <p>Volume process is either:</p>
                <p>
                  <ul>
                    <li>running on a node it should not be on, or</li>
                    <li>not running on a node it should be on</li>
                  </ul>
                </p>
              </entry>
              <entry>Run the cinder-migrate-volume playbook to migrate the volume and backup to
                correct node.</entry>
            </row>
            <row>
              <entry>Cinderlm diagnostics monitoring</entry>
              <entry>Alarms if Cinder monitoring cannot execute a task</entry>
              <entry>Cinder monitoring was unable to execute the cinder service check to determine
                the number of executing Cinder processes</entry>
              <entry>Review the monasca agent collector.log file on the affected node to determine
                why the service check did not execute as expected. </entry>
            </row>
            <row>
              <entry morerows="1">vsa</entry>
              <entry>VSA VM Status</entry>
              <entry>Raises alarm when vsa_vm_status is non zero (Not running)</entry>
              <entry>Node reboot or VM in paused state due to disk errors.</entry>
              <entry>Investigate the VSA VM status in the node.</entry>
            </row>
            <row>
              <entry>VSA VM Network Status</entry>
              <entry>Raises alarm when vsa_vm_net_status is non zero (Not running)</entry>
              <entry>Network/Bridge issues</entry>
              <entry>Investigate the VSA VM network status in the node.</entry>
            </row>
            <row>
              <entry morerows="1">ceph</entry>
              <entry>Process Check</entry>
              <entry>process_name=&lt;cluster_name>-osd.&lt;id></entry>
              <entry>Process crashed.</entry>
              <entry>Run the ceph-start.yml playbook to start the services (manual)</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=&lt;cluster_name>-mon.&lt;id></entry>
              <entry>Process crashed.</entry>
              <entry>Run the ceph-start.yml playbook to start the services (manual)</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>



    <section id="networking">
      <title>Networking</title>
      <table frame="all" rowsep="1" colsep="1" id="networking_alarms">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry morerows="10">networking</entry>
              <entry>Process Check</entry>
              <entry>neutron-openvswitch-agent pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-l3-agent pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-dhcp-agent pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-metadata-agent pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-rootwrap pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Currently neutron-rootwrap is only used to run ovsdb-client. To restart this,
                restart neutron-openvswitch-agent on the affected node. Review the associated
                logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaas-agent</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaasv2-agent</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-vpn-agent</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-server pid check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>Process stuck if neutron-server Process Check OK</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>Node crashed or connectivity lost if local node HTTP Status OK or
                UNKNOWN</entry>
              <entry>Reboot the node or repair the network. Review the associated logs.</entry>
            </row>

            <row>
              <entry morerows="4">dns</entry>
              <entry>Process Check</entry>
              <entry>designate-zone-manager</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>designate-pool-manager</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>designate-api</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>designate-mdns</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>designate-api</entry>
              <entry>...</entry>
              <entry>...</entry>
            </row>

            <row>
              <entry>powerdns</entry>
              <entry>Process Check</entry>
              <entry>pdns_server</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>




    <section id="identity"><title>Identity</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="identity_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="4">identity-service</entry>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,api_endpoint=public</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,api_endpoint=admin</entry>
                <entry>API is unresponsive.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=keystone-api,monitored_host_type=vip</entry>
                <entry>API is unresponsive on each node.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-main</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-admin</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>





    <section id="metering"><title>Telemetry</title>
      <p><table frame="all" rowsep="1" colsep="1" id="telemetry_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="4">telemetry</entry>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-notification</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-collector</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-central</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>
                  <p>service=telemetry and used with hostname of one of the controller </p>
                </entry>
                <entry>Ceilometer API.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>component=ceilometer-api </entry>
                <entry>Ceilometer API on administrator VIP.</entry>
                <entry><p>If this occurs with an http_status in error on all nodes, then restart
                    Apache on all controllers.</p>
                  <p>If this occurs with a specific host with http_status in non-error for
                    telemetry, then it should be a haproxy issue and it needs to be restarted.</p>
                  <p>For further troubleshooting, look into the Ceilometer access log in the
                    Ceilometer log directory for the ceilometer_modwsgi file and ceilometer-api
                    logs.</p></entry>
              </row>

              <row>
                <entry morerows="12">logging</entry>
                <entry>Elasticsearch Unassigned Shards</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch unassigned shards count is
                  greater than 0.</entry>
                <entry>Environment could be misconfigured.</entry>
                <entry>
                  <p>To find the unassigned shards, run the following command on the lifecycle
                    manager from the <codeph>~/scratch/ansible/next/hos/ansible</codeph>
                    directory:</p>
                  <codeblock>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</codeblock>
                  <p>This should show which shards are unassigned, like this:</p>
                  <codeblock>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</codeblock>
                  <p>The last column shows the name that Elasticsearch uses for the node that the
                    unassigned shards are on. To find the actual hostname, run:</p>
                  <codeblock>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</codeblock>
                  <p>Once you find the hostname, you can try the following:</p>
                  <ol>
                    <li>Make sure the node is not out of disk space, and free up space if
                      needed.</li>
                    <li>Restart the node (use caution, as this may affect other services as
                      well).</li>
                    <li>Check to make sure all versions of Elasticsearch are the same with this:
                      <codeblock>ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</codeblock></li>
                    <li>Contact customer support.</li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Elasticsearch Max Total Indices Size</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Total Indices size is greater
                  than elasticsearch_max_total_indices_size_in_bytes defined by user.</entry>
                <entry>elasticsearch_max_total_indices_size_in_bytes may be set too low, or you may
                  have insufficient resources.</entry>
                <entry>If the total size of all the indices exceeds the size of the
                  elasticsearch_max_total_indices_size_in_bytes as you have defined it, you can
                  adjust the value if you want to give more space and you have the resources. If
                  not, you may need to adjust the Curator to remove indices sooner.</entry>
              </row>
              <row>
                <entry>Elasticsearch Number of Log Entries</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Number of Log Entries.</entry>
                <entry>The number of log entries may get too large.</entry>
                <entry>Older versions of Kibana (version 3 and earlier) may hang if the number of
                  log entries is too large (e.g. above 40,000), and the page size would need to be
                  small enough (about 20,000 results), because if it is larger (e.g. 200,000), it
                  may hang the browser, but Kibana 4 should not have this issue.</entry>
              </row>
              <row>
                <entry>Elasticsearch Field Data Evictions</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Field Data Evictions count is
                  greater than 0.</entry>
                <entry>Field Data Evictions may be found even though it is nowhere near the limit
                  set.</entry>
                <entry>The elasticsearch_indices_fielddata_cache_size is tuned out-of-the box, but
                  if it is insufficient, you may need to increase this configuration parameter and
                  run a reconfigure.</entry>
              </row>
              <row>
                <entry>Elasticsearch Low Watermark</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Disk LOW Watermark. Backup
                  indices. If high watermark is reached, indices will be deleted. Adjust
                  curator_low_watermark_percent, curator_high_watermark_percent, and
                  elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
                <entry>Running out of disk space for
                  <codeph>/var/lib/elasticsearch</codeph>.</entry>
                <entry>Free up space by removing indices (backing them up first if desired).
                  Alternatively, adjust curator_low_watermark_percent,
                  curator_high_watermark_percent, and/or
                  elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
              </row>
              <row>
                <entry>Elasticsearch High Watermark</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Disk HIGH Watermark. Attempting
                  to delete indices to free disk space. Adjust curator_low_watermark_percent,
                  curator_high_watermark_percent, and elasticsearch_max_total_indices_size_in_bytes
                  if needed.</entry>
                <entry>Running out of disk space for
                  <codeph>/var/lib/elasticsearch</codeph>.</entry>
                <entry>Verify that disk space was freed up by the curator. If needed, free up
                  additional space by removing indices (backing them up first if desired).
                  Alternatively, adjust curator_low_watermark_percent,
                  curator_high_watermark_percent, and/or
                  elasticsearch_max_total_indices_size_in_bytes if needed.</entry>
              </row>
              <row>
                <entry>Elasticsearch Bulk Pool Rejected</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Bulk Pool Rejected count is
                  greater than 0.</entry>
                <entry>Thread pool size could be too small.</entry>
                <entry>You may need to increase the threadpool.bulk.queue_size (from 50 to 100, or
                  even 3000, for example). This is found in the config/logging/elasticsearch.yml.j2
                  file.</entry>
              </row>
              <row>
                <entry>Elasticsearch Open File Descriptors</entry>
                <entry><p>component = elasticsearch</p>Elasticsearch Open File Descriptors is
                  greater than 90%.</entry>
                <entry>Configured to allow too many file descriptors to be opened.</entry>
                <entry>Close some old indices. This can be performed by setting the
                  curator_close_indices_after_days value from the default of 0, which does not close
                  indices, to a specific number of days. This is located in the
                  config/logging-common/main.yml file.</entry>
              </row>
              <row>
                <entry>Kafka Kronos Consumer Lag</entry>
                <entry>component = kafka<p>Alarms when the Kronos consumer group is not keeping up
                    with the incoming messages on the metric topic.</p></entry>
                <entry>There is a slow down in the system or heavy load.</entry>
                <entry>Look for high load in the various systems. This alert can fire for multiple
                  topics or on multiple hosts. Which alarms are firing can help diagnose likely
                  causes, ie if all on one host it could be the host. If one topic across multiple
                  hosts it is likely the consumers of that topic, etc.</entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>
                  <p>Service log directory consuming more disk than its quota.</p>
                </entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <row>
                <entry>Log Partition Low Watermark</entry>
                <entry>The <codeph>/var/log</codeph> disk space usage has crossed the low watermark.
                  If the high watermark is reached, <codeph>logrotate</codeph> will be run to free
                  up disk space. Adjust <codeph>var_log_low_watermark_percent</codeph> if
                  needed.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <row>
                <entry>Log Partition High Watermark</entry>
                <entry>The <codeph>/var/log</codeph> volume is running low on disk space. Logrotate
                  will be run now to free up space. Adjust
                    <codeph>var_log_high_watermark_percent</codeph> if needed.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Separate alarms for each of these logging services, specified by the
                    <codeph>process_name</codeph> dimension:<ul>
                    <li>elasticsearch</li>
                    <li>logstash</li>
                    <li>beaver</li>
                    <li>apache2</li>
                    <li>kibana</li>
                  </ul></entry>
                <entry>Process has crashed.</entry>
                <entry>On the affected node, attempt to restart the process.<p>If the elasticsearch
                    process has crashed, use:</p><codeblock>sudo systemctl restart elasticsearch</codeblock>
                  <p>If the logstash process has crashed, use:</p>
                  <codeblock>sudo systemctl restart logstash</codeblock>
                  <p>The rest of the processes can be restarted using similar commands, listed
                    here:</p>
                  <codeblock>sudo systemctl restart beaver
sudo systemctl restart apache2
sudo systemctl restart kibana</codeblock>
                </entry>
              </row>
              <!-- MONITORING -->
              <row>
                <entry morerows="9">monitoring</entry>
                <entry>HTTP Status</entry>
                <entry><p>component = persister</p>Persister Health Check</entry>
                <entry>The process has crashed or a dependency is out.</entry>
                <entry>If the process has crashed, restart. If a dependent service is down, address
                  that issue.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry><p>component = api</p>API Health Check</entry>
                <entry>The process has crashed or a dependency is out.</entry>
                <entry>If the process has crashed, restart. If a dependent service is down, address
                  that issue.</entry>
              </row>
              <row>
                <entry>Kafka Consumer Lag</entry>
                <entry><p>component = kafka</p>Consumers are falling behind for a particular Kafka
                  topic.</entry>
                <entry>There is a slow down in the system or heavy load.</entry>
                <entry>Look for high load in the various systems. This alert can fire for multiple
                  topics or on multiple hosts. Which alarms are firing can help diagnose likely
                  causes, i.e., if all are on one machine it could be the machine. If one topic
                  across multiple machines it is likely the consumers of that topic, etc.</entry>
              </row>
              <row>
                <entry>Monasca Agent Collection Time</entry>
                <entry><p>component = monasca-agent</p>The time the agent took to collect
                  metrics.</entry>
                <entry>Heavy load on the box or a stuck agent plug-in.</entry>
                <entry>Address the load issue on the machine. If needed, restart the agent.</entry>
              </row>
              <row>
                <entry>Monasca Agent Emit Time</entry>
                <entry><p>component = monasca-agent</p>The time the agent took to send metrics to
                  the Monasca API.</entry>
                <entry>API or the network is slow.</entry>
                <entry>Check the network graphs and look for indication of network issues. If coming
                  from most boxes, it is possibly API problems.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry><p>component = kafka</p>Process not running.</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry><p>component = notification</p>Process not running.</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry><p>component = storm</p>Process not running.</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry><p>service = zookeeper</p>Process not running.</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>Zookeeper Latency</entry>
                <entry><p>component = zookeeper</p>Zookeeper is experiencing high latency.</entry>
                <entry>Heavy system load.</entry>
                <entry>Check the individual system as well as activity across the entire
                  service.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>




    <!-- CONSOLE SECTION INCLUDES OPS CONSOLE ONLY -->
    <section id="console"><title>Console</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="console_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="4"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="2">ops-console</entry>
                <entry>HTTP Status</entry>
                <entry>service=ops-console</entry>
                <entry>The Operations Console is unresponsive</entry>
                <entry><p>Review logs in <codeph>/var/log/ops-console</codeph> and logs in
                      <codeph>/var/log/apache2</codeph>. Restart ops-console by running the
                    following commands on the lifecycle manager:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=leia-leia_monitor</entry>
                <entry>Process crashed or unresponsive.</entry>
                <entry><p>Review logs in <codeph>/var/log/ops-console</codeph>. Restart ops-console
                    by running the following commands on the lifecycle manager:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ops-console-stop.yml 
ansible-playbook -i hosts/verb_hosts ops-console-start.yml</codeblock></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>

    <!-- OTHER ALARMS -->
    <section id="other">
      <title>Other Alarms</title>
      <table frame="all" rowsep="1" colsep="1" id="table_k33_xcq_bt">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <!-- HEAT -->
            <row>
              <entry morerows="7">orchestration</entry>
              <entry>Process Check</entry>
              <entry>heat-api process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cfn process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cloudwatch process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-engine process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <!-- HORIZON -->
            <row>
              <entry>web-ui</entry>
              <entry>HTTP Status</entry>
              <entry>Alerts when Horizon's login page is not returned.</entry>
              <entry>Apache is not running or there is a misconfiguration.</entry>
              <entry>Check that Apache is running; investigate Horizon logs.</entry>
            </row>
            <!-- MYSQL -->
            <row>
              <entry morerows="1">mysql</entry>
              <entry>MySQL Slow Query Rate</entry>
              <entry>MySQL is reporting many slow queries.</entry>
              <entry>System load.</entry>
              <entry>This could be an indication of near capacity limits or an exposed bad query.
                First, check overall system load and then investigate MySQL details.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>process="mysqld"</entry>
              <entry>MySQL crashed.</entry>
              <entry>Restart MySQL on the affected node.</entry>
            </row>
            <!-- HAPROXY -->
            <row>
              <entry>haproxy</entry>
              <entry>Process Check</entry>
              <entry><p>service=haproxy</p>process_name=haproxy</entry>
              <entry>HA Proxy is not running on this machine.</entry>
              <entry>Restart the process on the affected node using these steps: <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Use this playbook against the affected node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname></codeblock></li>
                </ol>
                <p>Review the associated logs.</p></entry>
            </row>
            <row>
              <entry>rabbitmq</entry>
              <entry>Process Check</entry>
              <entry>rabbitmq-server process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process on the affected node. Review the associated logs.</entry>
            </row>
            <row>
              <entry morerows="1">vertica</entry>
              <entry>Process Check</entry>
              <entry>The vertica process is not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart the process.</entry>
            </row>
            <row>
              <entry>Vertica Status</entry>
              <entry>Alarms if a Vertica node cannot connect to its database.</entry>
              <entry>Process crashed.</entry>
              <entry>Verify that the database node is down and then recover it if so by using these
                steps: <ol>
                  <li>Log in to the lifecycle manager.</li>
                  <li>Run the Monasca status playbook:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</codeblock></li>
                  <li>Run this playbook to do a recovery on Vertica:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</codeblock></li>
                </ol></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>



    <section><title>Alarms by Hostname</title></section>
    <section id="hlinux">
      <title>Uncategorized Alarms</title>
      <p>These alarms show under the Uncategorized section and are setup per hostname, so one for
        each host in your cloud environment.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_ik5_3km_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>CPU Usage</entry>
              <entry>Alarms on high CPU usage.</entry>
              <entry>Heavy load or runaway processes.</entry>
              <entry>Log onto the system and diagnose the heavy CPU usage.</entry>
            </row>
            <row>
              <entry>Crash Dump Count</entry>
              <entry>Alarms if it receives any metrics with <codeph>crash.dump_count</codeph> >
                0</entry>
              <entry>When a crash dump is generated by kdump, the crash dump file is put into the
                  <codeph>/var/crash</codeph> directory by default. Any crash dump files in this
                directory will cause the <codeph>crash.dump_count</codeph> metric to show a value
                greater than 0.</entry>
              <entry>Analyze the crash dump file(s) located in <codeph>/var/crash</codeph> on the
                hostname that generated the alarm to try to determine if a service or hardware
                caused the crash.<p>Move the file to a new location so that a developer can take a
                  look at it. Make sure all of the processes are back up after the crash (run the
                  &lt;service>-status.yml playbooks). When the <codeph>/var/crash</codeph> directory
                  is empty the Crash Dump Count alarm should transition back to OK.</p></entry>
            </row>
            <row>
              <entry>Disk Inode Usage</entry>
              <entry>Nearly out of inodes for a partition.</entry>
              <entry>Many files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Disk Usage</entry>
              <entry>High Disk usage.</entry>
              <entry>Large files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Host Status</entry>
              <entry>Alerts when a host is unreachable.</entry>
              <entry>Host or network is down.</entry>
              <entry>If a single host, attempt to restart the system. If multiple hosts, investigate
                network issues.</entry>
            </row>
            <row>
              <entry>Memory Usage</entry>
              <entry>High memory usage.</entry>
              <entry>Overloaded system or services with memory leaks.</entry>
              <entry>Log onto the system to investigate high memory users.</entry>
            </row>
            <row>
              <entry>Network Errors</entry>
              <entry>Alarms on a high network error rate.</entry>
              <entry>Bad network or cabling.</entry>
              <entry>Take this system out of service until the network can be fixed.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

















  </body>
</topic>
