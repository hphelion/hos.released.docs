<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_tb4_lqy_qt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering the Control Plane</title>
  <abstract><shortdesc outputclass="hdphidden">If one or more of your controller nodes has
      experienced an issue, such as power loss or hardware failure, and you need to perform disaster
      recovery then we provide different scenarios and how to resolve them to get your cloud
      repaired.</shortdesc></abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <section>
      <p>If one or more of your controller nodes has experienced an issue, such as power loss or
        hardware failure, and you need to perform disaster recovery then we provide different
        scenarios and how to resolve them to get your cloud repaired.</p>
    </section>

    <section>
      <ul>
        <li>Point-in-time database recovery</li>
        <li>Point-in-time Swift rings recovery</li>
        <li>Point-in-time lifecycle manager recovery</li>
        <li>One or two controller loss disaster recovery</li>
        <li>Three control plane node loss disaster recovery</li>
        <li>Dedicated lifecycle manager disaster recovery</li>
        <li>Full disaster recovery</li>
        <li>Swift Rings Recovery</li>

      </ul>
      <note>You should have backed up <codeph>/etc/group</codeph> of the deployer manually after
        installation. While recovering a lifecycle manager node, manually copy the
          <codeph>/etc/group</codeph> file from a backup of the old lifecycle manager. </note>
    </section>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All</sectiondiv>
    </section>

    <section id="pointdb">
      <title outputclass="headerH">Point-in-Time Database Recovery</title>
      <sectiondiv outputclass="insideSection"><p>In this scenario, everything is still running
          (lifecycle manager, cloud controller nodes, and compute nodes) but you want to restore the
          MySQL database to a previous state. </p>
        <b>Restore from a Swift backup</b>
        <ol>
          <li>Log in to the first node running the MySQL service, which may be your first controller
            node.</li>
          <li>Become root: <codeblock>sudo su</codeblock></li>
          <li>Source the backup environment file
            <codeblock>source /home/stack/backup.osrc</codeblock></li>
          <li>List the
            jobs<codeblock>freezer-scheduler -c &lt;hostname&gt; job-list</codeblock></li>
          <li>Get the id corresponding to the job "HLM Default: Mysql restore from Swift".</li>
          <li>Launch the restore.
            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock></li>
          <li>This will take some time. While you wait you can follow the progress in the
              <codeph>/var/log/freezer-agent/freezer-scheduler.log</codeph> log file.</li>
          <li>Log in to the lifecycle manager.</li>
          <li>Stop the Percona DB service:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
          <li>Log back in to the first node running the MySQL service.</li>
          <li>Clean the Mysql directory by removing everything there:
            <codeblock>sudo rm -r /var/lib/mysql/*</codeblock></li>
          <li>Copy the restored files back to the mysql
            directory:<codeblock>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</codeblock></li>
          <li>Log back in to the lifecycle manager.</li>
          <li>Start the Percona DB service back up:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock></li>
        </ol><p><b>Restore from an SSH backup</b></p><p>Follow the same procedure as the one for
          Swift but select the job "HLM Default: Mysql restore from SSH".</p><p><b>Restore MySQL
            manually </b></p><p>If restoring MySQL fails during the
            <codeph>percona-bootstrap.yml</codeph> procedure outlined above, you can follow this
          procedure to manually restore MySQL:</p><ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Stop the MySQL cluster:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
          <li>On all of the nodes running the MySQL service, which should be all of your controller
            nodes, run the following command to purge the old
            database:<codeblock>sudo rm -r /var/lib/mysql/*</codeblock></li>
          <li>On the first node running the MySQL service, which is likely your first controller
            node, restore the backup with the command below. If you have already restored to a
            temporary directory, copy the files
            again.<codeblock>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</codeblock></li>
          <li>If you need to restore the files manually from SSH, follow these steps: <ol>
              <li>Become root: <codeblock>sudo su</codeblock></li>
              <li>Create the <codeph>/root/mysql_restore.ini</codeph> file with the contents below.
                Be careful to substitute the <codeph>{{ values }}</codeph>. Note that the SSH
                information refers to the SSH server you configured for backup before installing.
                <codeblock>[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
hostname = {{ hostname of the first MySQL node }}</codeblock></li>
              <li>Execute the restore
                job:<codeblock>freezer-agent --config /root/mysql_restore.ini</codeblock></li>
            </ol></li>
          <li>Also on the first node running the MySQL service, follow the next steps to start the
            cluster. <ol>
              <li>Become root: <codeblock>sudo su</codeblock></li>
              <li>When the last step executed successfully, start the MySQL cluster:
                <codeblock>/etc/init.d/mysql bootstrap-pxc</codeblock></li>
              <li>Start the process with systemctl to make sure the process is monitored by upstard:
                <codeblock>systemctl start mysql</codeblock></li>
              <li>Make sure the mysql process started successfully by getting the status:
                <codeblock>systemctl status mysql</codeblock></li>
            </ol></li>
          <li>Log back in to the lifecycle manager.</li>
          <li>From the lifecycle manager, execute the following playbook to start all MySQL
            instances:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-start.yml</codeblock></li>
          <li>MySQL cluster status can be checked using the <codeph>percona-status.yml
            </codeph>playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock></li>
          <li>On all of the nodes running the MySQL service, run the following commands as
            root:<codeblock>sudo su
touch /var/lib/mysql/galera.initialised
chown mysql:mysql /var/lib/mysql/galera.initialised</codeblock></li>
          <li>After approximately 10-15 minutes, the output of the
              <codeph>percona-status.yml</codeph> playbook should show all the MySQL nodes in sync.
            MySQL cluster status can be checked using this
              playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock><p>An
              example output is as
            follows:</p><codeblock>TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] ************* 
ok: [helion-cp1-c1-m1-mgmt] => {
    "msg": "mysql is synced."
}
ok: [helion-cp1-c1-m2-mgmt] => {
    "msg": "mysql is synced."
}
ok: [helion-cp1-c1-m3-mgmt] => {
    "msg": "mysql is synced."
}</codeblock></li>
        </ol></sectiondiv>
    </section>


    <section id="pointswift">
      <title outputclass="headerH">Point-in-Time Swift Rings Recovery</title>
      <sectiondiv outputclass="insideSection"><p>In this situation, everything is still running
          (lifecycle manager, control plane nodes, and compute nodes) but you want to restore your
          Swift rings to a previous state.</p><note>Freezer backs up and restores Swift rings only,
          not Swift data.</note>
        <b>Restore from a Swift backup</b>
        <ol>
          <li>Log in to the first Swift Proxy (<codeph>SWF-PRX[0]</codeph>) node.</li>
          <li>Become root: <codeblock>sudo su</codeblock></li>
          <li>Source the backup environment file:
            <codeblock>source /home/stack/backup.osrc</codeblock></li>
          <li>List the jobs:
            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-list</codeblock></li>
          <li>Get the id corresponding to the job "HLM Default: Swift restore from Swift."</li>
          <li>Launch the restore
            job:<codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock></li>
          <li>This will take some time. While you wait you can follow the progress in the
              <codeph>/var/log/freezer-agent/freezer-scheduler.log</codeph> log file.</li>
          <li>Log in to the lifecycle manager.</li>
          <li>Stop the Swift service:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml</codeblock></li>
          <li>Log back in to the first Swift Proxy (<codeph>SWF-PRX[0]</codeph>) node.</li>
          <li>Copy restored files
            <codeblock>cp -pr /tmp/swift_builder_dir_restore/* /etc/swiftlm/builder_dir/</codeblock></li>
          <li>Log back in to the lifecycle manager.</li>
          <li>Reconfigure the Swift service:\
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
        </ol><p><b>Restore from an SSH backup</b></p><p>Follow the same procedure as for Swift in
          the section immediately preceding this one, but select the job "HLM Default: Swift restore
          from SSH."</p></sectiondiv>
    </section>



    <section id="pointlifecycle">
      <title outputclass="headerH">Point-in-time Lifecycle Manager Recovery</title>
      <sectiondiv outputclass="insideSection"><p>In this scenario, everything is still running
          (lifecycle manager, controller nodes, and compute nodes) but you want to restore the
          lifecycle manager to a previous state.</p>
        <b>Restore from a Swift backup</b>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Become root: <codeblock>sudo su</codeblock></li>
          <li>Source the backup environment file:
            <codeblock>source /home/stack/backup.osrc</codeblock></li>
          <li>List the jobs:
            <codeblock>freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-list</codeblock></li>
          <li>Get the id corresponding to the job "HLM Default: Deployer restore from swift."</li>
          <li>Stop the Dayzero UI: <codeblock>systemctl stop dayzero</codeblock></li>
          <li>Launch the restore job:
            <codeblock>freezer-scheduler -c &lt;lifecycle manager hostname&gt; job-start -j &lt;job-id&gt;</codeblock></li>
          <li>This will take some time. While you wait you can follow the progress in the
              <codeph>/var/log/freezer-agent/freezer-scheduler.log</codeph> log file.</li>
          <li>Start the Dayzero UI: <codeblock>systemctl start dayzero</codeblock></li>
        </ol><p><b>Restore from an SSH backup</b></p><p>Follow the same procedure as for Swift but
          select the job "HLM Default: Deployer restore from SSH."</p></sectiondiv>
    </section>


    <section outputclass="pageTarget1" id="nolifecycle_target">
      <title outputclass="headerH">Dedicated Lifecycle Manager Disaster Recovery</title>
      <sectiondiv outputclass="insideSection" id="deployer"><p>In this scenario everything is still
          running (controller nodes and compute nodes) but you've lost the dedicated lifecycle
          manager.</p><p>Ensuring that you use the same version of HPE Helion OpenStack that you
          previously had loaded on your lifecycle manager, you will need to download and install the
          lifecycle management software using the instructions from the <xref
            href="../../../installation/installing_kvm.dita#install_kvm/setup_deployer">installation
            guide</xref> before proceeding further.</p><p><b>Restore from a Swift backup</b></p><ol>
          <li>On the lifecycle manager, install the freezer-agent, as
              follows:<codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock><p>You
              must retrieve the following two files <codeph>/home/stack/backup.osrc</codeph> and
                <codeph>/opt/stack/service/freezer-agent/etc/systemd_env_vars.cfg</codeph> from any
              compute or controller node and put them in this directory
                (<codeph>/opt/stack/service/freezer-agent/etc/</codeph>) on the lifecycle
              manager.</p><p>You must then retrieve the /etc/hosts file from any compute or
              controller node and replace the one found on the lifecycle manager with the one
              retrieved. Be sure to edit the 127.0.0.1 line so it points to hlinux like
            so:</p><codeblock>127.0.0.1       localhost
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters
127.0.0.1       hlinux</codeblock></li>
          <li>On the lifecycle manager: <p>Become root:</p><codeblock>sudo su</codeblock> Source the
            credentials: <codeblock>source /home/stack/backup.osrc</codeblock> List the jobs
            <codeblock>freezer-scheduler -c &lt;hostname> job-list</codeblock>Get the id of the job
            corresponding to "HLM Default: Deployer restore from Swift." Stop that job so the
            freezer-scheduler doesn't begin making backups when started.
            <codeblock>freezer-scheduler -c &lt;hostname> job-stop -j &lt;job-id&gt;</codeblock>If
            it is present, also stop the lifecycle manager's SSH backup. <p>Next, stop the dayzero
              UI installer:</p><codeblock>systemctl stop dayzero</codeblock>Start the
            freezer-scheduler: <codeblock>systemctl start freezer-scheduler</codeblock>Get the id of
            the job corresponding to "HLM Default: deployer restore from Swift" and launch that job:
            <codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock>This
            will take some time; you can follow the progress in
            /var/log/freezer-agent/freezer-scheduler.log. <p>Start the dayzero UI
            installer:</p><codeblock>systemctl start dayzero</codeblock>When the lifecycle manager
            is restored, re-run the deployment to ensure the lifecycle manager is in the correct
            state:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
        </ol>
        <b>Restore from an SSH backup</b>
        <ol id="ol_q5k_qyh_1v">
          <li>On the lifecycle manager, edit the following file so it contains the same information
            as it did previously:
            <codeblock>~/helion/my_cloud/config/freezer/ssh_credentials.yml</codeblock></li>
          <li>On the lifecycle manager, copy the following files, change directories, and run
            _deployer_restore_helper.yml:
              <codeblock>cp -r ~/hp-ci/helion/* ~/helion/my_cloud/definition/
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock><p>Perform
              the restore. First become root and change directories:
              </p><codeblock>sudo su
cd /root/deployer_restore_helper/</codeblock><p>Then, stop the
              Dayzero UI installer:</p><codeblock>systemctl stop dayzero</codeblock>Execute the
            restore job: <codeblock>./deployer_restore_script.sh</codeblock>Start the Dayzero UI
            installer:<codeblock>systemctl start dayzero</codeblock>When the lifecycle manager is
            restored, re-run the deployment to ensure the lifecycle manager is in the correct state:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</codeblock></li>
        </ol></sectiondiv>
    </section>

    <section id="shared_lifecyclemgt">
      <title outputclass="headerH">Shared Controller / Lifecycle Manager Disaster Recovery</title>
      <sectiondiv outputclass="insideSection">
        <p>Ensuring that you use the same version of HPE Helion OpenStack that you previously had
          loaded on your lifecycle manager, you will need to download and install the lifecycle
          management software using the instructions from the installation guide. See <xref
            href="../../../installation/installing_kvm.dita#install_kvm/setup_deployer"/>
          for more details.</p>
      </sectiondiv>
    </section>

    <section id="onecontroller">
      <title outputclass="headerH">One or Two Controller Node Disaster Recovery</title>
      <sectiondiv outputclass="insideSection">
        <p>These steps make the following assumptions:</p>
        <ul>
          <li>Your lifecycle manager is still intact and working.</li>
          <li>One or two of your controller nodes went down, but not the entire cluster.</li>
          <li>The node needs to be rebuilt from scratch, not simply rebooted.</li>
        </ul>
        <p><b>Steps to recovering one or two controller nodes</b></p>
        <ol>
          <li>Ensure that your node has power and all of the hardware is functioning.</li>
          <li>Log in to the lifecycle manager.</li>
          <li>Verify that all of the information in your
              <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file is correct for
            your controller node. You may need to replace the existing information if you had to
            either replacement your entire controller node or just pieces of it.</li>
          <li>If you made changes to your <codeph>servers.yml</codeph> file then commit those
            changes to your local git:
            <codeblock>git add -A
git commit -a -m "editing controller information"</codeblock></li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Update your deployment directory:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Ensure that Cobbler has the correct system information: <ol>
              <li>If you replaced your controller node with a completely new machine, you need to
                verify that Cobbler has the correct list of controller nodes:
                <codeblock>sudo cobbler system list</codeblock></li>
              <li>Remove any controller nodes from Cobbler that no longer exist:
                <codeblock>sudo cobbler system remove --name=&lt;node></codeblock></li>
              <li>Add the new node into Cobbler:
                <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
            </ol></li>
          <li>Then you can image the node:
              <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node_name></codeblock><note>If
              you don't know the <codeph>&#60;node name></codeph> already, you can get it by using
                <codeph>sudo cobbler system list</codeph></note><p>Before proceeding, you may want
              to take a look at <b>info/server_info.yml</b> to see if the assignment of the node you
              have added is what you expect. It may not be, as nodes will not be numbered
              consecutively if any have previously been removed. This is to prevent loss of data;
              the config processor retains data about removed nodes and keeps their ID numbers from
              being reallocated. See the Persisted Server Allocations section in <xref
                keyref="persisteddata/persistedserverallocations">Input Model</xref> for information
              on how this works.</p></li>
          <li>[OPTIONAL] - Run the <codeph>wipe_disks.yml</codeph> playbook to ensure all of your
            partitions on your nodes are completely wiped prior to continuing with the installation:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &#60;controller_node_hostname></codeblock></li>
          <li>Complete the rebuilding of your controller node with the two playbooks
            below:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname>
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname></codeblock></li>
        </ol>
      </sectiondiv>
    </section>
    <section id="allcontrollers"><title outputclass="headerH">Three Control Plane Node Disaster
        Recovery</title>
      <sectiondiv outputclass="insideSection"><p>In this scenario, all control plane nodes are
          destroyed which need to be rebuilt or replaced.</p><b>Restore from a Swift backup:
          </b><p>Restoring from a Swift backup is not possible because Swift is gone.</p><b>Restore
          from an SSH backup: </b><p>
          <ol id="ol_ws5_pyh_1v">
            <li>On the lifecycle manager, follow the procedure below to deploy the control plane
              nodes:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit helion-ccp-c1-m1-mgmt,helion-ccp-c1-m2-mgmt,helion-ccp-c1-m3-mgmt -e '{ "freezer_backup_jobs_upload": false }'</codeblock>You
              can now perform the procedure for <xref href="#topic_tb4_lqy_qt/pointdb" format="dita"
                >Point-in-time database recovery</xref> at the top of this document. Once everything
              is restored, re-enable the backups. </li>
            <li>From the lifecycle manager run _freezer_manage_jobs.yml:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
          </ol>
        </p>
        <p>If you are using a dedicated lifecycle manager, then you can skip these initial steps as
          they are showing you how to recover your shared controller/lifecycle manager first.</p>
      </sectiondiv></section>








    <section id="full"><title outputclass="headerH">Full Disaster Recovery</title>
      <sectiondiv outputclass="insideSection">
        <p>In this disaster scenario, you've lost everything in the cloud, including
          Swift.</p><b>Restore from a Swift backup:</b><p>Restoring from a Swift backup is not
          possible because Swift is gone.</p><b>Restore from an SSH backup: </b><ol
          id="ol_jyz_qyh_1v">
          <li>On the lifecycle manager, edit the following file so it contains the same information
            as it had previously:
            <codeblock>~/helion/my_cloud/config/freezer/ssh_credentials.yml file</codeblock></li>
          <li>On the lifecycle manager copy the following files, switch to the ansible directory,
            and run _deployer_restore_helper.yml:
            <codeblock>cp -r ~/hp-ci/helion/* ~/helion/my_cloud/definition/
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</codeblock>Run
            as root, and change directories:
            <codeblock>sudo su
cd /root/deployer_restore_helper/</codeblock>Stop the Dayzero UI
            installer:<codeblock>systemctl stop dayzero</codeblock>Execute the restore:
            <codeblock>./deployer_restore_script.sh</codeblock>Start the Dayzero UI
            installer:<codeblock>systemctl start dayzero</codeblock>Follow the procedure to deploy
            your cloud:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_backup_jobs_upload": false }'</codeblock>You
            can now perform the procedures to restore MySQL and Swift. Once everything is restored,
            re-enable the backups from the lifecycle manager:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</codeblock></li>
        </ol>
      </sectiondiv>
    </section>
    <section><title outputclass="headerH">Swift Rings Recovery</title>
      <sectiondiv outputclass="insideSection"><p>To recover your Swift rings in the event of a
          disaster, follow the procedure that applies to your situation: either recover the rings
          from one Swift node if possible, or use the SSH backup that you have set up.</p>
        <b>Restore from the Swift deployment backup</b>
        <p>See <xref href="../../troubleshooting/objectstorage/recovering_builder_file.dita"
            />.</p><p><b>Restore from the SSH Freezer backup</b></p><p>In the very specific use case
          where you lost all system disks of all object nodes, and Swift proxy nodes are corrupted,
          you can recover the rings because a copy of the Swift rings is stored in Freezer. This
          means that Swift data is still there (the disks used by Swift needs to be still
          accessible).</p><ol id="ol_p2p_ryh_1v">
          <li>Recover the rings as follows. Note, you will need a node with the freezer-agent
              installed.<p>Become root</p><codeblock>sudo su</codeblock>Create the temporary
            directory to restore your files to:
            <codeblock>mkdir /tmp/swift_builder_dir_restore/</codeblock>Create a restore file with
            the following content:
            <codeblock>cat &lt;&lt; EOF &gt; ./restore_config.ini
[default]
action = restore
storage = ssh
compression = bzip2
restore_abs_path = /tmp/swift_builder_dir_restore/
ssh_key = /etc/freezer/ssh_key
ssh_host = &lt;freezer_ssh_host&gt;
ssh_port = &lt;freezer_ssh_port&gt;
ssh_user name = &lt;freezer_ssh_user name&gt;
container = &lt;freezer_ssh_base_rid>/freezer_swift_backup_name = freezer_swift_builder_backup
hostname = &lt;hostname of the old first Swift-Proxy (SWF-PRX[0])&gt;
EOF</codeblock>
            Edit the file and repave all &lt;tags&gt; with the right information.
            <codeblock>vim ./restore_config.ini</codeblock>You will also need to put the SSH key
            used to do the backups in /etc/freezer/ssh_key and remember to set the right
            permissions: 600. <p>Execute the restore
            job:</p><codeblock>freezer-agent --config ./restore_config.ini</codeblock> You now have
            the Swift rings in <codeph>/tmp/swift_builder_dir_restore/</codeph></li>
          <li>If the SWF-PRX[0] is already deployed, copy the contents of the restored directory
              (<codeph>/tmp/swift_builder_dir_restore/</codeph>) to
              <codeph>/etc/swiftlm/builder_dir/</codeph> on the SWF-PRX[0] Then from the lifecycle
            manager run:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock><p>If
              the SWF-ACC[0] is<b> not </b>deployed, from the lifecycle manager run these
              playbooks:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</codeblock>
              Copy the contents of the restored directory
                (<codeph>/tmp/swift_builder_dir_restore/</codeph>) to
                <codeph>/etc/swiftlm/builder_dir/</codeph> on the SWF-ACC[0] You will have to create
              the directories : <codeph>/etc/swiftlm/builder_dir/</codeph>
            </p></li>
          <li>From the lifecycle manager, run the <codeph>hlm-deploy.yml</codeph> playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml</codeblock></li>
        </ol></sectiondiv></section>

  </body>

</topic>
