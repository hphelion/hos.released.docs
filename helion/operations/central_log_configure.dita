<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd" >
<topic xml:lang="en-us" id="central_log_configure">
  <title>Configuring Centralized Logging</title>
  <body><!--not tested-->
    <section id="Clog_config_overview">
      <title>Installation</title>
      <p>The Centralized Logging feature is automatically installed as part of Helion installation. The base logging levels will be set during installation according to the amount of RAM allocated to your control plane nodes to ensure optimum performance.</p>
      <p>No specific configuration is required to use Centralized Logging. However, you can configure the individual components as needed for your environment. This topic provides information on how to configure the following settings:</p>
      
<ul>
  <li><xref href="#central_log_configure/CL_stop_start">Stopping and Starting the Service</xref></li>
  <li><xref href="#central_log_configure/CL_config_files">Main Configuration Files</xref></li>
  <li><xref href="#central_log_configure/CL_general_config">General Configuration</xref></li>
  <li><xref href="#central_log_configure/CL_kibana_config">Configuring Kibana</xref></li>
  <li><xref href="#central_log_configure/CL_elasticsearch_config">Configuring Elasticsearch</xref></li>
  <li><xref href="central_log_config_level.dita">Configuring the Logging Level for Services</xref></li>
  <li><xref href="#central_log_configure/CL_config_log_rotate">Configuring Log Rotation Settings</xref></li>
  <li><xref href="#central_log_configure/CL_disable">Enable or Disable Centralized Logging</xref></li>
</ul>
    </section>
    
    <section id="CL_stop_start">
      <title>Stopping and Starting the Logging Service</title>   
      <p>The steps in this section only impact centralized logging. Logrotate is an essential feature that keeps the service log files from filling the disk and will not be affected.
     
     <note type="attention">These playbooks must be run from the lifecycle manager.</note>
   </p>
    <p>To stop the Logging service:</p>
<ol>
  <li>To change to the directory containing the ansible playbook, run
    <codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
  <li>To run the ansible playbook that will stop the logging service, run:
    <codeblock>ansible-playbook -i hosts/verb_hosts logging-stop.yml</codeblock></li>
</ol>
      <p>To start the Logging service:</p>
      <ol>
        <li>To change to the directory containing the ansible playbook, run
          <codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
        <li>To run the ansible playbook that will stop the logging service, run:
          <codeblock>ansible-playbook -i ansible-playbook -i hosts/verb_hosts logging-start.yml
</codeblock></li>
      </ol>
    </section>
   
   
   <section id="CL_config_files"><title>Main Configuration Files</title>
      <p>Centralized Logging can be configured via the configuration files in the
          <codeph>~/helion/my_cloud/config/logging/</codeph> directory on the lifecycle manager.
        These files and their use are described below:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_wtv_rc5_st">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <thead>
            <row>
              <entry>File</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>main.yml</entry>
              <entry>Main configuration file for all centralized logging components.</entry>
            </row>
            <row>
              <entry>elasticsearch.yml.j2</entry>
              <entry>Main configuration file for Elasticsearch.</entry>
            </row>
            <row>
              <entry>elasticsearch-default.j2</entry>
              <entry>Default overrides for the Elasticsearch init script.</entry>
            </row>
            <row>
              <entry>kibana.yml.j2</entry>
              <entry>Main configuration file for Kibana.</entry>
            </row>
            <row>
              <entry>kibana-apache2.conf.j2</entry>
              <entry>Apache configuration file for Kibana.</entry>
            </row>
            <row>
              <entry>logstash.conf.j2</entry>
              <entry>Logstash inputs/outputs configuration.</entry>
            </row>
            <row>
              <entry>logstash-default.j2</entry>
              <entry>Default overrides for the Logstash init script.</entry>
            </row>
            <row>
              <entry>beaver.conf.j2</entry>
              <entry>Main configuration file for Beaver.</entry>
            </row>
            <row>
              <entry>vars</entry>
              <entry>Path to logrotate configuration files.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    
    <section id="CL_general_config">
      <title>General Configuration</title>
      <p>The Centralized Logging service needs to have enough resources available to it to perform
        adequately for different scale environments. The base logging levels are tuned during
        installation according to the amount of RAM allocated to your control plane nodes to ensure
        optimum performance.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_grx_2f5_st">
        <tgroup cols="7">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <colspec colname="c6" colnum="6"/>
          <colspec colname="newCol7" colnum="7" colwidth="1*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Ansible Variable</entry>
              <entry>Demo (&lt;32 GB)</entry>
              <entry>Small (&lt;64 GB)</entry>
              <entry>Medium (&lt;128 GB)</entry>
              <entry>Large (>= 128 GB)</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Elasticsearch</entry>
              <entry>elasticsearch_heap_size</entry>
              <entry>256 MB</entry>
              <entry>8 GB</entry>
              <entry>16 GB</entry>
              <entry>32 GB</entry>
              <entry>Amount of heap allocated to Elasticsearch.</entry>
            </row>
            <row>
              <entry>Logstash</entry>
              <entry>logstash_heap_size</entry>
              <entry>256 MB</entry>
              <entry>2 GB</entry>
              <entry>4 GB</entry>
              <entry>8 GB</entry>
              <entry>Amount of heap allocated to Logstash.</entry>
            </row>
            <row>
              <entry>Logstash</entry>
              <entry>logstash_num_workers</entry>
              <entry>10 threads</entry>
              <entry>10 threads</entry>
              <entry>20 threads</entry>
              <entry>30 threads</entry>
              <entry>Number of Logstash threads to spawn.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>These values can be viewed and changed in the
          <codeph>~/helion/my_cloud/config/logging/main.yml</codeph> file, but you will need to run
        a reconfigure of the Centralized Logging service if changes are made.</p>
      <note type="warning">The total process memory consumption for Elasticsearch will be the above
        allocated heap value plus any Java Virtual Machine (JVM) overhead.</note>
      <p><b>Disk Size Requirements</b></p>
      <p>In the entry-scale models, the disk partition sizes on your controller nodes for the
        logging and elasticsearch data are set as a percentage of your total disk size. You can see
        these in the <codeph>~/helion/my_cloud/definition/data/disks_controller.yml</codeph> file on
        the lifecycle manager.</p>
      <p>Here are snippets:</p>
      <codeblock># Local Log files.
- name: log
  size: 13%
  mount: /var/log
  fstype: ext4
  mkfs-opts: -O large_file

# Data storage for centralized logging. This holds log entries from all
# servers in the cloud and hence can require a lot of disk space.
- name: elasticsearch
  size: 30%
  mount: /var/lib/elasticsearch
  fstype: ext4</codeblock>
      <p>Given these percentages, you will want to ensure your total disk size for your controller
        nodes is enough that you meet the following partition size requirements depending on the
        total scale of your environment:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_xld_n31_t5">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>Partition</entry>
              <entry>Small Scale</entry>
              <entry>Medium Scale</entry>
              <entry>Large Scale</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>/var/log</entry>
              <entry>200 GB</entry>
              <entry>400 GB</entry>
              <entry>500 GB</entry>
            </row>
            <row>
              <entry>/var/lib/elasticsearch</entry>
              <entry>1.5 TB</entry>
              <entry>3 TB</entry>
              <entry>6 TB</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    
    <section id="CL_kibana_config">
      <title>Kibana Configuration</title>
      <p>You can use the Kibana dashboards to view log data. Kibana is a tool developed to create
        charts, graphs, tables, and histograms based on logs send to Elasticsearch by logstash.</p>
      <p>While creating Kibana dashboards is beyond the scope of this document, it is important to
        know that you can use the default Kibana dashboards or create custom dashboards. The
        dashboards are JSON files that you can modify or create new dashboards based on existing
        dashboards.</p>
      <note>Kibana is client-side software. To operate properly, the browser must be able to access
        port 5601 on the control plane.</note>
      
      <p>The access information for Kibana is defined in the
        <codeph>~/helion/my_cloud/config/logging/main.yml</codeph> file and implemented in the
        Kibana configuration file
        <codeph>~/helion/my_cloud/config/logging/kibana-apache2.conf.j2</codeph>.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_spl_rg5_st">
        <tgroup cols="3">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <thead>
            <row>
              <entry>Ansible Variable</entry>
              <entry>Default Value</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>kibana_user</entry>
              <entry>kibana</entry>
              <entry>Username that will be required for logging into the Kibana UI.</entry>
            </row>
            <row>
              <entry>kibana_pass</entry>
              <entry>random password is generated</entry>
              <entry>Password generated during installation that is used to login to the Kibana
                UI.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
  
      <p><b>Logging into Kibana</b></p>
      <p>To log into Kibana to view data, you must either access Kibana through Operations Console
        or using a direct link. Then make sure you have the correct login credentials.</p>
      <p><b>To log into Kibana:</b></p>
      <ol>
        <li>Access Kibana using the Operations Console or through a direct link.</li>
        <li>Verify login credentials.</li>
      </ol>
      <p><b>To use Operations Console:</b></p>
      <ol>
        <li>Access <xref href="monitoring_service.dita#monitoring/working" type="section">Operations
          Console</xref>.</li>
        <li>From the menu, choose <b>Logging Dashboard</b>.</li>
      </ol>
      <p><b>To use a direct link:</b></p>
      <p>This section helps you verify the Horizon virtual IP (VIP) address for Kibana that you
        should use.</p>
      <ol>
        <li>Navigate to and open in a text editor the following file:
          <codeblock>network_groups.yml</codeblock>
        </li>
        <li>Find the following entry: <codeblock><codeph>external-name</codeph></codeblock></li>
        <li>If your administrator set a hostname value in the <b>external-name</b> field during the
          configuration process for your cloud, then Kibana will be accessed over port 5601 on that
          hostname.</li>
        <li>If your administrator did not set a hostname value, then to determine which IP address
          to use, from your lifecycle manager, run: <codeblock>grep vip-HZN-WEB /etc/hosts</codeblock>
          <p>The output of that command will show you the virtual IP address for Kibana that you
            should use. Access to Kibana will be over port 5601 of that virtual IP address.
            Example:</p>
          <codeblock>http://&lt;VIP&gt;:5601</codeblock>
        </li>
      </ol>
      <p><b>Login Credentials</b></p>
      <p>During the installation of Kibana, a password is automatically set and it is randomized.
        Therefore, unless an administrator has already changed it, you need to retreive the default
        password from a file on the control plane node.</p>
      <p>The default settings for Kibana are:</p>
      <dl>
        <dlentry>
          <dt>username</dt>
          <dd>kibana</dd>
        </dlentry>
        <dlentry>
          <dt>password</dt>
          <dd>(randomized)</dd>
        </dlentry>
      </dl>
      <p><b>To find the randomized password:</b></p>
      <ol>
        <li>On your lifecycle manager, navigate to the following directory:
          <codeblock>~/scratch/ansible/next/hos/ansible/group_vars/</codeblock>
        </li>
        <li>To GREP for the <codeph>logging_kibana_password</codeph>, run:
          <codeblock>grep logging_kibana_password &lt;name-of-control-plane&gt;</codeblock>
        </li>
      </ol>
      <note type="note">For example, if you are using the Entry-scale KVM with VSA model and you
        kept the default naming scheme in the example files then your command would look similar to
        this:
        <codeblock>grep logging_kibana_password entry-scale-kvm-vsa-control-plane-1</codeblock>
      </note>
    </section>
    
    
    
    <section id="CL_elasticsearch_config">
      <title>Elasticsearch Configuration</title>
      <p>Elasticsearch includes some tunable options exposed in its configuration. Helion uses these options in Elasticsearch to prioritize indexing speed over search speed. Helion also configures Elasticsearch for optimal performance in low RAM environments.
        The options that Helion modifies are listed below along with an explanation about why they were modified.</p>
      <p>These configurations are defined in the
          <codeph>~/helion/my_cloud/config/logging/main.yml</codeph> file and are implemented in the
        Elasticsearch configuration file
          <codeph>~/helion/my_cloud/config/logging/elasticsearch.yml.j2</codeph>.</p>
      <table frame="all" rowsep="1" colsep="1" id="table_qsd_3h5_st">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Ansible Variable</entry>
              <entry>Configuration Parameter</entry>
              <entry>Default Value</entry>
              <entry>Elasticsearch Default Value</entry>
              <entry>Comments</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>elasticsearch_cluster_name</entry>
              <entry>cluster name</entry>
              <entry>elasticsearch</entry>
              <entry>elasticsearch</entry>
              <entry>Name of the Elasticsearch cluster. This variable cannot be changed after the
                initial deployment. Doing so will reset all Elasticsearch indices.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_fielddata_cache_size</entry>
              <entry>indices.fielddata.cache.size</entry>
              <entry>15%</entry>
              <entry>unbounded</entry>
              <entry>By default, this setting is <b>unbounded</b>â€”Elasticsearch will never evict
                data from fielddata, which is less than ideal when trying to conserve on memory
                usage.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_breaker_fielddata_limit</entry>
              <entry>indices.breaker.fielddata.limit</entry>
              <entry>25%</entry>
              <entry>60%</entry>
              <entry>The field data circuit breaker enables Elasticsearch to estimate the amount of
                memory a field will require to be loaded into memory. This value must be greater
                than <b>cache.size</b> to evict data properly.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_ttl_bulk_size</entry>
              <entry>indices.ttl.bulk_size</entry>
              <entry>100000</entry>
              <entry>10000</entry>
              <entry>The number of expired docs to delete at once.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_cache_filter_size</entry>
              <entry>indices.cache.filter.expire</entry>
              <entry>6h</entry>
              <entry>-1</entry>
              <entry>By default, this setting is -1 (i.e., never expires).</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_memory_index_buffer_size</entry>
              <entry>indices.memory.index_buffer_size</entry>
              <entry>50%</entry>
              <entry>10%</entry>
              <entry>Slanting performance in favor of heavier indexing usage over search
                usage.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_memory_min_index_buffer_size</entry>
              <entry>indices.memory.min_index_buffer_size</entry>
              <entry>200 MB</entry>
              <entry>48 MB</entry>
              <entry>Slanting performance in favor of heavier indexing usage over search
                usage.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_memory_min_shard_index_buffer_size</entry>
              <entry>indices.memory.min_shard_index_buffer_size</entry>
              <entry>12 MB</entry>
              <entry>4 MB</entry>
              <entry>Slanting performance in favor of heavier indexing usage over search
                usage.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_store_throttle_type</entry>
              <entry>indices.store.throttle.type</entry>
              <entry>merge</entry>
              <entry>merge</entry>
              <entry>Configures store module throttle for merges.</entry>
            </row>
            <row>
              <entry>elasticsearch_indices_store_throttle_max_bytes_per_sec</entry>
              <entry>indices.store.throttle.max_bytes_per_sec</entry>
              <entry>80 MB</entry>
              <entry>20 MB</entry>
              <entry>Slanting performance in favor of heavier indexing usage over search
                usage.</entry>
            </row>
            <row>
              <entry>elasticsearch_index_refresh_interval</entry>
              <entry>index.refresh_interval</entry>
              <entry>30s</entry>
              <entry>1s</entry>
              <entry>Increasing refresh interval reduces overhead.</entry>
            </row>
            <row>
              <entry>elasticsearch_index_merge_scheduler_max_thread_count</entry>
              <entry>index.merge.scheduler.max_thread_count</entry>
              <entry>1</entry>
              <entry>cores/2</entry>
              <entry>Elasticsearch recommends 1 if using spinning disks.</entry>
            </row>
            <row>
              <entry>elasticsearch_index_translog_flush_threashhold_ops</entry>
              <entry>index.translog.flush_threshhold_ops</entry>
              <entry>150000</entry>
              <entry>unlimited</entry>
              <entry>Each shard has a transaction log or write ahead log associated with it. This
                setting controls when commits occur.</entry>
            </row>
            <row>
              <entry>elasticsearch_index_translog_flush_threashhold_size</entry>
              <entry>index.translog.flush_threshhold_size</entry>
              <entry>1 GB</entry>
              <entry>512 MB</entry>
              <entry>Increasing this setting decreases fsync.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
   
   
   
   
    <section id="troubleshooting"><title>Troubleshooting</title>
     
      <ul>
        <li><xref href="#centralized_logging/review_logs"/></li>
        <li>
          <xref href="#centralized_logging/monitoring">Monitoring Centralized Logging</xref>
        </li>
        <li>
          <xref href="troubleshooting/troubleshooting_logging.dita">Troubleshooting Centralized
            Logging Issues</xref>
        </li>
        <li>
          <xref href="#centralized_logging/info">For More Information</xref>
        </li>
      </ul>
    </section>
    <section id="review_logs">
      <title>Reviewing Log Files</title>
      <p>You can troubleshoot service-specific issues by reviewing the logs. After logging into
        Kibana, follow these steps to load the logs for viewing:</p>
      <ol>
        <li>Navigate to the <b>Settings</b> menu to configure an index pattern to search for.</li>
        <li>In the <b>Index name or pattern</b> field, you can enter <codeph>logstash-*</codeph> to
          query all elasticsearch indices.</li>
        <li>Click the green <b>Create</b> button to create and load the index.</li>
        <li>Navigate to the <b>Discover</b> menu to load the index and make it available to
          search.</li>
      </ol>
      <note>If you want to search specific elasticsearch indices, you can run <codeph>curl
          localhost:9200/_cat/indices?v</codeph> from the control plane to get a full list of
        available indices.</note>
      <p>Once the logs load you can change the timeframe from the dropdown in the upper-righthand
        corner of the Kibana window. You have the following options to choose from:</p>
      <ul>
        <li>Quick - a variety of time frame choices will be available here</li>
        <li>Relative - allows you to select a start time relative to the current time to show this
          range</li>
        <li>Absolute - allows you to select a date range to query</li>
      </ul>
      <p>When searching there are common fields you will want to use, such as:</p>
      <ul>
        <li>type - this will include the service name, such as <codeph>keystone</codeph> or
            <codeph>ceilometer</codeph></li>
        <li>host - you can specify a specific host to search for in the logs</li>
        <li>file - you can specify a specific log file to search</li>
      </ul>
      <p>For more details on using Kibana and Elasticsearch to query logs, see <xref
          href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html"
          scope="external" format="html"
          >https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</xref></p>
    </section>
    <section id="monitoring"><title>Monitoring Centralized Logging</title>
      <p>To help keep ahead of potential logging issues and resolve issues before they affect
        logging, you may want to monitor the Centralized Logging Alarms.</p>
      <p><b>To monitor logging alarms:</b></p>
      <ol>
        <li>Log in to the Operations Console GUI</li>
        <li>Navigate to the Alarm Definitions page from the menu button in the upper left
          corner</li>
        <li>Find the alarm definitions that are applied to the various hosts. See the <xref
            href="alarm_resolutions.dita#alarmdefinitions/logging">Logging Alarm Definitions List</xref> for
          the Centralized Logging Alarm Definitions.</li>
        <li>Navigate to the Alarms page</li>
        <li>Find the alarm definitions applied to the various hosts. These should match the alarm
          definitions in the <xref href="alarm_resolutions.dita#alarmdefinitions/logging">Logging Alarm
            Definitions List</xref>.</li>
        <li>See if the alarm is green (good) or is in a bad state. If any are in a bad state, see
          the possible actions to perform in the <xref href="alarm_resolutions.dita#alarmdefinitions/logging"
            >Logging Alarms Definitions List</xref>.</li>
      </ol>
      <p>You can use this filtering technique in the "Alarms" page to look for the following:</p>
      <ol>
        <li>To look for Processes that may be down, filter for "Process" then make sure the process
          are up: <ol>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ol></li>
      </ol>
      <p>To look for sufficient Disk space, filter for "Disk"</p>
      <p>To look for sufficient RAM Memory, filter for "Memory"</p>
    </section>
    <section id="info">
      <title>For More Information</title>
      <p>For information the centralized logging components, see the following sites:</p>
      <ul>
        <li><xref href="https://www.elastic.co/guide/en/logstash/current/introduction.html"
            format="html" scope="external">Logstash</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide" scope="external" format="html"
            >Elasticsearch</xref></li>
        <li><xref href="http://www.elasticsearch.org/blog/scripting-security" scope="external"
            format="html">Elasticsearch Scripting and Security</xref></li>
        <li><xref href="https://media.readthedocs.org/pdf/beaver/latest/beaver.pdf" format="pdf"
            scope="external">Beaver</xref></li>
        <li><xref href="http://www.rabbitmq.com/" scope="external" format="html"
          >RabbitMQ</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide/en/kibana/current/index.html"
            scope="external" format="html">Kibana Dashboard</xref></li>
      </ul>
    </section>
    
    <section id="CL_config_log_rotate">
      <title>Configuring Log Rotation Settings</title>
      <p>As you use each service, you might need change the default configuration settings such as:
        <ul>
          <li>which files are rotated</li>
          <li>how often they are rotated</li>
          <li>how large they can grow before being rotated</li>
        </ul> 
        Helion Openstack uses the cron process which in turn calls
        <codeph>logrotate</codeph> to provide rotation, compression, and removal of log files. Each
        log file can be checked hourly, daily, weekly, or monthly. If no rotation period is set then
        the log file will only be rotated when it grows too large.</p>
      
      <p>You can find the configuration files for each service in the following 
        directory:
        <codeblock>.../logging-ansible/logging-common/vars/*</codeblock>
        Only files that have centralized logging enabled are copied to Elasticsearch.</p>
        
        <p>All of the variables for the logrotate process are found in the following file:
          <codeblock>../logging-ansible/logging-common/defaults/main.yml          
</codeblock></p>
      
      <p>All of the variables for logging are found in the following file:
        <codeblock>.../logging-ansible/logging-common/defaults/main.yml</codeblock>
      </p>
    </section>
    
    <section id="CL_disable">
      <title>Enable or Disable Centralized Logging For a Service</title>
      <p>To enable or disable Centralized Logging for a service you need to modify the configuration for the service, set the <b>enabled</b> flag to <b>true</b> of <b>false</b>, and then reconfigure logging.</p>
      
      <p>To enable Centralized Logging for a service:</p>
      <ol>
        <li>Use the documentation provided with the service to ensure it is configured for logging.</li>
        <li>To find the Helion Openstack file to edit, run:
        <codeblock>~/helion/my_cloud/config/logging/vars/*</codeblock>
        </li>
        <li>To edit the file, in a text editor, open:
        <codeblock></codeblock>
        </li>
        <li>To enable Centralized Logging, find the following code and change the enabled flag to <b>true</b>:
        <codeblock>
          logging options:
             - centralized_logging:
                 enabled: true
                 format : json
        </codeblock>
        </li>   
      </ol>
      
     <p>Sample of a BURA file enabled for Centralized logging:
      <codeblock>
          ---
          sub service:
            name: freezer-api
            service: freezer
            logging options:
             - centralized_logging:
                 enabled: true
                 format : json
        </codeblock>
     </p>
      
      <p>To disable Centralized Logging for a service:</p>
      <ol>
        <li>Use the documentation provided with the service to ensure it is not configured for logging.</li>
        <li>To find the Helion Openstack file to edit, run:
          <codeblock>~/helion/my_cloud/config/logging/vars/*</codeblock>
        </li>
        <li>To edit the file, in a text editor, open:
          <codeblock></codeblock>
        </li>
        <li>To enable Centralized Logging, find the following code and change the enabled flag to <b>false</b>:
          <codeblock>
          logging options:
             - centralized_logging:
                 enabled: false
                 format : json
        </codeblock>
        </li>   
      </ol>
      
      <p>Sample of a BURA file enabled for Centralized logging:
        <codeblock>
          ---
          sub service:
            name: freezer-api
            service: freezer
            logging options:
             - centralized_logging:
                 enabled: false
                 format : json
        </codeblock>
      </p>
      
    </section>
    
  </body>
</topic>
