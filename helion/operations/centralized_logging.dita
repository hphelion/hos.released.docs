<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd" >
<topic xml:lang="en-us" id="centralized_logging">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Centralized Logging Service</title>
  <abstract><shortdesc outputclass="hdphidden">Using the Centralized Logging
    service.</shortdesc></abstract>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>A typical <keyword keyref="kw-hos"/> cloud consists of multiple servers which makes locating
      a specific log from a single server difficult. The <keyword keyref="kw-hos"/> Centralized
      Logging feature helps the administrator evaluate and troubleshoot the distributed cloud
      deployment from a single location.</p>
    <p>The Centralized Logging feature collects logs on a central system, rather than leaving the
      logs scattered across the network. The administrator can use a single Kibana interface to view
      log information in charts, graphs, tables, histograms, and other forms.</p>
    <p>In addition to each of the <keyword keyref="kw-hos"/> services, Centralized Logging also
      processes logs for the following features:</p>
    <ul>
      <li>HAProxy</li>
      <li>syslog</li>
      <li>keepalived</li>
    </ul>
    <p>This document describes the Centralized Logging feature and contains the following
      sections:</p>
    <ul>
      <li><xref href="#centralized_logging/install">Installation</xref></li>
      <li><xref href="#centralized_logging/components">Centralized Logging Components</xref></li>
      <li><xref href="#centralized_logging/retaining_logs">Log Retention Overview</xref></li>
      <li><xref href="#centralized_logging/rotating_logs">Log Rotation Overview</xref></li>
      <li>
        <xref href="#centralized_logging/types">Centralized Logging Data</xref>
      </li>
      <li>
        <xref href="#centralized_logging/kibana">Kibana Configuration</xref>
        <ul>
          <li>
            <xref href="#centralized_logging/interface">Logging into Kibana</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref href="#centralized_logging/troubleshooting">Using Centralized Logging to Troubleshoot
          Issues</xref>
      </li>
      <li>
        <xref href="#centralized_logging/monitoring">Monitoring Centralized Logging</xref>
      </li>
      <li>
        <xref href="troubleshooting/troubleshooting_logging.dita">Troubleshooting Centralized
          Logging Issues</xref>
      </li>
      <li>
        <xref href="#centralized_logging/info">For More Information</xref>
      </li>
    </ul>
    <section id="install">
      <title>Installation</title>
      <p>The Centralized Logging feature is automatically installed as part of the <keyword
          keyref="kw-hos"/> installation. The base logging levels will be tuned during installation
        according to the amount of RAM allocated to your control plane nodes to ensure optimum
        performance.</p>
      <p>No specific configuration is required to use Centralized Logging. However, you can tune or
        configure the individual components as needed for your environment as detailed in the <xref
          href="configuring/configure_logging.dita">Configuration the Centralized Logging
          Service</xref> page.</p>
    </section>
    <section id="components">
      <title>Centralized Logging Components</title>
      <p>Centralized logging consists of several components, detailed below:</p>
      <ul>
        <li>
          <p>
            <b>Beaver</b> is a python daemon that takes information in log files and sends the
            content to RabbitMQ.</p>
        </li>
        <li>
          <p>
            <b>RabbitMQ</b> is a message broker for collection of logging data across nodes.</p>
        </li>
        <li>
          <p>
            <b>Logstash</b> is a log processing system for receiving, processing and outputting
            logs. Logstash retrieves logs from RabbitMQ, processes and enriches the data, then
            stores the data in Elasticsearch.</p>
        </li>
        <li>
          <p>
            <b>Elasticsearch</b> is a data store offering fast indexing and querying.</p>
        </li>
        <li>
          <p>
            <b>Kibana</b> is a client-side JavaScript application to visualize the data in
            Elasticsearch through a web browser. Kibana enables you to create charts and graphs
            using the log data.</p>
        </li>
        <li><b>Curator</b> is a tool provided by Elasticsearch to manage indices.</li>
      </ul>
      <p>These components are configured to work out-of-the-box and the admin should be able to view
        log data using the default configurations.</p>
      <p>In an overview of the process, at a high level, the Helion services forward logs to Beaver.
        Then, Beaver forwards JSON messages to RabbitMQ on the controller0 (management controller)
        node. Logstash connects to RabbitMQ to read queued messages and process the messages
        according to the Logstash configuration file. Logstash then forwards the processed log files
        in Elasticsearch. Users can use the Kibana interface to view and analyze the information, as
        shown in the following figure:</p>
      <p>
        <image href="../../media/centralized_logging_diagram.png" placement="break"/>
      </p>
      <note>The arrows come <b>from</b> the active (requesting) side <b>to</b> the passive
        (listening) side. The active side is always the one providing credentials, so the arrows may
        also be seen as coming from the credential holder to the application requiring
        authentication.</note>
    </section>
    <section id="retaining_logs"><title>Log Retention Overview</title>
      <p>The logs that are centrally stored are saved to persistent storage as Elasticsearch
        indices. These indices are stored in the partition <codeph>/var/lib/elasticsearch</codeph>
        on each of the Elasticsearch cluster nodes. Out of the box, each day&apos;s worth of logs is
        stored in one Elasticsearch index. As more days go by, the number of indices stored in this
        disk partition grows. Eventually the partition fills up. If they are "open," each of these
        indices takes up CPU and memory. If these indicies are left unattended they will continue to
        consume system resources and eventually deplete them.</p>
      <p>Elasticsearch, by itself, doesn't prevent this from happening.</p>
      <p><keyword keyref="kw-hos"/> uses a tool called <b>curator</b> that is developed by the
        Elasticsearch community to handle these situations. <keyword keyref="kw-hos"/> installs and
        uses a curator in conjunction with several configurable settings. This curator is called
        <b>cron</b> and perfoms the following checks:</p>
      <ul>
        <li><b>First Check.</b> The hourly cron job checks to see if the currently used Elasticsearch
          partition size is over the value set in:
          <codeblock>curator_low_watermark_percent</codeblock>
          If it is higher than this value, the curator deletes old indices according to the value set in:
            <codeblock>curator_num_of_indices_to_keep</codeblock></li>
        <li><b>Second Check.</b> Another check is made to verify if the partition size is below the high
          watermark percent. If it is still too high, curator will delete all indices except the current one that is over the size as set in:
            <codeblock>curator_max_index_size_in_gb</codeblock></li>
        <li><b>Third Check.</b> A third check verifies if the partition size is still too high. If it is, curator
          will delete all indices except the current one.</li>
        <li><b>Final Check.</b> A final check verifies if the partition size is still high. If it is, an error
          message is written to the log file but the current index is NOT deleted.</li>
      </ul>
      <p>In the case of an extreme network issue, log files can run out of disk space in under an hour. To avoid this <keyword keyref="kw-hos"/> uses a shell script called <codeph>logrotate_if_needed.sh</codeph>. The cron process runs this script every 5 minutes to see if the value in <codeph>/var/log</codeph> is at 95% or more of its default. If it is at or above this level, cron runs the <codeph>logrotate_if_needed.sh</codeph> script to rotate logs and to free up extra space. This script helps to minimize the chance of running out of disk space on <codeph>/var/log</codeph>.</p>
    </section>
    <section id="types">
      <title>Centralized Logging Data</title>
      <p>The following table lists the types of logs collected by Centralized Logging and provides
        information on how the logs are maintained.</p>
      <table>
        <tgroup cols="6">
          <colspec colname="col1" colwidth="1.41*"/>
          <colspec colname="col2" colwidth="1.59*"/>
          <colspec colname="col3" colwidth="1*"/>
          <colspec colname="col4" colwidth="1.29*"/>
          <colspec colname="col5" colwidth="1*"/>
          <colspec colname="col6" colwidth="4.17*"/>
          <thead>
            <row>
              <entry>Data name</entry>
              <entry>Confidentiality</entry>
              <entry>Integrity</entry>
              <entry>Availability</entry>
              <entry>Backup?</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Log records</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Log records have a limited life, and are not archived. The log file on the
                local filesystem provides a fallback source of logging data (up to 20GB or 45 days)
                if the logging system fails.</entry>
            </row>
            <row>
              <entry>Log metadata</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Elasticsearch indexes logged data to allow flexible searching.</entry>
            </row>
            <row>
              <entry>Credentials</entry>
              <entry>Confidential</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Credentials for access to Elasticsearch and RabbitMQ are stored in
                configuration files owned by root with mode 0600.</entry>
            </row>
            <row>
              <entry>Kibana metadata</entry>
              <entry>Confidential</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>No</entry>
              <entry>Kibana stores its search queries, visualizations and dashboards in the index
                called ".kibana" . This index is replicated across Elasticsearch cluster nodes and
                is highly available.</entry>
            </row>
            <row>
              <entry>Monitoring metrics</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>Monasca Server stores and manages the various logging metrics and
                alarm-definitions.</entry>
            </row>
            <row>
              <entry>Beaver configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>These settings are backed up as part of lifecycle manager repo changes
                maintained by the administrator.</entry>
            </row>
            <row>
              <entry>Logrotate configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>These are backed up as part of lifecycle manager repo changes maintained by the
                administrator.</entry>
            </row>
            <row>
              <entry>Curator configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>Yes</entry>
              <entry>Cron job to periodically run and keep the old Elasticsearch indices
                pruned/closed. These are backed up as part of lifecycle manager repos changes
                maintained by the administrator.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    
    <section id="rotating_logs"><title>Log Rotation Overview</title>
      <p>Helion Openstack uses the cron process which in turn calls <b>logrotate</b> to provide rotation, compression, and removal of log files. Each log file can be rotated hourly, daily, weekly, or monthly. If no rotation period is set then the log file will only be rotated when it grows too large.</p>
      
      <p>Rotating a file means that the logrotate process creates a copy of the log file with a new
        extension, for example, the <b>.1</b> extension. Then logrotate empties the contents of the
        original file. Logrotate continues to create the number of copies specified in the
          <b>Rotation</b> variable. When logrotate reaches the last possible file extension, it will
        delete the last file on the next rotation. By the time the logrotate process needs to delete
        a file, the results have been copied to Elasticsearch, the central logging database.</p> 
      
      <p>As an example, the Backup and Restore (BURA) service may be configured to create log files
        by setting the <b>Rotated Log Files</b> variable to
        <codeblock>/var/log/freezer-agent/freezer-scheduler.log</codeblock>This configuration means
        that in the <codeph>/var/log/freezer-agent</codeph> directory, in a live environment, there
        should be a file called <codeph>freezer-scheduler.log</codeph>. As the log file grows, the
        cron process runs every hour to check the log file size against the settings in the
        configuration files. The example freezer-agent settings are described in the table below. </p>
        
         <p>
      <table frame="all" rowsep="1" colsep="1" id="table_itd_tjh_ht">
        <tgroup cols="7">
            <colspec colname="c1" colnum="1" colwidth="1.32*"/>
            <colspec colname="c2" colnum="2" colwidth="1*"/>
            <colspec colname="c3" colnum="3" colwidth="3.26*"/>
            <colspec colname="c4" colnum="4" colwidth="1.46*"/>
            <colspec colname="c5" colnum="5" colwidth="1.05*"/>
            <colspec colname="c6" colnum="6" colwidth="1.4*"/>
            <colspec colname="c7" colnum="7" colwidth="3.38*"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Node Type</entry>
                <entry>Rotated Log Files</entry>
                <entry>Frequency</entry>
                <entry>Max Size</entry>
                <entry>Rotation</entry>
                <entry>Centrally Logged Files</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <p>BURA (Backup and Restore)</p>
                </entry>
                <entry>
                  <p>Control</p>
                </entry>
                <entry>
                  <p>/var/log/freezer-agent/freezer-scheduler.log</p>
                  <p>/var/log/freezer-agent/freezer-agent-json.log</p>
                </entry>
                <entry>
                  <p>Daily</p>
                </entry>
                <entry>
                  <p>32 MB</p>
                </entry>
                <entry>
                  <p>7</p>
                </entry>
                <entry>
                  <p>/var/log/freezer-agent/freezer-agent-json.log</p>
                </entry>
              </row>
            </tbody>
          </tgroup>
      </table>
    </p>
        
        <p>For the <codeph>freezer-scheduler.log</codeph> file specifically, the information in the
        table tells the cron and logrotate processes that the log file is to be rotated daily, and
        it can have a maximum size of 32 MB. After a week of log rotation, you would see something
        similar to this list:
        <codeblock>
          freezer-scheduler.log at 10K
          freezer-scheduler.log.1 at 123K
          freezer-scheduler.log.2 at 13K
          freezer-scheduler.log.3 at 17K
          freezer-scheduler.log.4 at 128K
          freezer-scheduler.log.5 at 22K
          freezer-scheduler.log.6 at 323K
          freezer-scheduler.log.7 at 123K
        </codeblock>
        Since the Rotation value is set to 7 for this log file, there will never be a
          <codeph>freezer-scheduler.log8</codeph>. When the cron process runs its checks, if the
          <codeph>freezer-scheduler.log</codeph> size is more than 32 MB, then logrotate rotates the
        file.</p>
      
      <p>In this example, the following log files are rotated:
      <codeblock>
        /var/log/freezer-agent/freezer-scheduler.log
        /var/log/freezer-agent/freezer-agent-json.log
      </codeblock>
      </p>
      
      <p>However, in this example, only the following file is centrally logged with Elasticsearch:
        <codeblock>
          /var/log/freezer-agent/freezer-agent-json.log
        </codeblock>
        Only files that are listed in the <b>Centrally Logged Files</b> variable are copied to
        Elasticsearch. </p>
        
        <p>All of the variables for logging are found in the following file:
          <codeblock>
            .../logging-ansible/kronos-common/defaults/main.yml
          </codeblock>
        </p>
        
        <p>Cron uses the following values to determine when to call the logrotate process: <ul>
          <li>Low watermark:
            <codeblock>
          #The following low watermark will be used to trigger alarms if the /var/log parition size grows over it.
          #Tuning this to a higher percent may not give sufficient time to free disk space on /var/log before
          # it reaches 100% full, at which time log rotate will not work.
          
          <b>var_log_low_watermark_percent:80</b>
        </codeblock>
          </li>
          <li> High watermark:
            <codeblock>
                #The following high watermark will be used to alert the user that when /var/log is 100% full,
                #log rotate will not work, and to free up space on /var/log.
                #Don't set it at 100%, as by then alarms may not fire properly.
                
                <b>var_log_high_watermark_percent:95</b>
              </codeblock>
          </li>
        </ul>
      </p>
      
    </section>
    
    <section id="kibana">
      <title>Kibana Configuration</title>
      <p>You can use the Kibana dashboards to view log data. Kibana is a tool developed to create
        charts, graphs, tables, and histograms based on logs send to Elasticsearch by logstash.</p>
      <p>While creating Kibana dashboards is beyond the scope of this document, it is important to
        know that you can use the default Kibana dashboards or create custom dashboards. The
        dashboards are JSON files that you can modify or create new dashboards based on existing
        dashboards.</p>
      <note>Kibana is client-side software. To operate properly, the browser must be able to access
        port 5601 on the control plane.</note>
    </section>
    <section id="interface">
      <title>Logging into Kibana</title>
      <p>To log into Kibana to view data, you must either access Kibana through Operations
        Console or using a direct link. Then make sure you have the correct login credentials.</p>
      <p><b>To log into Kibana:</b></p>
      <ol>
        <li>Access Kibana using the Operations Console or through a direct link.</li>
        <li>Verify login credentials.</li>
      </ol>
      <p><b>To use Operations Console:</b></p>
      <ol>
        <li>Access <xref href="monitoring_service.dita#monitoring/working" type="section">Operations
          Console</xref>.</li>
        <li>From the menu, choose <b>Logging Dashboard</b>.</li>
      </ol>
       
      <p><b>To use a direct link:</b></p>
      <p>This section helps you verify the Horizon virtual IP (VIP) address for Kibana that you should use.</p>
      <ol>
        <li>Navigate to and open in a text editor the following file:
          <codeblock>network_groups.yml</codeblock> </li>
          <li>Find the following entry:
            <codeblock><codeph>external-name</codeph></codeblock></li>
          <li>If your administrator set a hostname value in the <b>external-name</b> field during the configuration process for your cloud,
            then Kibana will be accessed over port 5601 on that hostname.</li>
            <li>If your administrator did not set a hostname value, then to determine which IP
              address to use, from your lifecycle manager, run:
            <codeblock>grep vip-HZN-WEB /etc/hosts</codeblock>
            <p>The output of that command will show you the virtual IP address for Kibana that you should
              use. Access to Kibana will be over port 5601 of that virtual IP address. Example:</p>
            <codeblock>http://&lt;VIP&gt;:5601</codeblock>
        </li>
      </ol>
      <p><b>Login Credentials</b></p>
      <p>During the installation of Kibana, a password is automatically set and it is randomized. Therefore, unless an administrator has already changed it, you need to retreive the default password from a file on the control plane node.</p>
      <p>The default settings for Kibana are:</p>
      <dl>
        <dlentry>
          <dt>username</dt>
          <dd>kibana</dd>
        </dlentry>
      <dlentry>
        <dt>password</dt>
        <dd>(randomized)</dd>
      </dlentry>
      </dl>
      <p><b>To find the randomized password:</b></p>
      <ol>
        <li>On your lifecycle manager, navigate to the following directory:
          <codeblock>~/scratch/ansible/next/hos/ansible/group_vars/</codeblock>
        </li>
        <li>To GREP for the <codeph>logging_kibana_password</codeph>, run:
          <codeblock>grep logging_kibana_password &lt;name-of-control-plane&gt;</codeblock> </li>
      </ol>
      
      <note type="note">For example, if you are using the Entry-scale KVM with VSA model and you kept the default naming scheme in the example files
        then your command would look similar to this:
        <codeblock>grep logging_kibana_password entry-scale-kvm-vsa-control-plane-1</codeblock>
      </note>

      
    </section>
    <section id="troubleshooting"><title>Using Centralized Logging to Troubleshoot Issues</title>
      <p>You can troubleshoot service-specific issues by reviewing the logs. After logging into
        Kibana, follow these steps to load the logs for viewing:</p>
      <ol>
        <li>Navigate to the <b>Settings</b> menu to configure an index pattern to search for.</li>
        <li>In the <b>Index name or pattern</b> field, you can enter <codeph>logstash-*</codeph> to
          query all elasticsearch indices.</li>
        <li>Click the green <b>Create</b> button to create and load the index.</li>
        <li>Navigate to the <b>Discover</b> menu to load the index and make it available to
          search.</li>
      </ol>
      <note>If you want to search specific elasticsearch indices, you can run <codeph>curl
          localhost:9200/_cat/indices?v</codeph> from the control plane to get a full list of
        available indices.</note>
      <p>Once the logs load you can change the timeframe from the dropdown in the upper-righthand
        corner of the Kibana window. You have the following options to choose from:</p>
      <ul>
        <li>Quick - a variety of time frame choices will be available here</li>
        <li>Relative - allows you to select a start time relative to the current time to show this
          range</li>
        <li>Absolute - allows you to select a date range to query</li>
      </ul>
      <p>When searching there are common fields you will want to use, such as:</p>
      <ul>
        <li>type - this will include the service name, such as <codeph>keystone</codeph> or
            <codeph>ceilometer</codeph></li>
        <li>host - you can specify a specific host to search for in the logs</li>
        <li>file - you can specify a specific log file to search</li>
      </ul>
      <p>For more details on using Kibana and Elasticsearch to query logs, see <xref
          href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html"
          scope="external" format="html"
          >https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</xref></p>
    </section>
    <section id="monitoring"><title>Monitoring Centralized Logging</title>
      <p>To help keep ahead of potential logging issues and resolve issues before they affect
        logging, you may want to monitor the Centralized Logging Alarms.</p>
      <p><b>To monitor logging alarms:</b></p>
      <ol>
        <li>Log in to the Operations Console GUI</li>
        <li>Navigate to the Alarm Definitions page from the menu button in the upper left
          corner</li>
        <li>Find the alarm definitions that are applied to the various hosts. See the <xref
            href="alarms.dita#alarmdefinitions/logging">Logging Alarm Definitions List</xref> for
          the Centralized Logging Alarm Definitions.</li>
        <li>Navigate to the Alarms page</li>
        <li>Find the alarm definitions applied to the various hosts. These should match the alarm
          definitions in the <xref href="alarms.dita#alarmdefinitions/logging">Logging Alarm
            Definitions List</xref>.</li>
        <li>See if the alarm is green (good) or is in a bad state. If any are in a bad state, see
          the possible actions to perform in the <xref href="alarms.dita#alarmdefinitions/logging"
            >Logging Alarms Definitions List</xref>.</li>
      </ol>
      <p>You can use this filtering technique in the "Alarms" page to look for the following:</p>
      <ol>
        <li>To look for Processes that may be down, filter for "Process" then make sure the process
          are up: <ol>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ol></li>
      </ol>
      <p>To look for sufficient Disk space, filter for "Disk"</p>
      <p>To look for sufficient RAM Memory, filter for "Memory"</p>
    </section>
    <section id="info">
      <title>For More Information</title>
      <p>For information the centralized logging components, use the following links:</p>
      <ul>
        <li><xref href="https://www.elastic.co/guide/index.html" scope="external" format="html"
          >Logstash</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide" scope="external" format="html"
            >Elasticsearch</xref></li>
        <li><xref href="http://www.elasticsearch.org/blog/scripting-security" scope="external"
            format="html">Elasticsearch Scripting and Security</xref></li>
        <li><xref href="https://media.readthedocs.org/pdf/beaver/latest/beaver.pdf" scope="external" format="html"
          >Beaver</xref></li>
        <li><xref href="http://www.rabbitmq.com/" scope="external" format="html"
          >RabbitMQ</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide/en/kibana/current/index.html"
            scope="external" format="html">Kibana Dashboard</xref></li>
      </ul>
    </section>
  </body>
</topic>
