<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="replacing_controller">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Controller Node</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section>
      <p>For <keyword keyref="kw-hos-phrase"/> versions, you must have three controller nodes.
        Therefore, adding or removing nodes is not an option. However, if you need to repair or
        replace a controller node, you may do so by following the steps outlined here. Note that to
        run any playbooks whatsoever for cloud maintenance, you will always run the steps from the
        lifecycle manager.</p>
      <p>These steps will depend on whether you are needing to replace a shared lifecycle
        manager/controller node or whether this is a standalone controller node.</p>
      <ul>
        <li><xref href="replace_controller.dita#replacing_controller/shared">Shared Lifecycle
            Manager/Controller Node</xref></li>
        <li><xref href="replace_controller.dita#replacing_controller/standalone">Standalone
            Controller Node</xref></li>
      </ul>
    </section>
    <section id="notes"><title>Notes</title>
      <p>Do not add entries for a new server; instead, just update the entries for the broken
        one.</p>
      <p>Be aware that all management commands are run on the node where the lifecycle manager is
        running.</p>
    </section>
    <section id="shared"><title>Procedure for Replacing a Shared Lifecycle Manager/Controller
        Node</title>
      <p>If the controller node you need to replace was also being used as your lifecycle manager
        then use these steps below. If this is not a shared controller then skip to the next
        section.</p>
      <ol>
        <li>Ensuring that you use the same version of HPE Helion OpenStack that you previously had
          loaded on your lifecycle manager, you will need to download and install the lifecycle
          management software using the instructions from the installation guide: <ol>
            <li><xref href="../installation/install_entryscale_kvm.dita#install_kvm/setup_deployer"
                >Set up the Lifecycle-manager</xref></li>
          </ol></li>
        <li>Then you will want to restore your data using the <xref
            href="../bura/cloud_control_plane_recovery.dita#topic_tb4_lqy_qt/deployer">Deployer
            Disaster Recovery</xref> instructions. <ol>
            <li>If your node was restored from a backup (meaning the <codeph>/home</codeph>
              directory was replaced and the users were recreated) then the replaced node will fail
              as these steps will check for the existence of the <codeph>vertica</codeph> user to
              determine if we need to rebuild. So, in these cases, you need to do the following
              step:</li>
            <li>Delete the files below from the node you replaced:
              <codeblock>/home/dbadmin/.ssh/id_rsa
/home/dbadmin/.ssh/known_hosts</codeblock></li>
          </ol></li>
        <li> Update your cloud model (<codeph>servers.yml</codeph>) with the new
            <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>, <codeph>ilo-password</codeph>, or
            <codeph>ilo-user</codeph> fields where these have changed. Do not change the
            <codeph>id</codeph>, <codeph>ip-addr</codeph>, <codeph>role</codeph>, or
            <codeph>server-group</codeph> settings. (Please follow the procedure for updating your
          cloud model in the git repo)</li>
        <li> Get the <b>servers.yml</b> file stored in git:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock> then change,
          as necessary, the <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>,
            <codeph>ilo-password</codeph>, and <codeph>ilo-user</codeph> fields of this existing
          controller node. Save and commit the change
          <codeblock>git commit -a -m "repaired node X"</codeblock></li>
        <li>Run the configuration processor as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          Then run ready-deployment:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Once that is complete, copy the Cobbler images to the correct location:
          <codeblock>sudo mkdir /srv/www/cobbler/ks_mirror/hlinux-cattleprod 
sudo cp /opt/hlm_packager/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux /srv/www/cobbler/ks_mirror/hlinux-cattleprod
sudo cp /opt/hlm_packager/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz /srv/www/cobbler/ks_mirror/hlinux-cattleprod</codeblock></li>
        <li>Deploy Cobbler:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock><note>After
            this step you may see failures because MySQL has not finished syncing. If so, please
            rerun this step (7).</note></li>
        <li>Delete the haproxy user: <codeblock>sudo deluser haproxy</codeblock></li>
        <li>SSH to your other controller nodes and run the following command:
          <codeblock>sudo ssh-keygen -f "/home/dbadmin/.ssh/known_hosts" -R &lt;IP_of_replaced node></codeblock></li>
        <li>Install the software on your new lifecycle manager/controller node with these three
          playbooks:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</codeblock></li>
      </ol>
    </section>

    <section id="standalone"><title>Procedure for Replacing a Standalone Controller Node</title>
      <p>If the controller node you need to replace is a standalone controller node, these steps
        will assist you in replacing it.</p>
      <ol>
        <li> Update your cloud model (<codeph>servers.yml</codeph>) with the new
            <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>, <codeph>ilo-password</codeph>, or
            <codeph>ilo-user</codeph> fields where these have changed. Do not change the
            <codeph>id</codeph>, <codeph>ip-addr</codeph>, <codeph>role</codeph>, or
            <codeph>server-group</codeph> settings. (Please follow the procedure for updating your
          cloud model in the git repo)</li>
        <li> Get the <b>servers.yml</b> file stored in git:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock> then change,
          as necessary, the <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>,
            <codeph>ilo-password</codeph>, and <codeph>ilo-user</codeph> fields of this existing
          controller node. Save and commit the change
          <codeblock>git commit -a -m "repaired node X"</codeblock></li>
        <li>Run the configuration processor as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          Then run ready-deployment:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Log onto the node that is running the lifecycle manager. Remove the broken control plane
          node from cobbler by running sudo cobbler system remove --name &lt;server-id>. Here's an
          example:<codeblock>sudo cobbler system remove --name controller2</codeblock>
        </li>
        <li>Remove the ssh key of the old controller node from the known hosts file. Here's an
          example:
          <codeblock>ssh-keygen -f "/home/stack/.ssh/known_hosts" -R 10.123.12.3</codeblock></li>
        <li>Run the cobbler deploy playbook to add the replacement node
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock>
        </li>
        <li> Re-image the new nodes by running <b>bm-reimage.yml</b>: ansible-playbook -i
          hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name>. Note: Note: you must ensure
          that the old controller node is powered off before completing this step. This is because
          the new controller node will re-use the original IP address.
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2</codeblock></li>
        <li>Configure the necessary keys used for the database etc:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</codeblock></li>
        <li>SSH to your other controller nodes and run the following
          command:<codeblock>sudo ssh-keygen -f "/home/dbadmin/.ssh/known_hosts" -R &lt;IP of replaced node></codeblock></li>
        <li>Run osconfig on the replacement controller node. For example:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname></codeblock>
        </li>
        <li> If the controller being replaced is the first server running the swift-proxy service
          (see <xref href="../objectstorage/first_proxy_server.dita">Identifying the First Proxy
            Server</xref>) you need to restore the Swift Ring Builder files to the
          /etc/swiftlm/builder_dir directory. See <xref
            href="../objectstorage/recovering_builder_file.dita#topic_gbz_13t_mt">Recovering Builder
            Files</xref> for details.</li>
        <!-- DOCS-2306
        <li>Run swift-deploy across all controllers using:
          <codeblock>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
          -->
        <li> Run the hlm-deploy playbook on the replacement controller. <p>If the node being
            replaced is the first proxy node then you only need to use the <codeph>--limit</codeph>
            switch for that node, otherwise you need to specify the hostname of your first proxy and
            the hostname of the node being replaced.</p>
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname>,&lt;first-proxy-hostname></codeblock>
          <p>See <xref href="../objectstorage/first_proxy_server.dita">Identifying the First Swift
              Proxy Server</xref> for more details on how to determine which node is your first
            Swift proxy node.</p>
        </li>
      </ol>
      <!--
      <ul>
      <li>When the node is ready to be restored, the process for re-introducing it will depend on the
        actions performed to repair it. If it was simply rebooted to introduce new software, etc.
        then running the <b>hlm-start.yml</b> playbook will suffice. If needed, use
        <b>bm-power-up.yml </b>playbook to restart the node. Specify just the node(s) you want
        to start in the 'nodelist' parameter aguments, i.e.
        nodelist=&lt;node1>[,&lt;node2>][,&lt;node3>]
        <codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=cpn-0004</codeblock>
      </li><li>Execute the <b>hlm-start.yml </b>playbook. Specifying the node(s) you want to start in the
        'limit' parameter arguments. This parameter accepts wildcard arguments and also
        '@&lt;filename>' to process all hosts listed in the file.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml - -limit hlm004-ccp-comp0004-mgmt</codeblock>
      </li><li>If the node has undergone repairs that impact disk contents, then running the
        <b>re-image.yml</b> and <b>hlm-deploy.yml </b>playbooks will be required.
        Run:<codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0004</codeblock>
      </li><li>Then execute the <b>hlm-start.yml</b> playbook.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml - -limit hlm004-ccp-comp0004-mgmt</codeblock>
      </li>
      </ul>
      -->
    </section>
  </body>
</topic>
