<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_e32_tm2_rt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a VSA Node For Maintenance</title>
  <abstract><shortdesc outputclass="hdphidden">Removing a VSA node temporarily for
      maintenance.</shortdesc><p>This process is used when you want to remove a VSA node from a
      cluster or management group for maintenance, using the HPE StoreVirtual Management Console
      (CMC) utility.</p>
    <p>After maintenance, the VSA node is added back to the cluster.</p></abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <section id="notes"><title>Notes</title>
      <p>In order for the Block Storage volumes to remain available even after the VSA storage node
        goes offline, you must ensure that the VSA network RAID configuration/data protection
        aspects are taken care of prior to removing a VSA node. See the following sections of the
        HPE StoreVirtual Storage Online Help (available in the StoreVirtual Management Console)
        prior to configuring RAID:</p>
      <ul>
        <li>Using disk RAID with Network RAID in a cluster</li>
        <li>Planning the RAID configuration</li>
        <li>Data protection</li>
      </ul>
      <p>You must configure RAID before adding the storage system to the management group. You must
        set the data protection level while creating the volume type.</p>
      <p>See the following pages for more information:</p>
      <ul>
        <li><xref
            href="http://docs.openstack.org/liberty/config-reference/content/HP-LeftHand-StoreVirtual-driver.html"
            format="html" scope="external">HPE LeftHand/StoreVirtual driver</xref></li>
        <li><xref href="../creating_voltype.dita"/></li>
      </ul>
    </section>
    <section>
      <title>To Remove a VSA Storage System for Maintenance</title>
      <p>Perform the following steps to remove a VSA storage system for maintenance:</p>
      <ol>
        <li>Ensure that the volume is available if the storage system/VSA node to be removed goes
          offline. For instructions, see the "Determining volume and snapshot availability" section
          in the <b>HPE StoreVirtual Storage Online help</b>.</li>
        <li>If the repair option is available for the VSA storage system, choose the appropriate
          option in repair. For instructions, see the "Using Repair Storage System" section in the
            <b>HPE StoreVirtual Storage Online help in CMC</b>.<p>OR</p><p>If the repair option is
            unavailable, perform either of the below steps to remove the VSA storage system for
            maintenance:</p><p>Perform any of the below operations to ensure that the cluster
            functions normally and the volumes continue to be available even after the storage
            system is removed/replaced. (For instructions see "Maintaining storage systems in
            clusters" section in the <b>HPE StoreVirtual Storage Online help in CMC</b>).</p><p>
            <ul id="ul_dmj_sq2_rt">
              <li>Add and remove storage systems for an existing cluster (the new storage system
                from the list of available VSA storage systems as shown in CMC). </li>
              <li>Exchange a storage system in a cluster. </li>
              <li>Remove a storage system from a cluster (provided the remaining storage system is
                sufficient for the data availability: a minimum of 3 nodes should be available after
                the remove operation). Once the maintenance is done, the storage system may be added
                back to the cluster. Follow the instructions for <b>"Adding a storage system to a
                  cluster"</b> to add the VSA storage system back to the cluster.</li>
            </ul>
          </p></li>
      </ol>
    </section>
    <section>
      <title>Excluding the VSA Node</title>
      <p>Even after the removable of a VSA node, it should continue to remain in the
          <codeph>data/servers.yml</codeph> file. You should also create a
          <codeph>servers.yml</codeph> file on the lifecycle manager with a list of VSA nodes that
        are currently offline, so that these nodes get excluded from any further reconfiguration or
        upgrades. This prevents a VSA node that is offline from being included in deployments.</p>
      <p>Perform the following steps to remove the VSA node for maintenance:<ol>
          <li>Login to the lifecycle manager.</li>
          <li>Verify the node that needs to be kept under maintenance from
              <codeph>hosts/verb_hosts</codeph>
            file:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts</codeblock></li>
          <li>Create a file (for example: @offline-vsa) and enter the node information that needs to
            be placed under maintenance. If, for example, you want to place
              <codeph>padawan-ccp-vsa001</codeph> under maintenance, you must use the host name
            along with ! and * (for example: <codeph>!padawan-ccp-vsa001*</codeph>)</li>
          <li>Execute the following
              command:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock><p>In
              the following example <b>offline-vsa</b> file is created with the VSA host
              name:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit @offline-vsa</codeblock></p><p>The
              node which is included in the offline-vsa is placed under maintenance.</p></li>
          <li>Run the following
              command:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname></codeblock><p>For
              example:</p><codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit @offline-vsa</codeblock><p>When
              performing Helion lifecycle management deployments, specify the name of this file in a
              --limit option to prevent the deployment process trying to access the VSA nodes that
              are offline.</p></li>
        </ol></p>
    </section>
  </body>
</topic>
