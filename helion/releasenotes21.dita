<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="release-notes-21">
    <title><keyword keyref="kw-hos-tm"/> <keyword keyref="kw-hos-version-21"/>: Release Notes</title>
    <body>
        
        <section id="about">
            <p>This document provides an overview of the features contained within
                <keyword keyref="kw-hos-tm"/> <keyword keyref="kw-hos-version-21"/>, 
                including known issues and workarounds for this release:</p>
            <ul>
                <li><xref href="releasenotes21.dita#release-notes-21/features">Features Available in this
                    Release</xref></li>
                <li><xref href="releasenotes21.dita#release-notes-21/issues_fixed">Issues Fixed in this
                    Release</xref></li>
                <li><xref href="releasenotes21.dita#release-notes-21/known_issues">Known Issues in this
                    Release</xref></li>
            </ul>
        </section>   
        
        <section id="features"><title>Features Available in this Release</title>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2316 -->
            <p id="DOCS-2316"><b>NIC hardware that provides multiple interfaces using a single PCI address is now supported.</b></p>
            
            <p><codeph>nic_mappings.yml</codeph> now supports multiport cards. See the 
                <xref href="input_model.dita#input_model/co_nicmappings">NIC Mappings</xref> section in the Input Model documentation.
            </p>


            <!--  https://jira.hpcloud.net/browse/DOCS-2321 -->
            <p id="DOCS-2321"><b>Backup and Restore GUI added to Horizon</b></p>
            
            <p>A cloud operator or administrator can schedule backups and perform restores via a GUI in Horizon. For more information,
                see the section on <xref href="bura/bura_overview.dita#bura_overview">Backing Up and Restoring Your Data</xref></p>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2331 -->
            <p id="DOCS-2331"><b>OVSvApp Install on VXLAN backed Tenant Networks</b></p>
            
            <p><keyword keyref="kw-hos-phrase-21"/> can automatically install OVSvApp agents on tenant networks that are backed by VXLAN.</p>

       
            <!--  https://jira.hpcloud.net/browse/DOCS-2342 -->
            <p id="DOCS-2342"><b>Ceph Multiple Networks</b></p>
            
            <p>For security and performance reasons, it is recommended that Ceph Storage Cluster runs with two networks - 
                a public (front-side) network and a cluster (back-side) network. <keyword keyref="kw-hos-phrase-21"/> 
                now supports this configuration.
            </p>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2343 -->
            <p id="DOCS-2343"><b>Linux for HPE Helion update</b></p>
            
            <p><keyword keyref="kw-hos-phrase-21"/>  contains an update for Linux for HPE Helion. This update requires nodes to be rebooted as per
                the instructions in the Upgrade documentation located <xref href="installation/upgrade.dita#upgradeto21">here</xref>.</p>
            
            <p>The update includes patches for CVE-2015-8104 and CVE-2015-5307.</p>
            
            
        <!--

            <p><b>Ceph Multi-network support</b></p>
            <p><b>hLinux: security updates via Top of Tree</b></p>
            <p><b>Mid-scale support</b></p>
            <p><b>~100 patches</b></p>
            <p><b>Includes all fixes in HOS V2.0.1 hotfix</b></p>

            <p><b>kernel update => managed reboots</b></p>-->
            
        </section>



        <section id="issues_fixed">
            <title>Issues Fixed in this Release</title>
            
            <!-- https://jira.hpcloud.net/browse/DOCS-2327 -->
            <p id="DOCS-2327"><b>The softlockup issue when attempting to remove many LUNs simultaneously has been fixed.</b></p>
            
            <p>See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1902">here</xref></p>

            <!--  https://jira.hpcloud.net/browse/DOCS-2257  -->
            <p id="DOCS-2257"><b>IPv6 access to <keyword keyref="kw-hos"/>  components is blocked</b></p>
            
            <p><keyword keyref="kw-hos-phrase-21"/> will upgrade your system to include firewall rules 
                preventing IPv6 access to <keyword keyref="kw-hos"/> components.</p>
            
            <p>                
                For <keyword keyref="kw-hos-phrase-20"/>, the release notes indicated IPv4 access to <keyword keyref="kw-hos"/> components 
                was restricted to known interfaces and services by firewall rules - 
                see <xref href="releasenotes.dita#release_notes/DOCS-1835">IPv6 Traffic Not Supported</xref>. 
                It also indicated IPv6 access was 
                not restricted in this way. The <keyword keyref="kw-hos"/> <keyword keyref="kw-hos-version-20"/> release notes provided instructions to disable IPv6 on your 
                operating system and a suggestion to disable IPv6 at your router. This is no longer necessary. 
                
            </p>
            <p> If you have previously disabled IPv6
                on your operating systems by adding the following lines to <b>/etc/sysctl.conf</b>:
<codeblock>
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</codeblock>
                you should  remove these lines and then either reboot or run the following command as root:
                <codeblock># sysctl -p</codeblock>
            </p>
            
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2252 -->
            <p id="DOCS-2252"><b>Rsync requests are no longer blocked by the firewall on systems with a dedicated Swift
                network</b></p>
            
            <p>In <keyword keyref="kw-hos-phrase-20"/>, it was necessary to add the <codeph>swift-rsync</codeph> 
                service to the <codeph>component-endpoints</codeph> in the
                <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file. This is not required in <keyword keyref="kw-hos-phrase-21"/>
                and can be removed if you are upgrading from <keyword keyref="kw-hos-phrase-20"/>.
            </p>
            
            <p>See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/> 
                <xref href="releasenotes.dita#release_notes/DOCS-2143">here</xref>.</p>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2154 -->
            <p id="DOCS-2154"><b>The Configuration Processor now encrypts Ansible output.</b></p>   
            
            <p>In <keyword keyref="kw-hos-phrase-20"/>, the Configuration Processor did not encrypt Ansible output if encryption was enabled. 
                This issue has been fixed in <keyword keyref="kw-hos-phrase-21"/>.
            </p>
            
            <p>
                If you ran the configuration processor with encryption turned on in <keyword keyref="kw-hos-phrase-20"/>,  you will need to delete the 
                git branch used to store the Ansible output and then recreate it in order to clear the unencrypted Ansible output.
            </p>
<codeblock>
cd helion
git branch -D ansible
git checkout -b ansible
git checkout site
</codeblock>            
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2053 -->
            <p id="DOCS-2053"><b>The behavior associated with the <codeph>curator_num_of_indicies_to_keep</codeph> configuration variable has changed in <keyword keyref="kw-hos-phrase-21"/>.</b></p>
            
            <p>The <codeph>curator_num_of_indicies_to_keep</codeph> configuration variable is specified in the 
                file <codeph>roles/logging-common/defaults/main.yml</codeph> and is used to control the number of 
                old log indices to keep. The default value has increased in <keyword keyref="kw-hos-phrase-21"/> from 3 to 7.</p>
            <p>Log indices are now automatically deleted once their number exceeds <codeph>curator_num_of_indicies_to_keep</codeph>. 
                Logs are always deleted in reverse chronological order so that there are at most 
                <codeph>curator_num_of_indicies_to_keep</codeph> indices available. In the case where the high watermark disk space 
                has been exceeded, logs will be removed in reverse chronological order until the disk space is again below the high watermark.
                This measn that the number of indices available may be less than the number specified in <codeph>curator_num_of_indicies_to_keep</codeph>.
            </p>
            
            <p>This functionality differs significantly from <keyword keyref="kw-hos"/> <keyword keyref="kw-hos-version-20"/> where indices were deleted based on size rather than age
                when the high watermark disk space was exceeded. In addition, <keyword keyref="kw-hos-phrase-20"/> did not strictly 
                enforce the <codeph>curator_num_of_indicies_to_keep</codeph> configuration value if the high watermark disk space was not exceeded
                so more indices than that number were allowed to exist.
            </p>
            
            
            <!-- https://jira.hpcloud.net/browse/DOCS-2323 -->
            <p id="DOCS-2323"><b>Threshold Engine is correctly deployed when there is a single Control Plane node.</b></p>
            
            <p>A workaround is no longer required to get the Threshold Engine to run when there is only a single Control Plane node. 
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-2056">here</xref></p>       
 
 
            <!--https://jira.hpcloud.net/browse/DOCS-2286-->
            <p id="DOCS-2286"><b>The 'Cinderlm diagnostics monitor' alarm no longer alerts with an 'UNDETERMINED'
                state</b></p>
            
            <p>
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1832">here</xref>   
            </p>
 
 
            <!--https://jira.hpcloud.net/browse/DOCS-2344-->
            <p id="DOCS-2344"><b>Moonshot remote console no longer goes blank after operating system install</b></p>
            
            <p>
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1881">here</xref>   
            </p>
            


            <!--https://jira.hpcloud.net/browse/DOCS-2345-->
            <p id="DOCS-2345"><b>Horizon no longer throws error when compute is not present</b></p>
            
            <p>
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1932">here</xref>   
            </p>

            <!--https://jira.hpcloud.net/browse/DOCS-2346-->
            <p id="DOCS-2346"><b>Monasca Forwarder Log no longer reports False Errors </b></p>
            
            <p>
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1850">here</xref>   
            </p>
            
            <!--https://jira.hpcloud.net/browse/DOCS-2277-->
            <p id="DOCS-2277"><b>VSA VM names now prefixed</b></p>
            
            <p>
                Prior to <keyword keyref="kw-hos-phrase-21"/>, all VSA VMs had the same name <codeph>vsa-hostname</codeph>. 
                With <keyword keyref="kw-hos-phrase-21"/>, the hostname of VSA VMs is dynamically generated based on the 
                node id in <codeph>servers.yml</codeph> but prefixed by VSA-VM. So, if <codeph>servers.yml</codeph> has a 
                VSA node of <codeph>vsa-1</codeph> then the hostname of the corresponding VSA VM will be <codeph>VSA-VM-vsa-1</codeph>."
                
            </p>           
       </section>



        <section id="known_issues">
            <title>Known Issues in this Release</title>
            
            
            <p><b>Upgrade prevents core HPE Helion Development Platform services from restarting</b></p>
            <p>The process for upgrading HPE Helion OpenStack to version 2.1 requires a reboot which may prevent 
                core HPE Helion Development Platform services from restarting. If your environment relies on 
                HPE Helion Development Platform, please refrain from upgrading HPE Helion OpenStack until the 
                complete HPE Helion Development Platform upgrade process is made available.</p>
            
            
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2052  -->
            <p id="DOCS-2052"><b>Changes to Alarm Definition in Centralized Logging 
                may require existing alarms to be deleted and recreated on upgrade from 
                <keyword keyref="kw-hos-version-20"/> to <keyword keyref="kw-hos-version-21"/></b></p>   
            
            <p>
                A new dimension <codeph>service=logging</codeph> has been added for alarm definitions.
                During an upgrade, you should delete exisiting alarms that won't have this dimension and 
                then recreate them, so that they will have the correct dimension and appear as expected 
                in the Ops Console "logging" bucket. Before performing an upgrade, remove the old logging alarms manually,
                specifically the "Process Detect" alarms for Apache, Beaver, Kibana and Logstash. 
                </p>
                        
              <p>
                If you do not perform this step, two sets of alarm metrics will be generated after the upgrade. 
                As a result, the metrics for the older definition might appear in the "logging" bucket while those for the 
                newer definition might appear in the Unknown or Other bucket.
              </p>
  
            <p>
                See the corresponding release note for <keyword keyref="kw-hos-phrase-20"/>
                <xref href="releasenotes.dita#release_notes/DOCS-1929">here</xref>   
            </p> 
            <!--  https://jira.hpcloud.net/browse/DOCS-2218  -->
            <p id="DOCS-2218"><b>Nova services may be down after upgrade from  <keyword keyref="kw-hos"/>  
                <keyword keyref="kw-hos-version-20"/> to <keyword keyref="kw-hos-version-21"/></b>
                
            </p>
           
            <p>Afer upgrading, the nova services on a controller node may be down - if you list the services, you may see output as follows:
            
<codeblock>
stack@hlm:~/gavin$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:32.000000 | -               |
| 7  | nova-conductor   | hlm002-cp1-c1-m3-mgmt    | internal | enabled | down  | 2015-10-27T15:03:43.000000 | -               |
| 13 | nova-scheduler   | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:33.000000 | -               |
| 16 | nova-conductor   | hlm002-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-10-27T15:11:32.000000 | -               |
| 19 | nova-scheduler   | hlm002-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-10-27T15:11:35.000000 | -               |
| 22 | nova-scheduler   | hlm002-cp1-c1-m3-mgmt    | internal | enabled | down  | 2015-10-27T15:03:45.000000 | -               |
| 25 | nova-consoleauth | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:35.000000 | -               |
| 28 | nova-compute     | hlm002-cp1-comp0003-mgmt | nova     | enabled | down  | 2015-10-27T15:10:22.000000 | -               |
| 31 | nova-compute     | hlm002-cp1-comp0002-mgmt | nova     | enabled | down  | 2015-10-27T15:10:13.000000 | -               |
| 34 | nova-compute     | hlm002-cp1-comp0001-mgmt | nova     | enabled | down  | 2015-10-27T15:10:13.000000 | -               |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
</codeblock>
            This issue resolves itself, typically after approximately 20 minutes, but if  you wish to resolve it immediately, 
            restart nova on the affected node:
            
 <codeblock>
ansible-playbook -i hosts/verb_hosts nova-stop.yml --limit hlm002-cp1-c1-m3-mgmt
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit hlm002-cp1-c1-m3-mgmt    
</codeblock>           
                
            </p>

            <!--  https://jira.hpcloud.net/browse/DOCS-2269 -->
            <p id="DOCS-2269"><b>Multipath attach over iSCSI is not supported for 3PAR backend storage</b></p>
            
            <p><keyword keyref="kw-hos-phrase-21"/> does not support Multipath attach over iSCSI for 3PAR backend storage.</p>
            
            
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2329 -->
            <p id="DOCS-2329"><b>Ceph logs on monitor nodes are not being pushed to Kibana for centralized logging</b></p>

            <p>Logs from monitor nodes cannot be pushed to Kibana due to a lack of read permissions on the files. 
                
                As a workaround, you can execute a cron job that will run every thirty minutes and will fixup the permissions 
                so that the logs can be copied.  Execute <codeph>crontab -e</codeph> and add the following line 
                (replacing &lt;ceph-clustername&gt; with the actual name of Ceph cluster): 
            </p>
<codeblock>
*/30 * * * * chmod 0644 /var/log/ceph/&lt;ceph-clustername>.log    
</codeblock>
    
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2330 -->
            <p id="DOCS-2330"><b>wipe_disks.yml can fail on Ceph nodes</b></p>
            
            <p>Running the playbook <codeph>wipe_disks.yml</codeph> can fail on Ceph nodes. As a workaround, you can 
                <codeph>ssh</codeph> to the target node and run:</p>
<codeblock>
/sbin/sgdisk --zap-all -- /dev/sdb    
</codeblock>
 
            <!--  https://jira.hpcloud.net/browse/DOCS-2311 -->
            <p id="DOCS-2311"><b>Failure to restart VPNaaS due to ipsec issue after re-boot</b></p>
            
            <p> When the controller node (or network node) running ipsec process from Openswan is 
                rebooted, it is observed that the ipsec process fails to start by default. In such an instance, 
                to get VPNaaS working, the following commands need to be executed after the system reboot.
             </p>
<codeblock>
sudo service ipsec start
sudo service neutron-vpn-agent restart
</codeblock>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2281 -->
            <p id="DOCS-2281"><b>InstanceResourceQuota in Nova not supported</b></p> 
            
            <p><keyword keyref="kw-hos-phrase-21"/> does not support InstanceResourceQuota in Nova.</p>
            
            
        </section>
        
    </body>
</topic>
