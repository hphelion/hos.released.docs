<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="example_configurations">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Example Configurations</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <note type="attention">Hyperlinks intermittently do not work in the Google Chrome browser. <xref
        href="http://docs.hpcloud.com/helion/example_configurations.html" scope="external"
        format="html">Click here</xref> for a frameless version of this page where the links should
      work.</note>
    <p>The <keyword keyref="kw-hos-phrase"/> system ships with a collection of pre-qualified example
      configurations. These are designed to help you to get up and running quickly with a minimum
      number of configuration changes.</p>
    <p>The <keyword keyref="kw-hos"/> input model allows a wide variety of configuration parameters
      that may, at first glance, appear daunting. The example configurations are designed to
      simplify this process by providing pre-built and pre-qualified examples that need only a
      minimum number of modifications to get started.</p>
    <section id="contents">
      <ul>
        <li><xref href="example_configurations.dita#example_configurations/example_configs">
            <keyword keyref="kw-hos"/> Example Configurations</xref>
          <ul>
            <li><xref href="example_configurations.dita#example_configurations/entryscale_kvm_vsa"
                >Entry-Scale KVM with VSA Model</xref></li>
            <li><xref
                href="example_configurations.dita#example_configurations/entryscale_kvm_dedicated"
                >Entry-scale KVM with VSA model with Dedicated Cluster for Metering, Monitoring, and
                Logging</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/entryscale_esx"
                >Entry-Scale ESX Model</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/entryscale_swift"
                >Entry-Scale Swift Model</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/entryscale_ceph"
                >Entry-Scale KVM with Ceph Model</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/midscale_kvm_vsa"
              >Mid-Scale KVM with VSA Model</xref></li>
          </ul>
        </li>
    
         <li><xref href="example_configurations.dita#example_configurations/alternative"
              >Alternative Configurations</xref>
            <ul>
            <li><xref
                href="example_configurations.dita#example_configurations/entryscale_ceph_multinetwork"
                >Entry-Scale KVM with Ceph Model with Two Networks</xref>
              <ul>
                <li><xref href="example_configurations.dita#example_configurations/ceph_nicmappings"
                    >Nic_mappings.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_netinterfaces"
                    >Net_interfaces.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_networksgroups"
                    >Network_groups.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph_networks"
                    >Netwprks.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_servergroups"
                    >Server_groups.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/ceph_firewallrules"
                    >Firewall_rules.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/ceph_readme">Edit
                    the README.html and README.md Files</xref></li>
              </ul>
            </li>
            </ul>
         </li>
      
        <li><xref
            href="example_configurations.dita#example_configurations/modify_entryscale_kvm_vsa"
            >Modifying the Entry-scale KVM with VSA Model for Your Environment</xref>
          <ul>
            <li><xref
                href="example_configurations.dita#example_configurations/localizing_inputmodel"
                >Localizing the Input Model</xref>
              <ul>
                <li><xref href="example_configurations.dita#example_configurations/networks"
                    >Networks.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/nicmappings"
                    >Nic_mappings.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/netinterfaces"
                    >Net_interfaces.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/networkgroups"
                    >Network_groups.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/servers"
                    >Servers.yml</xref></li>
              </ul></li>
            <li><xref
                href="example_configurations.dita#example_configurations/customizing_inputmodel"
                >Customizing the Input Model</xref>
              <ul>
                <li><xref href="example_configurations.dita#example_configurations/disks_controller"
                    >Disks_controller.yml</xref>
                  <ul>
                    <li><xref href="example_configurations.dita#example_configurations/filesystems"
                        >File Systems Storage</xref></li>
                    <li><xref href="example_configurations.dita#example_configurations/swiftstorage"
                        >Swift Storage</xref></li>
                  </ul></li>
                <li><xref href="example_configurations.dita#example_configurations/disks_vsa"
                    >Disks_vsa.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/disks_compute"
                    >Disks_compute.yml</xref></li>
              </ul></li>
            <li><xref href="example_configurations.dita#example_configurations/standalone">Using a
                Standalone Lifecycle-Manager Node</xref>
              <ul>
                <li><xref
                    href="example_configurations.dita#example_configurations/control_plane_yml"
                    >Control_plane.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/server_roles_yml"
                    >Server_roles.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/net_interfaces_yml"
                    >Net_interfaces.yml</xref></li>
                <li><xref
                    href="example_configurations.dita#example_configurations/disks_lifecycle_manager_yml"
                    >Disks_lifecycle_manager.yml</xref></li>
                <li><xref href="example_configurations.dita#example_configurations/servers_yml"
                    >Servers.yml</xref></li>
              </ul></li>
            <li><xref href="example_configurations.dita#example_configurations/without_dvr"
                >Configuring <keyword keyref="kw-hos"/> without DVR</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/without_l3agent"
                >Configuring <keyword keyref="kw-hos"/> with Provider VLANs and Physical Routers
                Only</xref></li>
            <li><xref href="example_configurations.dita#example_configurations/twosystems"
                >Considerations When Installing Two Systems on One Subnet</xref></li>
          </ul></li>

      </ul>
    </section>
    <section id="example_configs"><title><keyword keyref="kw-hos"/> Example Configurations</title>
      <p>This section briefly describes the various example configurations and their capabilities.
        It also describes in detail, for the entry-scale-kvm-vsa example, how you can adapt the
        input model to work in your environment.</p>
      <p><keyword keyref="kw-hos-phrase"/> ships with two classes of sample cloud models: examples
        and tech-preview. The models in the examples directory have been qualified by our Quality
        Engineering team, while the tech-preview models are more experimental.</p>
      <p>The following pre-qualified examples are shipped with <keyword keyref="kw-hos-phrase"
        />:</p>
      <ul>
        <li>Entry-scale KVM with VSA model (entry-scale-kvm-vsa)</li>
        <li>Entry-scale KVM with VSA model with Dedicated Cluster for Metering, Monitoring, and
          Logging (entry-scale-kvm-vsa-mml)</li>
        <li>Entry-scale Swift-only model (entry-scale-swift)</li>
        <li>Entry-scale KVM with Ceph model (entry-scale-kvm-ceph)</li>
        <li>Entry-scale ESX model (entry-scale-esx)</li>
        <li>Mid-scale KVM with VSA model (mid-scale-kvm-vsa)</li>
      </ul>
      <p>The entry-scale systems are designed to provide an entry-level solution that can be scaled
        from a small number of nodes to a moderately high node count (approximately 100 compute
        nodes, for example).</p>
      <p>In the mid-scale model, the cloud control plane is subdivided into a number of dedicated
        service clusters to provide more processing power for individual control plane elements.
        This enables a greater number of resources to be supported (compute nodes, Swift object
        servers). This model also shows how a segmented network can be expressed in the <keyword
          keyref="kw-hos"/> model.</p>
    </section>
    <section id="entryscale_kvm_vsa"><title>Entry-Scale KVM with VSA Model</title>
      <p>This model provides a KVM-based cloud with VSA for volume storage, and has been tested to a
        scale of 100 compute nodes.</p>
      <p>The example is focused on the minimum server count to support a highly-available (HA)
        compute cloud deployment. The first (manually installed) server, often referred to as the
        deployer or lifecycle manager, is also used as one of the controller nodes. This model
        consists of a minimum server count of seven, with three controllers, three VSA storage
        servers, and one compute server. Swift storage in this example is contained on the
        controllers.</p>
      <p>Note that the VSA storage requires a minimum of three servers for a HA configuration,
        although the deployment will work with as little as one VSA node.</p>
      <p>This model can also be deployed without the VSA servers and configured to use an external
        storage device, such as a 3PAR array, which would reduce the minimum server count to
        four.</p>
      <p><image href="../media/examples/entry_scale_kvm_vsa.png"/></p>
      <p><xref href="../media/examples/entry_scale_kvm_vsa_lg.png" scope="external" format="html"
          >Download a high-resolution version</xref></p>
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud.</li>
        <li><b>External VM</b> - This is the network that will be used to provide access to virtual
          machines (via floating IP addresses).</li>
        <li><b>Guest/VxLAN</b> - This is the network that will carry traffic between virtual
          machines on private networks within the cloud.</li>
        <li><b>Management</b> - This is the network that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram, "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API and External VM networks. Note that the EXTERNAL_API
        network must be reachable from the EXTERNAL_VM network if you want virtual machines to be
        able to make API calls to the cloud. "Internal Routing" refers to whatever routing you want
        to provide to allow administrators to access the Management network.</p>
      <p>If you are using <keyword keyref="kw-hos"/> to install the operating system, then an
        IPMI/iLO network connected to the IPMI/iLO ports of all servers and routable from the
        lifecycle manager server is also required for BIOS and power management of the nodes during
        the operating system installation process.</p>
      <p>The example uses the following disk configurations:</p>
      <ul>
        <li><b>Controllers</b> - One operating system disk and two disks for Swift storage.</li>
        <li><b>VSA</b> - One operating system disk and two disks for VSA storage.</li>
        <li><b>Compute</b> - One operating system disk and one disk for virtual machine ephemeral
          storage.</li>
      </ul>
      <p>For details about how to modify this example to match your environment, see <xref
          href="example_configurations.dita#example_configurations/modify_entryscale_kvm_vsa"
          >Modifying the Entry-scale KVM with VSA model for your Environment</xref>.</p>
    </section>

    <section id="entryscale_kvm_dedicated"><title>Entry-scale KVM with VSA model with Dedicated
        Cluster for Metering, Monitoring, and Logging</title>
      <p>This model is a variant of the previous Entry-Scale KVM with VSA model. It is designed to
        support greater levels of metering, monitoring, and logging.</p>
      <ul>
        <li>Metering - All meters required to support charge-back/show-back for core Infrastructure
          as a Service (IaaS) elements.</li>
        <li>Logging - Run all services at INFO level with the ability to change the settings to
          DEBUG in order to triage specific error conditions. Minimum retention for logs is 30 days
          to satisfy audit and compliance requirements.</li>
        <li>Monitoring - Full performance metrics and health checks for all services.</li>
      </ul>
      <p>In order to provide increased processing power for these services, the following
        configuration changes are made to the control plane in this model:</p>
      <ul>
        <li>All services associated with metering, monitorign, and logging run on a dedicated
          three-node cluster. Three nodes are required for high availability with quorum.</li>
        <li>A dedicated three node cluster is used for RabbitMQ message queue and database services.
          This cluster is also used to provide additional processing for the message queue and
          database load associated with the additional metering, monitoring, and logging load. Three
          nodes are required for high availability with quorum.</li>
        <li>The main API cluster is reduced to two nodes. These services are stateless and do not
          require a quorum node for high availability.</li>
      </ul>
      <p>This diagram below illustrates the physical networking used in this configuration.</p>
      <p><image href="../media/hos.docs/entry_scale_kvm_vsa_ext.png"/></p>
      <p><xref href="../media/hos.docs/entry_scale_kvm_vsa_ext_lg.png" scope="external"
          format="html">Download a high-resolution version</xref></p>
    </section>

    <section id="entryscale_esx"><title>Entry-Scale ESX Model</title>
      <p>This example shows how to integrate <keyword keyref="kw-hos"/> with ESX. The controller
        configuration is essentially the same as in the KVM example, but the resource nodes are
        provided by vCenter. In addition, a number of controller virtual machines are created for
        each vCenter cluster: one ESX Compute virtual machine (which provides the nova-compute proxy
        for vCenter) and one OVSvApp virtual machine per cluster member (which provides network
        access). These virtual machines are created automatically by <keyword keyref="kw-hos"/> as
        part of activating the vCenter cluster, and are therefore not defined in the example.</p>
      <p><image href="../media/hos.docs/entry_scale_esx.png"/></p>
      <p><xref href="../media/hos.docs/entry_scale_esx_lg.png" scope="external" format="html"
          >Download a high-resolution version</xref></p>
      <p>The physical networking configuration is also largely the same as the KVM example, with the
        exception of the GUEST network which uses tenant VLANs as the Neutron networking model
        rather than VxLAN.</p>
      <p>A separate configuration network (CONF) is required for configuration access from the
        lifecycle manager. This network must be reachable from the Management network.</p>
    </section>
    <section id="entryscale_swift"><title>Entry-Scale Swift Model</title>
      <p>This example shows how <keyword keyref="kw-hos"/> can be configured to provide a Swift-only
        configuration, consisting of three controllers and one or more Swift object servers.</p>
      <p><image href="../media/examples/entry_scale_swift.png"/></p>
      <p><xref href="../media/examples/entry_scale_swift_lg.png" scope="external" format="html"
          >Download a high-resolution version</xref></p>
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud.</li>
        <li><b>Swift</b> - This is the network that will be used for all data traffic between the
          Swift services.</li>
        <li><b>Management</b> - This is the network that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API. "Internal Routing" refers to whatever routing you want to
        provide to allow administrators to access the Management network.</p>
      <p>If you are using <keyword keyref="kw-hos"/> to install the operating system, then an
        IPMI/iLO network connected to the IPMI/iLO ports of all servers and routable from the
        lifecycle manager is also required for BIOS and power management of the node during the
        operating system installation process.</p>
      <p>In the example the controllers use one disk for the operating system and two disks for
        Swift proxy and account storage. The Swift object servers use one disk for the operating
        system and four disks for Swift storage. These values can be modified to suit your
        environment.</p>
    </section>
    <section id="entryscale_ceph"><title>Entry-Scale KVM with Ceph Model</title>
      <p>This example provides a KVM-based cloud using Ceph for volume storage. This controller and
        compute configuration are essentially the same as in the KVM example, but the backend Cinder
        storage is provided by Ceph.</p>
      <p><image href="../media/examples/entry_scale_kvm_ceph.png"/></p>
      <p><xref href="../media/examples/entry_scale_kvm_ceph_lg.png" scope="external" format="html"
          >Download a high-resolution version</xref></p>
    </section>
    <section id="midscale_kvm_vsa"><title>Mid-Scale KVM with VSA Model </title>
      <p>The mid-scale model illustrates two important aspects of configuring <keyword
        keyref="kw-hos"/> for increased scale. The controller services are distributed across a
        greater number of controllers and a number of the networks are configured as multiple L3
        segments (implementing per-rack networking).</p>
      <p><image href="../media/examples/mid_scale_kvm_vsa.png"/></p>
      <p><image href="../media/examples/mid_scale_kvm_vsa_notes.png"/></p>
      <p><xref href="../media/examples/mid_scale_kvm_vsa_lg.png" scope="external" format="html"
        >Download a high-resolution version</xref></p>
      <p>The distribution of services across controllers is only one possible configuration, and
        other combinations can also be expressed.</p>
    </section>
    
    <section id="alternative"><title>Alternative Configurations</title>
      <p>In <keyword keyref="kw-hos-phrase"/> there are alternative configurations that we recommend for specific purposes and this section we will outline them.</p>
    </section>
    
    <section id="entryscale_ceph_multinetwork"><title>Entry-Scale KVM with Ceph Model with Two
        Networks</title>
      <p>HPE Helion OpenStack Ceph is a unified storage system for various storage use cases for an
        OpenStack-based cloud. It is highly reliable, easy to manage, and horizontally scalable as
        demand grows.</p>
      <p>Ceph clients directly talk to OSD daemons for storage operations instead of client routing
        the request to a specific gateway as is commonly found in other storage solutions. OSD
        daemons perform data replication and participate in recovery activities. In general, a pool
        is configured with a replica count of three, causing daemons to transact three times the
        amount of client data over the cluster network. So, every 4 MB of write data is likely to
        result in 12 MB of data movement across Ceph clusters. Considering this network traffic, it
        is important to segregate Ceph data traffic, which can be primarily categorized into three
        segments:</p>
      <ul>
        <li><b>Management traffic</b> - primarily includes all admin related operations such as pool
          creation, crush map modification, user creation, etc.</li>
        <li><b>Client (data) traffic</b> - primarily includes client requests sent to OSD
          daemons.</li>
        <li><b>Cluster (replication) traffic</b> - primarily includes replication and recovery data
          traffic among OSD daemons.</li>
      </ul>
      <p>For a high performing cluster, the network configuration is important. Segregating the data
        traffic using multiple networks allows for this. For medium-size production environments we
        recommend to have a cluster with at least two networks: a client data network (front-side)
        and a cluster (back-side) network. For larger production environments we recommend that you
        segregate all three network traffic types by utilizing three networks. This particular
        document shows you how to setup two networks but you can use the same principles to create
        three networks.</p>
      <p>Also, segregating networks provides additional security as well because your cluster
        network does not need to be connected to the internet directly. This helps in preventing
        spoof attacks and allows the OSD daemons to keep communicating without intervention so that
        placement groups can be brought to active + clean state whenever required.</p>
      <p>This model is a variant of the Entry-Scale KVM with Ceph model. It is designed with two
        VLANs: a public (front-side) network and a cluster (back-side) network. This enables more
        options in regards to scaling.</p>
      <p>This model uses the following components:<ul id="ul_ckz_3tl_s5">
          <li>Three controller nodes, one KVM compute node, and three Ceph OSD nodes.</li>
          <li>The Ceph monitor component of the Ceph cluster is deployed on the controller nodes
            along with other OpenStack service components. This limits your cloud to three monitor
            nodes which should be suitable for most production environments.</li>
          <li>Allows two VLANs (i.e. management VLAN and OSD VLAN) which segregates Ceph client
            traffic from Ceph cluster traffic. The management network will be used to carry cloud
            management data, such as RabbitMQ, HOPS, and database traffic, Ceph management data,
            such as pool creation, as well as client data traffic, such as cinder-volume writing
            blocks to Ceph storage pools. The Ceph cluster network will be dedicated for OSD daemons
            and will be used to carry replication traffic.</li>
          <li>A single compute node is initially provided with this example configuration. If
            additional compute capacity is required then further compute nodes can be added to the
            configuration by adding more nodes to the compute resource plane. The same applies to
            OSD nodes as well. Three OSD nodes are initially provided with this example
            configuration. If additional OSD capacity is required then further OSD nodes can be
            added to the configuration by adding more nodes to the OSD resource plane.</li>
        </ul></p>
      <p>This diagram below illustrates the physical networking used in this configuration.</p>
      <p><image href="../media/hos.docs/entry_scale_kvm_ceph_two_network.png"/></p>
      <p><xref href="../media/hos.docs/entry_scale_kvm_ceph_two_network_lg.png" scope="external"
          format="html">Download a high-resolution version</xref></p>
      <p>This configuration is baesd on the entry-scale-kvm-ceph cloud input model which is included
        with the HPE Helion OpenStack distro. You will need to make the changes outlined below prior
        to the deployment of your Ceph cluster with two networks. Note that if you already have a
        Ceph cluster deployed with a single network these steps cannot be used to migrate to a
        dual-network setup. The recommendation in these cases will be that you make a clean
        installation which will result in the loss of your existing data unless you make
        arrangements to have it backed up beforehand.</p>
    </section>
    <section id="ceph_nicmappings"><title>Nic_mappings.yml</title>
      <p>Ensure that your baremetal server NIC interfaces are correctly specified in the
          <codeph>~/helion/my_cloud/definition/data/nic_mappings.yml</codeph> file and that they
        meet the server requirements.</p>
    </section>
    <section id="ceph_netinterfaces"><title>Net_interfaces.yml</title>
      <p>Define a new interface set for your OSD interfaces in the
          <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file.</p>
      <p>Ensure that the appropriate NIC is configured to both the Management and OSD network
        groups, indicated below:</p>
      <codeblock>- name: OSD-INTERFACES
 network-interfaces:
   - name: ETH3
     device:
        name: hed3
     network-groups:
        - MANAGEMENT
   - name: ETH4
     device:
        name: bed4
     network-groups:
        - OSD</codeblock>
    </section>
    <section id="ceph_networksgroups"><title>Network_groups.yml</title>
      <p>Define the OSD network group in the
          <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file:</p>
      <codeblock>#
# OSD
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD
  hostname-suffix: osd

  component-endpoints:
    - ceph-osd-internal</codeblock>
    </section>
    <section id="ceph_networks"><title>Networks.yml</title>
      <p>Define the OSD VLAN in the <codeph>~/helion/my_cloud/definition/data/networks.yml</codeph>
        file:</p>
      <codeblock>
- name: OSD-NET
  vlanid: 112
  tagged-vlan: false
  cidr: 10.0.1.0/24
  gateway-ip: 10.0.1.1
  network-group: OSD</codeblock>
    </section>
    <section id="ceph_servergroups"><title>Server_groups.yml</title>
      <p>Add the OSD network to the server groups in the
          <codeph>~/helion/my_cloud/definition/data/server_groups.yml</codeph> file, indicated by
        the bold portion below:</p>
      <codeblock>- name: CLOUD
 server-groups:
   - AZ1
   - AZ2
   - AZ3
 networks:
   - EXTERNAL-API-NET
   - EXTERNAL-VM-NET
   - GUEST-NET
   - MANAGEMENT-NET
   <b>- OSD-NET</b></codeblock>
    </section>
    <section id="ceph_firewallrules"><title>Firewall_rules.yml</title>
      <p>Modify the firewall rules in the
          <codeph>~/helion/my_cloud/definition/data/firewall_rules.yml</codeph> file to allow OSD
        nodes to be pingable via the OSD network, indicated by the bold portion below:</p>
      <codeblock>
- name: PING
  network-groups:
  - MANAGEMENT
  - GUEST
  - EXTERNAL-API
  <b>- OSD</b>
  rules:
  # open ICMP echo request (ping)
  - type: allow
    remote-ip-prefix:  0.0.0.0/0
    # icmp type
    port-range-min: 8
    # icmp code
    port-range-max: 0
    protocol: icmp</codeblock>
    </section>
    <section id="ceph_readme"><title>Edit the README.html and README.md Files</title>
      <p>You can edit the <codeph>~/helion/my_cloud/definition/README.html</codeph> and
          <codeph>~/helion/my_cloud/definition/README.md</codeph> files to reflect the OSD network
        group information if you wish. This change does not have any semantic implication and only
        assists with the readability of your model.</p>
    </section>

    <section id="entryscale_kvm_ceph_threenetwork"><title>Entry-Scale KVM with Ceph Model with Three Networks</title>


</section>
    
    

    
    <section id="modify_entryscale_kvm_vsa"><title>Modifying the Entry-Scale KVM with VSA Model for
        Your Environment</title>
      <p>This section covers the changes that need to be made to the input model to deploy and run
        this cloud model in your environment.</p>
      <p>This section is written from the perspective of the <codeph>entry-scale-kvm-vsa</codeph>
        example, although the same principles apply to all of the examples.</p>
      <p>There are two categories of modifications that we will look at:</p>
      <ol>
        <li><b>Localizations</b> - These are the minimum set of changes that you need to make to
          adapt the examples to run in your environment. These are mostly concerned with
          networking.</li>
        <li><b>Customizations</b> - These describe more general changes that you can make to your
          model, e.g. changing disk storage layouts.</li>
      </ol>
      <p>Note that, as a convention, the examples use upper case for the object names, but these
        strings are only used to define the relationships between objects and have no specific
        significance to the configuration processor. You can change the names to values that are
        relevant to your context providing you do so consistently across the input model.</p>
    </section>
    <section id="localizing_inputmodel"><title>Localizing the Input Model</title>
      <p>This section covers the minimum set of changes needed to localize the cloud for your
        environment. This assumes you are using other features of the example unchanged:</p>
      <ul>
        <li>Update <codeph>networks.yml</codeph> to specify the network addresses (VLAN IDs and CIDR
          values) for your cloud.</li>
        <li>Update <codeph>nic_mappings.yml</codeph> to specify the PCI bus information for your
          servers' Ethernet devices.</li>
        <li>Update <codeph>net_interfaces.yml</codeph> to provide network interface configurations,
          such as bond settings and bond devices.</li>
        <li>Update <codeph>network_groups.yml</codeph> to provide the public URL for your cloud and
          to provide security certificates.</li>
        <li>Update <codeph>servers.yml</codeph> to provide information about your servers.</li>
      </ul>
    </section>
    <section id="networks">
      <title>Networks.yml</title>
      <p>You will need to allocate site specific CIDRs and VLANs for these networks and update these
        values in the <codeph>networks.yml</codeph> file. The example models define the following
        networks:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_xrn_zzy_st">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>Network</entry>
              <entry>CIDR</entry>
              <entry>VLAN ID</entry>
              <entry>Tagged / Untagged</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>External API</entry>
              <entry>10.0.1.0/24</entry>
              <entry>101</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>External VM</entry>
              <entry>Addresses configured by Neutron, leave blank in the file.</entry>
              <entry>102</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Guest</entry>
              <entry>10.1.1.0/24</entry>
              <entry>103</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Management</entry>
              <entry>192.168.10.0/24</entry>
              <entry>100</entry>
              <entry>Untagged</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>You will need to edit this file to provide your local values for these networks.</p>
      <p>The CIDR for the External VM network is configured separately using the Neutron API. (For
        instructions, see <xref href="administration/create_extnet.dita"/>.) You will only specify
        its VLAN ID during the installation process.</p>
      <p>The Management network is shown as untagged. This is required if you are using this network
        to PXE install the operating system on the cloud nodes.</p>
      <p>The example <codeph>networks.yml</codeph> file is shown below. Modify the bolded fields to
        reflect your site values.</p>
      <codeblock>networks:
   #
   # This example uses the following networks
   #
   # Network       CIDR             VLAN
   # -------       ----             ----
   # External API  10.0.1.0/24      101 (tagged)
   # External VM   see note 1       102 (tagged)
   # Guest         10.1.1.0/24      103 (tagged)
   # Management    192.168.10.0/24  100 (untagged)
   #	
   # Notes:
   # 1. Defined as part of Neutron configuration
   #
   # Modify these values to match your environment
   #
   - name: EXTERNAL-API-NET
     vlanid: <b>101</b>
     tagged-vlan: true
     cidr: <b>10.0.1.0/24</b>
     gateway-ip: <b>10.0.1.1</b>
     network-group: EXTERNAL-API
        
   - name: EXTERNAL-VM-NET
     vlanid: <b>102</b>
     tagged-vlan: true
     network-group: EXTERNAL-VM
        
   - name: GUEST-NET
     vlanid: <b>103</b>
     tagged-vlan: true
     cidr: <b>10.1.1.0/24</b>
     gateway-ip: <b>10.1.1.1</b>
     network-group: GUEST
        
   - name: MANAGEMENT-NET
     vlanid: 100
     tagged-vlan: false
     cidr: <b>192.168.10.0/24</b>
     gateway-ip: <b>192.168.10.1</b>
     network-group: MANAGEMENT</codeblock>
    </section>
    <section id="nicmappings"><title>Nic_mappings.yml</title>
      <p>This file maps Ethernet port names to specific bus slots. Due to inherent race conditions
        associated with multiple PCI device discovery there is no guarantee that Ethernet devices
        will be named as expected by the operating system, and it is possible that different port
        naming will exist on different servers with the same physical configuration.</p>
      <p>To provide a deterministic naming pattern, the input model supports an explicit mapping
        from PCI bus address to a user specified name. <keyword keyref="kw-hos"/> uses the prefix
          <b>hed</b> (Helion Ethernet Device) to name such devices to avoid any name clashes with
        the <b>eth</b> names assigned by the operating system.</p>
      <p>The example <codeph>nic_mappings.yml</codeph> file is shown below.</p>
      <codeblock>nic-mappings:
        
   - name: HP-DL360-4PORT
     physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"
        
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"
        
    - name: MY-2PORT-SERVER
      physical-ports:
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:04:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:04:00.1"</codeblock>
      <p>This defines two sets of NIC mappings, representing two different physical server types.
        The name of each mapping is used as a value in the <codeph>servers.yml</codeph> file to
        associate each server with its required mapping. This enables the use of different server
        models or servers with different network hardware.</p>
      <p>Each mapping lists a set of ports with the following information:</p>
      <ul>
        <li><b>Logical name</b> - <keyword keyref="kw-hos"/> uses the form
          <codeph>hedN</codeph>.</li>
        <li><b>Type</b> - Only simple-port types are supported in <keyword keyref="kw-hos-phrase"
          />.</li>
        <li><b>Bus-address</b> - The PIC bus address of the port.</li>
      </ul>
      <p>The PCI bus address can be found using the <codeph>lspci</codeph> command on one of the
        servers. This command can produce a lot of output, so you can use the following command
        which will limit the output to list Ethernet class devices only:</p>
      <codeblock>sudo lspci -D |grep -i net</codeblock>
      <p>Here is an example output:</p>
      <codeblock>$ sudo lspci -D |grep -i net
0000:02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.2 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.3 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:04:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
0000:04:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</codeblock>
      <p>To localize this file, replace the mapping names with the names of your choice and
        enumerate the ports as required.</p>
    </section>
    <section id="netinterfaces"><title>Net_interfaces.yml</title>
      <p>This file is used to define how the network interfaces are to be configured. The example
        reflects the slightly different configuration of controller, compute nodes, and VSA
        nodes.</p>
      <p>If network bonding is to be used, this file specifies how bonding is to be set up. It also
        specifies which networks are to be associated with each interface.</p>
      <p>The example uses a bond of interfaces <codeph>hed3</codeph> and <codeph>hed4</codeph>. You
        only need to modify this file if you have mapped your physical ports to different names, or
        if you need to modify the bond options.</p>
      <p>The section of configuration file is shown below, which will create a bonded interface
        using the named <codeph>hed3</codeph> and <codeph>hed4</codeph> NIC mappings described in
        the previous section.</p>
      <codeblock>
    - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
          provider: linux
          devices:
              - name: hed3
              - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT</codeblock>
      <p>If your system cannot support bonding, then you can modify this specification to specify a
        non-bonded interface, for example using device <codeph>hed3</codeph>:</p>
      <codeblock>
   - name: CONTROLLER-INTERFACES
     network-interfaces:
        - name: hed3
          device:
             name: hed3
          network-groups:
             - EXTERNAL-API
             - EXTERNAL-VM
             - GUEST
             - MANAGEMENT</codeblock>
    </section>
    <section id="networkgroups">
      <title>Network_groups.yml</title>
      <p>This file defines the networks groups used in your cloud. A network-group defines the
        traffic separation model, and all of the properties that are common to the set of L3
        networks that carry each type of traffic. They define where services and load balancers are
        attached to the network model and the routing within that model.</p>
      <p>In this example, the following network groups are defined:</p>
      <ul>
        <li><b>EXTERNAL-API</b> - This network group is used for external IP traffic to the cloud.
          In addition, it defines: <ul>
            <li>The characteristics of the load balancer to be used for the external API.</li>
            <li>The Transport Layer Security (TLS) attributes.</li>
          </ul></li>
        <li><b>EXTERNAL-VM</b> - Floating IPs for virtual machines are created on this network
          group. This is identified by the tag value
            <codeph>neutron.l3_agent.external_network_bridge</codeph>.</li>
        <li><b>GUEST</b> - Tenant VxLAN traffic is carried on this network group. This is identified
          by the tag value <codeph>neutron.networks.vxlan</codeph>.</li>
        <li><b>MANAGEMENT</b> - This is the default network group for traffic between service
          components in the cloud. In addition, it defines: <ul>
            <li>An internal load balancer is defined on this network group for managing internal and
              administrative API requests.</li>
          </ul></li>
      </ul>
      <p>Most of the values in this file should be left unmodified if you are using the network
        model defined by the example. More complex modifications are supported but are outside the
        scope of this document.</p>
      <p>However, the values related to the external API network are site-specific and need to be
        modified:</p>
      <ul>
        <li>Provide an external URL for the cloud.</li>
        <li>Provide the name of the security certificate to use.</li>
      </ul>
      <p>The example <codeph>network_groups.yml</codeph> file is shown below, modify the bolded
        fields to reflect your site values.</p>
      <codeblock>
   # External API
   #
   # This is the network group that users will use to
   # access the public API endpoints of your cloud
   #
   - name: EXTERNAL-API
     hostname-suffix: extapi
        
     load-balancers:
       - provider: ip-cluster
         name: extlb
         # If external-name is set then public urls in keystone
         # will use this name instead of the IP address
         # You must either set this to a name that can be resolved
         # in your network 
         # or comment out this line to use IP addresses
         <b>external-name</b>:
        
         <b>tls-components</b>:
            - default
         roles:
            - public
         <b>cert-file: my-public-cert</b></codeblock>
      <p>The above bolded sections as follows:</p>
      <p><b>external-name</b> - The external name defines how the public URLs will be registered in
        Keystone. Users of your cloud will need to be able to resolve this URL to access the cloud
        APIs, and if you are using the TLS, the name must match the certificate used.</p>
      <p>Because this value is difficult to change after initial deployment, this value is left
        blank in the supplied example which prevents the configuration processor from running until
        a value has been supplied. If you want to register the public URLs as IP addresses instead
        of a name, then you can comment out this line.</p>
      <p><b>cert-file</b> - Provide the name of the file located in
          <codeph>~/helion/my_cloud/config/tls/certs/</codeph> that will be used for your cloud
        endpoints. As shown above, this can be either a single certificate for all endpoints or a
        default certificate file and a set of service-specific certificate files.</p>
      <p><b>tls-components</b> - If you do not want to use a TLS for the public URLs then change the
        entry that says <codeph>tls-components</codeph> to <codeph>components</codeph>.</p>
    </section>
    <section id="servers">
      <title>Servers.yml</title>
      <p>This file is where you provide the details of the physical servers that make up your cloud.
        There are two sections to this file: <codeph>baremetal</codeph> and
        <codeph>servers</codeph>:</p>
      <codeblock>
   baremetal:
      # NOTE: These values need to be changed to match your environment.
      # Define the network range that contains the ip-addr values for
      # the individual servers listed below.
      subnet: <b>192.168.10.0</b>
      netmask: <b>255.255.255.0</b></codeblock>
      <p>The two values in this section are used to configure cobbler for operating system
        installation and must match the network values for the addresses given for the servers.</p>
      <p>The servers section below provides the details of each individual server. For example, here
        are the details for the first controller:</p>
      <codeblock>
   servers:
        
      # Controllers
      - id: controller1
        ip-addr: <b>192.168.10.3</b>
        role: CONTROLLER-ROLE
        server-group: RACK1
        nic-mapping: <b>HP-DL360-4PORT</b>
        mac-addr: <b>b2:72:8d:ac:7c:6f</b>
        ilo-ip: <b>192.168.9.3</b>
        ilo-password: <b>password</b>
        ilo-user: <b>admin</b></codeblock>
      <p>Here is a description of each of the above bolded sections:</p>
      <p><b>id</b> - A name you provide to uniquely identify a server. This can be any string which
        is makes sense in your context, such as an asset tag, descriptive name, etc. The system will
        use this value to remember how the server has been allocated.</p>
      <p><b>ip-addr</b> - The IP address that the system will use for SSH connections to the server
        for deployment and configuration changes. This address must be in the IP range of one of the
        networks in the model. In the example, the servers are provided with addresses from the
        MANAGEMENT network.</p>
      <p><b>role</b> - A string that refers to an entry in <codeph>server_roles.yml</codeph> that
        tells the system how to configure the disks and network interfaces for this server. Roles
        are also used to define which servers can be used for specific purposes. Adding and changing
        roles is beyond the scope of this walkthrough - for more information, see <xref
          href="input_model.dita"><keyword keyref="kw-hos-phrase"/> Input Model</xref>.</p>
      <p><b>server-group</b> - Tells the system how this server is physically related to networks
        and other servers. Server groups are used to ensure that servers in a cluster are selected
        from different physical groups. The example provides a set of server groups that divide the
        servers into three sets called <b>RACK1</b>, <b>RACK2</b>, and <b>RACK3</b>. Modifying the
        server group structure is beyond the scope of this walkthrough - for more information, see
          <xref href="input_model.dita"><keyword keyref="kw-hos-phrase"/> Input Model</xref>.</p>
      <p><b>nic-mapping</b> - The name of a network port mapping definition (for more information,
        see <xref href="example_configurations.dita#example_configurations/nicmappings"
          >Nic_mappings.yml</xref>). You need to set this to the mapping that corresponds to this
        server.</p>
      <p><b>mac-addr</b> - The MAC address of the interface associated with this server that will be
        used for PXE boot.</p>
      <p><b>ilo-ip</b> - The IP address of the iLO or IPMI port for this server.</p>
      <p><b>ilo-user and ilo-password</b> - The login details used to access the iLO or IPMI port of
        this server. The iLO password value can be provided as an OpenSSL encrypted string. (For
        instructions on how to generate encrypted passwords, see <xref
          href="installation/install_entryscale_kvm.dita#install_kvm/configuration"/>.</p>
    </section>
    <section id="customizing_inputmodel"><title>Customizing the Input Model</title>
      <p>This section covers additional changes that you can make to further adapt the example to
        your environment:</p>
      <ul>
        <li>Update <codeph>disks_controller.yml</codeph> to add additional disk capacity to your
          controllers.</li>
        <li>Update <codeph>disks_vsa.yml</codeph> to add additional disk capacity to your VSA
          servers.</li>
        <li>Update <codeph>disks_compute.yml</codeph> to add additional disk capacity to your
          compute servers.</li>
      </ul>
    </section>
    <section id="disks_controller"><title>Disks_controller.yml</title>
      <p>The disk configuration of the controllers consists of two sections: a definition of a
        volume group that provides a number of file-systems for various subsystems, and device-group
        that provides disk capacity for Swift.</p>
    </section>
    <section id="filesystems"><title>File Systems Storage</title>
      <p>The root volume group (hlm-vg) is divided into a number of logical volumes that provide
        separate file systems for the various services that are co-hosted on the controllers in the
        entry-scale examples. The capacity of each file system is expressed as a percentage of the
        overall volume group capacity. Because not all file system usage scales linearly, two
        different disk configurations are provided:</p>
      <ul>
        <li><b>CONTROLLER-DISKS</b> - Based on a 512 GB root volume group.</li>
        <li><b>CONTROLLER-1TB-DISKS</b> - Provides a higher percentage of space for the logging
          service.</li>
      </ul>
      <p>As supplied, the example uses the smaller disk model. To use the larger disk model you need
        to modify the <codeph>disk-models</codeph> parameter in the
          <codeph>server_roles.yml</codeph> file, as shown below:</p>
      <codeblock>
    server-roles:
        
       - name: CONTROLLER-ROLE
         interface-model: CONTROLLER-INTERFACES
         disk-model: CONTROLLER-1TB-DISKS</codeblock>
      <p>To add additional disks to the root volume group, you need to modify the volume group
        definition in whichever disk model you are using. The following example shows adding an
        additional disk, <codeph>/dev/sdd</codeph> to the <codeph>disks_controller.yml</codeph>
        file:</p>
      <codeblock>
   disk-models:
      - name: CONTROLLER-DISKS
        
        volume-groups:
         - name: hlm-vg
           physical-volumes:
        
              # NOTE: 'sda_root' is a templated value. This value is checked in
              # os-config and replaced by the partition actually used on sda
              #e.g. sda1 or sda5
              - /dev/sda_root
              <b>- /dev/sdd</b></codeblock>
    </section>
    <section id="swiftstorage"><title>Swift Storage</title>
      <p>Swift storage is configured as a device-group and has a syntax that allows disks to be
        allocated to specific rings. In the example, two disks are allocated to Swift to be shared
        by the account, container, and object-0 rings.</p>
      <codeblock>
   device-groups:
       - name: swiftobj
         devices:
            - name: /dev/sdb
            - name: /dev/sdc
            # Add any additional disks for swift here
            # -name: /dev/sdd
            # -name: /dev/sde
         consumer:
           name: swift
           attrs:
              rings:
                 - account
                 - container
                       - object-0</codeblock>
      <p>For instruction to configure additional Swift storage, see <xref
          href="objectstorage/allocating_disk_drives.dita"/>.</p>
    </section>
    <section id="disks_vsa"><title>Disks_vsa.yml</title>
      <p>VSA storage is configured as a device-group and has a syntax that allows disks to be
        allocated for data storage or for adaptive optimization (caching). As a best practice, you
        should use solid state drives for adaptive optimization. The example disk configuration for
        VSA nodes has two disks, one for data and one of adaptive optimization. (For more
        information, see <xref
          href="installation/configure_vsa.dita#config_vsa/deploy-vsa-with-ao-without-ao">VSA with
          AO or without AO</xref>.)</p>
      <codeblock>
   device-groups:
       - name: vsa-data
         consumer:
           name: vsa
           usage: data
         devices:
           - name: /dev/sdc
       - name: vsa-cache
         consumer:
           name: vsa
           usage: adaptive-optimization
         devices:
            - name: /dev/sdb</codeblock>
      <p>Additional capacity can be added by adding more disks to the <codeph>vsa-data</codeph>
        device group. Similarly, caching capacity can be increased by adding more high speed storage
        devices to the <codeph>vsa-cache</codeph> device group.</p>
    </section>
    <section id="disks_compute"><title>Disks_compute.yml</title>
      <p>The example disk configuration for compute nodes consists of two volume groups: one for the
        operating system and one for the ephemeral storage for virtual machines, with one disk
        allocated to each.</p>
      <p>Additional virtual machine ephemeral storage capacity can be configured by adding
        additional disks to the <codeph>vg-comp</codeph> volume group. The following example shows
        the addition of two more disks, <codeph>/dev/sdc</codeph> and <codeph>/dev/sdd</codeph>, to
        the <codeph>disks_compute.yml</codeph> file:</p>
      <codeblock>
   - name: vg-comp
        physical-volumes:
          - /dev/sdb
          - /dev/sdc
          - /dev/sdd
        logical-volumes:
          - name: compute
            size: 95%
            mount: /var/lib/nova
            fstype: ext4
                  mkfs-opts: -O large_file</codeblock>
    </section>
    <section id="standalone"><title>Using a Standalone Lifecycle-Manager Node</title>
      <p>All of the examples described above host the lifecycle manager on one of the control nodes.
        It is also possible to deploy this service on a dedicated node, as shown below: </p>
      <p><image href="../media/examples/entry_scale_kvm_vsa_shared.png"/></p>
      <p><xref href="../media/examples/entry_scale_kvm_vsa_shared_lg.png" scope="external"
          format="html">Download a high-resolution version</xref></p>
      <p>A typical use case for wanting to run the dedicated lifecycle manager is to be able to test
        the deployment of different configurations without having to re-install the first server.
        Some administrators might also prefer the additional security of keeping all of the
        configuration data on a separate server from those that users of the cloud connect to
        (although all of the data can be encrypted and SSH keys can be password protected).</p>
      <p>This requires the following changes to the configuration files:</p>
      <ul>
        <li>Update <uicontrol>control_plane.yml</uicontrol> to add the lifecycle manager.</li>
        <li>Update <uicontrol>server_roles.yml</uicontrol> to add the lifecycle manager role.</li>
        <li>Update <uicontrol>net_interfaces.yml</uicontrol> to add the interface definition for the
          lifecycle manager.</li>
        <li>Create a <uicontrol>disks_lifecycle_manager.yml</uicontrol> file to define the disk
          layout for the lifecycle manager.</li>
        <li>Update <uicontrol>servers.yml</uicontrol> to add the dedicated lifecycle manager
          node.</li>
      </ul>
    </section>
    <section id="control_plane_yml"><title>Control_plane.yml</title>
      <p>The snippet below shows the addition of a single node cluster into the control plane to
        host the lifecycle manager service. Note that, in addition to adding the new cluster, you
        also have to remove the lifecycle manager component from the <codeph>cluster1</codeph> in
        the examples:</p>
      <codeblock>
  clusters:
     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1	
       allocation-policy: strict
       service-components:
         - lifecycle-manager
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-server</codeblock>
      <p>This specifies a single node of role <codeph>LIFECYCLE-MANAGER-ROLE</codeph> hosting the
        lifecycle manager.</p>
    </section>
    <section id="server_roles_yml"><title>Server_roles.yml</title>
      <p>The snippet below shows the insertion of the new server roles definition:</p>
      <codeblock>
   server-roles:
      
      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS	
      
      - name: CONTROLLER-ROLE</codeblock>
      <p>This defines a new server role which references a new interface-model and disk-model to be
        used when configuring the server.</p>
    </section>
    <section id="net_interfaces_yml"><title>Net-interfaces.yml</title>
      <p>The snippet below shows the insertion of the network-interface info:</p>
      <codeblock>
    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</codeblock>
      <p>This assumes that the server uses the same physical networking layout as the other servers
        in the example. For details on how to modify this to match your configuration, see <xref
          href="example_configurations.dita#example_configurations/netinterfaces"
          >Net_interfaces.yml</xref>.</p>
    </section>
    <section id="disks_lifecycle_manager_yml"><title>Disks_lifecycle_manager.yml</title>
      <p>In the examples, disk-models are provided as separate files (this is just a convention, not
        a limitation) so the following should be added as a new file named
          <codeph>disks_lifecycle_manager.yml</codeph>:</p>
      <codeblock>---
   product:
      version: 2
        
   disk-models:
   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5
        
     volume-groups:
       - name: hlm-vg
         physical-volumes:
           - /dev/sda_root
        
       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</codeblock>
    </section>
    <section id="servers_yml"><title>Servers.yml</title>
      <p>The snippet below shows the insertion of an additional server used for hosting the
        lifecycle manager. Provide the address information here for the server you are running on,
        i.e., the node where you have installed the <keyword keyref="kw-hos"/> ISO.</p>
      <codeblock>
  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required
        
     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: &#60;your IP address here>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       # ipmi information is not needed 
          
     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</codeblock>
    </section>
    <section id="without_dvr"><title>Configuring <keyword keyref="kw-hos"/> without DVR</title>
      <p>By default in the KVM model, the Neutron service utilizes distributed routing (DVR). This
        is the recommended setup because it allows for high availability. However, if you would like
        to disable this feature, here are the steps to achieve this.</p>
      <p>On your lifecycle manager, make the following changes:</p>
      <ol>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/neutron.conf.j2</codeph> file, change
          the line below from: <codeblock>router_distributed = {{ router_distributed }}</codeblock>
          <p>to:</p>
          <codeblock>router_distributed = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/ml2_conf.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_distributed_routing = True</codeblock>
          <p>to:</p>
          <codeblock>enable_distributed_routing = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/l3_agent.ini.j2</codeph> file, change
          the line below from: <codeblock>agent_mode = {{ neutron_l3_agent_mode }}</codeblock>
          <p>to:</p>
          <codeblock>agent_mode = legacy</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, remove
          the following values from the Compute resource <codeph>service-components</codeph> list:
          <codeblock>
   - neutron-l3-agent
   - neutron-metadata-agent</codeblock></li>
      </ol>
    </section>
    <section id="without_l3agent"><title>Configuring <keyword keyref="kw-hos"/> with Provider VLANs
        and Physical Routers Only</title>
      <p>Another option for configuring Neutron is to use provider VLANs and physical routers only,
        here are the steps to achieve this.</p>
      <p>On your lifecycle manager, make the following changes:</p>
      <ol>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/neutron.conf.j2</codeph> file, change
          the line below from: <codeblock>router_distributed = {{ router_distributed }}</codeblock>
          <p>to:</p>
          <codeblock>router_distributed = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/ml2_conf.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_distributed_routing = True</codeblock>
          <p>to:</p>
          <codeblock>enable_distributed_routing = False</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/config/neutron/dhcp_agent.ini.j2</codeph> file, change
          the line below from: <codeblock>enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</codeblock>
          <p>to:</p>
          <codeblock>enable_isolated_metadata = True</codeblock></li>
        <li>In the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, remove
          the following values from the Compute resource <codeph>service-components</codeph> list:
          <codeblock>
  - neutron-l3-agent
  - neutron-metadata-agent</codeblock></li>
      </ol>
    </section>
    <section id="twosystems"><title>Considerations When Installing Two Systems on One Subnet</title>
      <p>If you wish to install two separate <ph conkeyref="HOS-conrefs/product-title"/> systems
        using a single subnet, you will need to consider the following notes.</p>
      <p>The <codeph>ip_cluster</codeph> service includes the <codeph>keepalived</codeph> daemon
        which maintains virtual IPs (VIPs) on cluster nodes. In order to maintain VIPs, it
        communicates between cluster nodes over the VRRP protocol.</p>
      <p>A VRRP virtual routerid identifies a particular VRRP cluster and must be unique for a
        subnet. If you have two VRRP clusters with the same virtual routerid, causing a clash of
        VRRP traffic, the VIPs are unlikely to be up or pingable and you are likely to get the
        following signature in your <codeph>/etc/keepalived/keepalived.log</codeph>:</p>
      <codeblock>Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: ip address associated with VRID not present in received packet : 10.2.1.11
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: one or more VIP associated with VRID mismatch actual MASTER advert
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: bogus VRRP packet received on br-bond0 !!!
Dec 16 15:43:43 helion-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: VRRP_Instance(VI_2) ignoring received advertisment...</codeblock>
      <p>To resolve this issue, our recommendation is to install your separate <ph
          conkeyref="HOS-conrefs/product-title"/> systems with VRRP traffic on different
        subnets.</p>
      <p>If this is not possible, you may also assign a unique routerid to your separate <ph
          conkeyref="HOS-conrefs/product-title"/> system by changing the
          <codeph>keepalived_vrrp_offset</codeph> service configurable. The routerid is currently
        derived using the <codeph>keepalived_vrrp_index</codeph> which comes from a configuration
        processor variable and the <codeph>keepalived_vrrp_offset</codeph>.</p>
      <p>For example, </p>
      <ol>
        <li>Log in to your lifecycle manager.</li>
        <li>Edit your <codeph>~/helion/my_cloud/config/keepalived/defaults.yml</codeph> file and
          change the value of the following line: <codeblock>keepalived_vrrp_offset: 0</codeblock>
          <p>Change the off value to a number that uniquely identifies a separate vrrp cluster. For
            example:</p>
          <p><codeph>keepalived_vrrp_offset: 0</codeph> for the 1st vrrp cluster on this subnet.</p>
          <p><codeph>keepalived_vrrp_offset: 1</codeph> for the 2nd vrrp cluster on this subnet.</p>
          <p><codeph>keepalived_vrrp_offset: 2</codeph> for the 3rd vrrp cluster on this
          subnet.</p></li>
        <li>Commit your configuration to the <xref href="installation/using_git.dita">local git
            repo</xref>, as follows:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "changing Admin password"</codeblock></li>
        <li>Run the configuration processor with this command:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Use the playbook below to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>If you are making this change after your initial install, run the following reconfigure
          playbook to make this change in your environment:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts FND-CLU-reconfigure.yml
        </codeblock></li>
      </ol>
    </section>
  </body>
</topic>
