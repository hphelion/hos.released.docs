<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic67234">
  <title>A10 Pre-Installation Checklist</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>Before installing the A10 Networks LBaaS v2 driver in your Helion OpenStack deployment, make
      sure you fulfill all of the following requirements:</p>
    <section id="checklist"><title>A10 Pre-Installation Checklist</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_ckx_rd4_ht">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="25pt"/>
            <colspec colname="c2" colnum="2" colwidth="1*"/>
            <thead>
              <row>
                <entry>&#9744;</entry>
                <entry>Item</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry/>
                <entry><xref type="section" href="#topic67234/a10_supported_hardware">Use Supported Hardware Configuration</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topic67234/a10_config_proxy">Configure Your Environment Proxy Settings</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/verify_cluster">Verify You Have a Working Helion OpenStack Cluster</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/config_glance">Configure the Helion OpenStack Cluster with Glance Image</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/config_extnet">Configure the Helion OpenStack Cluster with an External Network</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/config_tenant_net">Configure the Helion OpenStack Cluster with a Tenant Network</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/verify_floatingIP">Verify Floating IP is Working</xref></entry>
              </row>
              <row>
                <entry/>
                <entry><xref type="section" href="#topicid67234/a10_install_Python">Manually Install Python Packages Into Each Controller Node</xref></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
   
    <section id="a10_supported_hardware"> <title>Use a Supported Hardware Configuration</title>
            <p>For testing purposes, the following hardware configuration was used to test the A10 Networks LBaaS driver integration with Helion OpenStack 2.0:</p>
            <table frame="all" rowsep="1" colsep="1" id="table_a10_supp_hw">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="3.14*"/>
                <thead>
                  <row>
                    <entry>Appliance</entry>
                    <entry>Requirement</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>Make</entry>
                    <entry>A10 Networks</entry>
                  </row>
                  <row>
                    <entry>Model</entry>
                    <entry>Thunder ADC 930</entry>
                  </row>
                  <row>
                    <entry>Operating System</entry>
                    <entry>64 bit Advanced Core OS Version 2.7.1-P5 (build 68)</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>
          </section>
          <section>
            <p>For more information on the A10 Networks Thunder ADC hardware specifications, see the 
              <xref href="https://www.a10networks.com/sites/default/files/A10-DS-15100-EN.pdf" scope="external" format="html">Thunder ADC Datasheet</xref>.</p>
          </section>
  

    <section id="a10_config_proxy"> <title>Configure Your Environment Proxy Settings</title>
      <p>
        To install the required Python packages, you must know your proxy settings for the HTTP protocol and add them to your configuration file.</p>
    </section>
    <section id="findproxy"><title>To find your current proxy settings:</title>
      <ol>
        <li>At the command prompt, run:
          <codeblock>ENV|grep -i proxy</codeblock>
        </li>
      </ol>
    </section>
    <section id="setproxy"><title>To set your environment proxy settings:</title>
      <ol>
        <li>To open the configuration file in the vi text editor, at the command prompt, run:
          <codeblock>#vi etc/sampledirectory/Virtualenv.config</codeblock>
        </li>
        <li>To modify data in the configuration file and go into insert mode, press <b>i</b>.</li>
        <li>To set the HTTP proxy, enter your settings in the following format:
          <codeblock>export http_proxy=http://&lt;proxy-server-ip&gt;:&lt;port&gt;</codeblock>
          <p>For example:
            <codeblock>export http_proxy=http://10.1.24.18:8080</codeblock>
          </p>
        </li>
      </ol>
    </section>
    
    
    <section id="verify_cluster"><title>Verify You Have a Working Helion OpenStack Cluster</title>
      <p>Perform the following action to see a list of active clusters and their status.</p>
      <ol>
        <li>To view a list of clusters for the given vCenter, run:
          <codeblock>#eon cluster-list --vcenter-id</codeblock>
        </li>
      </ol>
      <dl>
        <dlentry>
          <dt>Sample Output</dt>
          <dd><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674</codeblock>
            <table frame="all" rowsep="1" colsep="1" id="table_verify_cluster">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/>
                <colspec colname="c3" colnum="3" colwidth="1*"/>
                <colspec colname="c4" colnum="4" colwidth="1*"/>
                <thead>
                  <row>
                    <entry>MOID</entry>
                    <entry>Name</entry>
                    <entry>Datacenter</entry>
                    <entry>Import Status</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>domain-22</entry>
                    <entry>cluster2</entry>
                    <entry>DC1</entry>
                    <entry>imported</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>
          </dd>
        </dlentry>
      </dl>
    </section>
    
    <section id="config_glance"><title>Configure the Helion OpenStack Cluster with Glance Image </title>
      <p>The Glance project provides the following features:</p>
      <ul>
        <li>image registration</li>
        <li>discovery service</li>
        <li>image delivery service</li>
      </ul>
      <p>These services are used by Nova to deliver images from object stores, such as OpenStack's Swift service to Nova's compute nodes. Glance may be deployed in a number of ways.</p>
      <p>For instructions on how to deploy Glance, refer to the Glance product documentation: 
        <xref href="https://jujucharms.com/glance/" scope="external" format="html">Glance</xref></p>
      <note type="warning">All deployments require the existence of the other core OpenStack services deployed through Juju charms, specifically: mysql, keystone and nova-cloud-controller. The referenced documentation assumes these services have already been deployed.</note>
    </section>
    
    <section id="config_extnet"><title>Configure the Helion OpenStack Cluster with an External Network</title>
      <p>You must have an external network set up to allow your Compute instances to reach the internet. There are multiple methods you can use to create this external network. The HPE Helion OpenStack Documentation provides information on two ways you can create an external network. Click on the following links for more information.</p>
      <p>To configure an external network, choose one of the following options:</p>
      <ul>
        <li><xref href="http://docs.hpcloud.com/#helion/administration/create_extnet.html" scope="external" format="html">Using the Ansible Playbook</xref>The HPE Helion OpenStack installer provides an Ansible playbook that will create this network for use across your projects. This playbook will query the Networking service for an existing external network, and then create a new one if you do not already have one. The resulting external network will have the name ext-net with a subnet matching the CIDR you specify in the command.
        </li>
        <li><xref href="http://docs.hpcloud.com/#helion/administration/create_extnet.html" scope="external" format="html">Using the NeutronClient CLI</xref>Use the command line tool from your lifecycle manager to create an external network.
        </li>
      </ul>
    </section>
    
    <section id="config_tenant_net"><title>Configure the Helion OpenStack Cluster with a Tenant Network</title>
      <p>In OpenStack user interfaces and documentation, a group of users is referred to as a project or tenant. These terms are interchangeable. In an OpenStack environment, you can create tenant networks using the Networking service (code named Neutron). Tenant networks are used to isolate access to Compute resources.</p>
      <note>Refer to the Helion OpenStack 2.0 Documentation for general information on <xref href="http://docs.hpcloud.com/#helion/userguide/create_network.html" scope="external" format="html">Creating a Private Network</xref>.</note>
      <ol>
        <li>To create a tenant network, run: <codeblock># neutron net-create tenant-network
          # neutron subnet-create –name tenant-subnet –gateway &lt;ip-address&gt; -- allocation pool start &lt;ip-address&gt;,end=&lt;ip-address&gt; tenant-network &lt;ip-address/prefix&gt;</codeblock></li>
        <li>To provision the baremetal node, run: <codeblock># nova boot --nic net-id=&lt;tenant network id&gt; --image &lt;glance image id&gt; --flavor baremetal --key-name &lt;keypair-name&gt;</codeblock>
          <p>Using the following values:</p>
          <dl>
            <dlentry>
              <dt>tenant network id</dt>
              <dd>is the UUID of the tenant network</dd>
            </dlentry>
            <dlentry>
              <dt>image</dt>
              <dd>is the name of ID of image (see <b>nova image-list</b></dd>
            </dlentry>
            <dlentry>
              <dt>flavor</dt>
              <dd>is the name or ID of flavor</dd>
            </dlentry>
            <dlentry>
              <dt>keypair-name</dt>
              <dd>is the name of the keypair to use</dd>
            </dlentry>
          </dl>
          <codeblock>#nova image-list</codeblock>
          <table frame="all" rowsep="1" colsep="1" id="nova_image_list">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1.33*"/>
              <colspec colname="c2" colnum="2" colwidth="1*"/>
              <colspec colname="c3" colnum="3" colwidth="1.16*"/>
              <colspec colname="c4" colnum="4" colwidth="1.16*"/>
              <thead>
                <row>
                  <entry>ID</entry>
                  <entry>Name</entry>
                  <entry>Status</entry>
                  <entry>Server</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>ca797ded-5fa0-4ef2-8635</entry>
                  <entry>hoscg-2.0</entry>
                  <entry>Active</entry>
                  <entry/>
                </row>
                <row>
                  <entry>2394d02e-2f58-4b85-b9e4</entry>
                  <entry>hoscg-2.0</entry>
                  <entry>Active</entry>
                  <entry/>
                </row>
              </tbody>
            </tgroup>
          </table>
        </li>
        <li>To view details about the network, run:<codeblock>#neutron net-show tenant-network</codeblock>
        </li>
      </ol>
      <p><b>Sample Output</b></p>
      <table frame="all" rowsep="1" colsep="1" id="sample_output">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="1*"/>
          <thead>
            <row>
              <entry>Field</entry>
              <entry>Value</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>admin_state_up</entry>
              <entry>True</entry>
            </row>
            <row>
              <entry>ID</entry>
              <entry>2c4f260c-507a-47a7-a0f4-7faebfe312b0</entry>
            </row>
            <row>
              <entry>mtu</entry>
              <entry>0</entry>
            </row>
            <row>
              <entry>name</entry>
              <entry>tenant-network</entry>
            </row>
            <row>
              <entry>provider:network_type</entry>
              <entry>vlan</entry>
            </row>
            <row>
              <entry>provider:physical_network</entry>
              <entry>physnet1</entry>
            </row>
            <row>
              <entry>provider:segmentation_id</entry>
              <entry>102</entry>
            </row>
            <row>
              <entry>router:external</entry>
              <entry>False</entry>
            </row>
            <row>
              <entry>shared</entry>
              <entry>True</entry>
            </row>
            <row>
              <entry>subnets</entry>
              <entry>6a952e6b-903f-41db-b31d-62dc52c65dd0</entry>
            </row>
            <row>
              <entry>tenant_id</entry>
              <entry>c288a2816237465c8ea6efc16dc05b88</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    
    <section id="verify_floatingIP"><title>Verify Floating IP is Working</title>
      <p>When a single OpenStack cluster boots up, fixed IPs are allocated dynamically by the nova-network component. A fixed IP works well when you need connectivity only between instances inside your cloud deployment. </p>
      <p>To provide IP addresses that are publicly routable and to allow users to explicitly allocate an IP address, the cloud administrator must configure a pool of floating IPs. When an instance starts up, this pool allows users to allocate floating IP addresses to their instances. The result is that the floating IP address is now visible to different ISPs or external networks outside of your cloud deployment. If an instance dies, the user can reuse the floating IP by attaching it to another instance. Only one floating IP address can be allocated to an instance at any given time.</p>
      <p>Perform the following actions to verify that floating IP Pools are configured:</p>
      <ol>
        <li>To list all pools that provide floating IP addresses, run:
          <codeblock># nova floating-ip-pool-list</codeblock>
          <b>Sample Output</b>
          <codeblock>Name     Public Test</codeblock>
          <note type="attention"> If this list is empty, the cloud administrator must configure a pool of floating IP addresses. OpenStack provides documentation on how to <xref href="http://docs.openstack.org/admin-guide-cloud/compute-networking-nova.html" scope="external" format="html">Configure A Pool of Floating IPs</xref>.</note>
        </li>
        <li>To list all floating IP addresses that are allocated to the current project, run: <codeblock># nova floating-ip-list</codeblock>
          <p><b>Sample Output</b></p>
          <table frame="all" rowsep="1" colsep="1" id="floating_ip_list_sample">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1*"/>
              <colspec colname="c2" colnum="2" colwidth="2.94*"/>
              <colspec colname="c3" colnum="3" colwidth="1.43*"/>
              <colspec colname="c4" colnum="4" colwidth="1.39*"/>
              <thead>
                <row>
                  <entry>IP</entry>
                  <entry>Instance ID</entry>
                  <entry>Fixed IP</entry>
                  <entry>Pool</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>172.24.4.255</entry>
                  <entry>4a60ff6a-7a3c-49d7-9515-86ae501044c6</entry>
                  <entry>10.0.0.2</entry>
                  <entry>public</entry>
                </row>
                <row>
                  <entry>172.24.4.226</entry>
                  <entry>None</entry>
                  <entry>None</entry>
                  <entry>Public</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <p>For each floating IP address that is allocated to the current project, the command outputs the floating IP address, the ID for the instance to which the floating IP address is assigned, the associated fixed IP address, and the pool from which the floating IP address was allocated.</p>
        </li>
      </ol>
    </section>
    
    <section id="a10_install_python"><title>Manually Install Python Packages Into Each Controller Node</title>
      <p>The term “package” in this context is being used as a synonym for a distribution, or a bundle of software to be installed. Do not confuse it with the kind of package that you import in your Python source code (i.e. a container of modules).</p>
      <p>Refer to the Python Website to read more about <xref href="https://packaging.python.org/en/latest/installing/" scope="external" format="html">Installing Packages</xref>.</p>
    </section>
  </body>
</topic>
